{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e626a4",
   "metadata": {},
   "source": [
    "# Tutorial:SMAGE-seq (no label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8cab9",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how to cluster SMAGE-seq data using scMAGCA. As an example, we use a human peripheral blood mononuclear sample dataset 'GSM4949911' containing 8213 cells. It contains two omics data, with ATAC containing 66828 features and RNA containing 36601 features, and is not labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c16980",
   "metadata": {},
   "source": [
    "## Loading package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b34dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import read_dataset, preprocess_dataset\n",
    "from utils import *\n",
    "from scMAGCA import scMultiCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2919a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "random.seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.manual_seed(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1004cdb",
   "metadata": {},
   "source": [
    "## Reading SMAGE-seq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3db79",
   "metadata": {},
   "source": [
    "The required input files include: \n",
    "1) x1: Chromatin accessibility matrix (data format is h5ad file) : GSM4949911_tea_atac.h5ad;\\\n",
    "2) x2: Gene expression matrix (data format is h5ad file) : GSM4949911_tea_rna.h5ad.\n",
    "\n",
    "To ensure reproducibility of the results, please read the above data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3027d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(sc.read_h5ad('../datasets/GSM4949911_tea/GSM4949911_tea_atac.h5ad').to_df()).astype('float32')\n",
    "x2 = np.array(sc.read_h5ad('../datasets/GSM4949911_tea/GSM4949911_tea_rna.h5ad').to_df()).astype('float32')\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823cd688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 2., 0., ..., 0., 0., 0.],\n",
       "        [8., 2., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 2., ..., 0., 0., 2.],\n",
       "        [0., 2., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[  0.,   0.,   0., ..., 102.,  70.,  43.],\n",
       "        [  0.,   0.,   0., ...,  22.,   8.,  27.],\n",
       "        [  1.,   0.,   0., ...,  11.,  12.,  11.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0., ...,  27.,  30.,  38.],\n",
       "        [  0.,   0.,   0., ...,  20.,  19.,  19.],\n",
       "        [  5.,   0.,   0., ...,  11.,  16.,  17.]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfb6c1",
   "metadata": {},
   "source": [
    "We select the two omics data for high expression, and the number of chosen features are both set to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb64fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen offset: 1.24\n",
      "Chosen offset: 0.33\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importantGenes = geneSelection(x1, n=2000)\n",
    "x1 = x1[:, importantGenes]\n",
    "importantGenes = geneSelection(x2, n=2000)\n",
    "x2 = x2[:, importantGenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a812db1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 2000 features and 8213 cells.\n"
     ]
    }
   ],
   "source": [
    "adata1 = sc.AnnData(x1)\n",
    "adata1 = read_dataset(adata1, copy=True)\n",
    "adata1 = preprocess_dataset(adata1, normalize_input=True, logtrans_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee7b5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 8213 × 2000\n",
       "    obs: 'DCA_split', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c354c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 2000 features and 8213 cells.\n"
     ]
    }
   ],
   "source": [
    "adata2 = sc.AnnData(x2)\n",
    "adata2 = read_dataset(adata2, copy=True)\n",
    "adata2 = preprocess_dataset(adata2, normalize_input=True, logtrans_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8545de66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 8213 × 2000\n",
       "    obs: 'DCA_split', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0bddd",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630d62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scMultiCluster(input_dim1=adata1.n_vars, input_dim2=adata2.n_vars, device='cuda').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "546c7dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scMultiCluster(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_gnn): ModuleList(\n",
       "      (0): GCNConv(4000, 1024)\n",
       "      (1): GCNConv(1024, 256)\n",
       "      (2): GCNConv(256, 64)\n",
       "      (3): GCNConv(64, 32)\n",
       "    )\n",
       "    (stacked_bns): ModuleList(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (stacked_prelus): ModuleList(\n",
       "      (0-3): 4 x PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): PReLU(num_parameters=1)\n",
       "    (6): Linear(in_features=1024, out_features=4000, bias=True)\n",
       "  )\n",
       "  (dec_mean): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=4000, bias=True)\n",
       "    (3): MeanAct()\n",
       "  )\n",
       "  (dec_disp): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=4000, bias=True)\n",
       "    (3): DispAct()\n",
       "  )\n",
       "  (dec_pi): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=4000, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (zinb_loss): ZINBLoss()\n",
       "  (discriminator): Discriminator(\n",
       "    (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430b94b",
   "metadata": {},
   "source": [
    "The ad_out parameter is set to 32 in the SMAGE-seq dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624c6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining stage\n",
      "Pretrain epoch 1, recon_loss:1.177419, zinb_loss:0.945166, adversial_loss:1.372231\n",
      "Pretrain epoch 2, recon_loss:1.087229, zinb_loss:0.921236, adversial_loss:1.373194\n",
      "Pretrain epoch 3, recon_loss:0.948224, zinb_loss:0.890657, adversial_loss:1.373617\n",
      "Pretrain epoch 4, recon_loss:0.885800, zinb_loss:0.858559, adversial_loss:1.358683\n",
      "Pretrain epoch 5, recon_loss:0.866452, zinb_loss:0.824501, adversial_loss:1.359133\n",
      "Pretrain epoch 6, recon_loss:0.856318, zinb_loss:0.789515, adversial_loss:1.355022\n",
      "Pretrain epoch 7, recon_loss:0.850889, zinb_loss:0.759701, adversial_loss:1.351529\n",
      "Pretrain epoch 8, recon_loss:0.842909, zinb_loss:0.738662, adversial_loss:1.351824\n",
      "Pretrain epoch 9, recon_loss:0.836522, zinb_loss:0.726100, adversial_loss:1.351907\n",
      "Pretrain epoch 10, recon_loss:0.829639, zinb_loss:0.718908, adversial_loss:1.350247\n",
      "Pretrain epoch 11, recon_loss:0.824786, zinb_loss:0.713263, adversial_loss:1.348227\n",
      "Pretrain epoch 12, recon_loss:0.821581, zinb_loss:0.707017, adversial_loss:1.347948\n",
      "Pretrain epoch 13, recon_loss:0.818954, zinb_loss:0.699969, adversial_loss:1.349329\n",
      "Pretrain epoch 14, recon_loss:0.816677, zinb_loss:0.693693, adversial_loss:1.350878\n",
      "Pretrain epoch 15, recon_loss:0.813920, zinb_loss:0.688917, adversial_loss:1.350498\n",
      "Pretrain epoch 16, recon_loss:0.811877, zinb_loss:0.685424, adversial_loss:1.349127\n",
      "Pretrain epoch 17, recon_loss:0.809966, zinb_loss:0.682478, adversial_loss:1.348064\n",
      "Pretrain epoch 18, recon_loss:0.808354, zinb_loss:0.679629, adversial_loss:1.347751\n",
      "Pretrain epoch 19, recon_loss:0.807258, zinb_loss:0.676862, adversial_loss:1.347479\n",
      "Pretrain epoch 20, recon_loss:0.806017, zinb_loss:0.674137, adversial_loss:1.346404\n",
      "Pretrain epoch 21, recon_loss:0.804428, zinb_loss:0.671521, adversial_loss:1.344638\n",
      "Pretrain epoch 22, recon_loss:0.802824, zinb_loss:0.668903, adversial_loss:1.343253\n",
      "Pretrain epoch 23, recon_loss:0.801624, zinb_loss:0.666379, adversial_loss:1.341949\n",
      "Pretrain epoch 24, recon_loss:0.800622, zinb_loss:0.663973, adversial_loss:1.340480\n",
      "Pretrain epoch 25, recon_loss:0.799793, zinb_loss:0.661805, adversial_loss:1.340241\n",
      "Pretrain epoch 26, recon_loss:0.799017, zinb_loss:0.659948, adversial_loss:1.340883\n",
      "Pretrain epoch 27, recon_loss:0.798392, zinb_loss:0.658303, adversial_loss:1.340143\n",
      "Pretrain epoch 28, recon_loss:0.797557, zinb_loss:0.656784, adversial_loss:1.339819\n",
      "Pretrain epoch 29, recon_loss:0.796525, zinb_loss:0.655244, adversial_loss:1.339375\n",
      "Pretrain epoch 30, recon_loss:0.795610, zinb_loss:0.653714, adversial_loss:1.339210\n",
      "Pretrain epoch 31, recon_loss:0.794745, zinb_loss:0.652404, adversial_loss:1.337723\n",
      "Pretrain epoch 32, recon_loss:0.794299, zinb_loss:0.651255, adversial_loss:1.338467\n",
      "Pretrain epoch 33, recon_loss:0.793691, zinb_loss:0.650216, adversial_loss:1.335549\n",
      "Pretrain epoch 34, recon_loss:0.792576, zinb_loss:0.648704, adversial_loss:1.335775\n",
      "Pretrain epoch 35, recon_loss:0.792066, zinb_loss:0.647766, adversial_loss:1.335634\n",
      "Pretrain epoch 36, recon_loss:0.791833, zinb_loss:0.647147, adversial_loss:1.333670\n",
      "Pretrain epoch 37, recon_loss:0.790898, zinb_loss:0.646018, adversial_loss:1.333558\n",
      "Pretrain epoch 38, recon_loss:0.790367, zinb_loss:0.645174, adversial_loss:1.333227\n",
      "Pretrain epoch 39, recon_loss:0.789837, zinb_loss:0.644639, adversial_loss:1.331793\n",
      "Pretrain epoch 40, recon_loss:0.789169, zinb_loss:0.643823, adversial_loss:1.332080\n",
      "Pretrain epoch 41, recon_loss:0.788582, zinb_loss:0.643115, adversial_loss:1.331233\n",
      "Pretrain epoch 42, recon_loss:0.788083, zinb_loss:0.642528, adversial_loss:1.329755\n",
      "Pretrain epoch 43, recon_loss:0.787743, zinb_loss:0.642012, adversial_loss:1.330270\n",
      "Pretrain epoch 44, recon_loss:0.787233, zinb_loss:0.641469, adversial_loss:1.329388\n",
      "Pretrain epoch 45, recon_loss:0.786937, zinb_loss:0.641088, adversial_loss:1.328254\n",
      "Pretrain epoch 46, recon_loss:0.786540, zinb_loss:0.640810, adversial_loss:1.328790\n",
      "Pretrain epoch 47, recon_loss:0.786258, zinb_loss:0.640202, adversial_loss:1.327587\n",
      "Pretrain epoch 48, recon_loss:0.785733, zinb_loss:0.639601, adversial_loss:1.327571\n",
      "Pretrain epoch 49, recon_loss:0.785445, zinb_loss:0.639265, adversial_loss:1.327048\n",
      "Pretrain epoch 50, recon_loss:0.785088, zinb_loss:0.638992, adversial_loss:1.325881\n",
      "Pretrain epoch 51, recon_loss:0.784727, zinb_loss:0.638529, adversial_loss:1.326414\n",
      "Pretrain epoch 52, recon_loss:0.784363, zinb_loss:0.638133, adversial_loss:1.325543\n",
      "Pretrain epoch 53, recon_loss:0.784103, zinb_loss:0.637940, adversial_loss:1.324549\n",
      "Pretrain epoch 54, recon_loss:0.783760, zinb_loss:0.637597, adversial_loss:1.325459\n",
      "Pretrain epoch 55, recon_loss:0.783446, zinb_loss:0.637262, adversial_loss:1.324351\n",
      "Pretrain epoch 56, recon_loss:0.783150, zinb_loss:0.637093, adversial_loss:1.324194\n",
      "Pretrain epoch 57, recon_loss:0.782939, zinb_loss:0.636965, adversial_loss:1.323673\n",
      "Pretrain epoch 58, recon_loss:0.782717, zinb_loss:0.636879, adversial_loss:1.324108\n",
      "Pretrain epoch 59, recon_loss:0.782628, zinb_loss:0.637086, adversial_loss:1.321680\n",
      "Pretrain epoch 60, recon_loss:0.782271, zinb_loss:0.636703, adversial_loss:1.323040\n",
      "Pretrain epoch 61, recon_loss:0.781838, zinb_loss:0.636147, adversial_loss:1.320967\n",
      "Pretrain epoch 62, recon_loss:0.781431, zinb_loss:0.635768, adversial_loss:1.320497\n",
      "Pretrain epoch 63, recon_loss:0.781378, zinb_loss:0.635902, adversial_loss:1.320936\n",
      "Pretrain epoch 64, recon_loss:0.781003, zinb_loss:0.635670, adversial_loss:1.319416\n",
      "Pretrain epoch 65, recon_loss:0.780600, zinb_loss:0.635297, adversial_loss:1.319215\n",
      "Pretrain epoch 66, recon_loss:0.780387, zinb_loss:0.635358, adversial_loss:1.319530\n",
      "Pretrain epoch 67, recon_loss:0.780119, zinb_loss:0.635134, adversial_loss:1.317977\n",
      "Pretrain epoch 68, recon_loss:0.779724, zinb_loss:0.634855, adversial_loss:1.317650\n",
      "Pretrain epoch 69, recon_loss:0.779647, zinb_loss:0.634964, adversial_loss:1.318024\n",
      "Pretrain epoch 70, recon_loss:0.779239, zinb_loss:0.634626, adversial_loss:1.316499\n",
      "Pretrain epoch 71, recon_loss:0.778984, zinb_loss:0.634477, adversial_loss:1.316283\n",
      "Pretrain epoch 72, recon_loss:0.778745, zinb_loss:0.634437, adversial_loss:1.316512\n",
      "Pretrain epoch 73, recon_loss:0.778422, zinb_loss:0.634171, adversial_loss:1.315362\n",
      "Pretrain epoch 74, recon_loss:0.778190, zinb_loss:0.634086, adversial_loss:1.314840\n",
      "Pretrain epoch 75, recon_loss:0.777988, zinb_loss:0.634006, adversial_loss:1.314906\n",
      "Pretrain epoch 76, recon_loss:0.777692, zinb_loss:0.633817, adversial_loss:1.314137\n",
      "Pretrain epoch 77, recon_loss:0.777451, zinb_loss:0.633776, adversial_loss:1.313697\n",
      "Pretrain epoch 78, recon_loss:0.777205, zinb_loss:0.633718, adversial_loss:1.313863\n",
      "Pretrain epoch 79, recon_loss:0.776992, zinb_loss:0.633652, adversial_loss:1.312968\n",
      "Pretrain epoch 80, recon_loss:0.776972, zinb_loss:0.633797, adversial_loss:1.312713\n",
      "Pretrain epoch 81, recon_loss:0.776964, zinb_loss:0.634136, adversial_loss:1.312556\n",
      "Pretrain epoch 82, recon_loss:0.777046, zinb_loss:0.634610, adversial_loss:1.311600\n",
      "Pretrain epoch 83, recon_loss:0.776634, zinb_loss:0.634098, adversial_loss:1.311629\n",
      "Pretrain epoch 84, recon_loss:0.775969, zinb_loss:0.633054, adversial_loss:1.311265\n",
      "Pretrain epoch 85, recon_loss:0.776029, zinb_loss:0.633389, adversial_loss:1.310533\n",
      "Pretrain epoch 86, recon_loss:0.775819, zinb_loss:0.633396, adversial_loss:1.310469\n",
      "Pretrain epoch 87, recon_loss:0.775534, zinb_loss:0.632921, adversial_loss:1.309945\n",
      "Pretrain epoch 88, recon_loss:0.775337, zinb_loss:0.632915, adversial_loss:1.309785\n",
      "Pretrain epoch 89, recon_loss:0.775196, zinb_loss:0.632826, adversial_loss:1.309403\n",
      "Pretrain epoch 90, recon_loss:0.775106, zinb_loss:0.632720, adversial_loss:1.308682\n",
      "Pretrain epoch 91, recon_loss:0.774817, zinb_loss:0.632539, adversial_loss:1.308952\n",
      "Pretrain epoch 92, recon_loss:0.774637, zinb_loss:0.632432, adversial_loss:1.308161\n",
      "Pretrain epoch 93, recon_loss:0.774580, zinb_loss:0.632434, adversial_loss:1.307136\n",
      "Pretrain epoch 94, recon_loss:0.774255, zinb_loss:0.632254, adversial_loss:1.307646\n",
      "Pretrain epoch 95, recon_loss:0.774129, zinb_loss:0.632149, adversial_loss:1.306916\n",
      "Pretrain epoch 96, recon_loss:0.773953, zinb_loss:0.632100, adversial_loss:1.306219\n",
      "Pretrain epoch 97, recon_loss:0.773769, zinb_loss:0.631987, adversial_loss:1.306318\n",
      "Pretrain epoch 98, recon_loss:0.773663, zinb_loss:0.631933, adversial_loss:1.305686\n",
      "Pretrain epoch 99, recon_loss:0.773468, zinb_loss:0.631802, adversial_loss:1.305677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 100, recon_loss:0.773266, zinb_loss:0.631716, adversial_loss:1.305090\n",
      "Pretrain epoch 101, recon_loss:0.773141, zinb_loss:0.631620, adversial_loss:1.305019\n",
      "Pretrain epoch 102, recon_loss:0.772991, zinb_loss:0.631532, adversial_loss:1.304864\n",
      "Pretrain epoch 103, recon_loss:0.772879, zinb_loss:0.631514, adversial_loss:1.303902\n",
      "Pretrain epoch 104, recon_loss:0.772725, zinb_loss:0.631433, adversial_loss:1.304510\n",
      "Pretrain epoch 105, recon_loss:0.772648, zinb_loss:0.631401, adversial_loss:1.303271\n",
      "Pretrain epoch 106, recon_loss:0.772535, zinb_loss:0.631453, adversial_loss:1.303749\n",
      "Pretrain epoch 107, recon_loss:0.772562, zinb_loss:0.631585, adversial_loss:1.302562\n",
      "Pretrain epoch 108, recon_loss:0.772743, zinb_loss:0.631804, adversial_loss:1.303632\n",
      "Pretrain epoch 109, recon_loss:0.773139, zinb_loss:0.632166, adversial_loss:1.301710\n",
      "Pretrain epoch 110, recon_loss:0.772615, zinb_loss:0.631620, adversial_loss:1.302210\n",
      "Pretrain epoch 111, recon_loss:0.772099, zinb_loss:0.631244, adversial_loss:1.302199\n",
      "Pretrain epoch 112, recon_loss:0.772691, zinb_loss:0.631441, adversial_loss:1.300339\n",
      "Pretrain epoch 113, recon_loss:0.771753, zinb_loss:0.630972, adversial_loss:1.301244\n",
      "Pretrain epoch 114, recon_loss:0.771948, zinb_loss:0.631223, adversial_loss:1.301844\n",
      "Pretrain epoch 115, recon_loss:0.771766, zinb_loss:0.630892, adversial_loss:1.300058\n",
      "Pretrain epoch 116, recon_loss:0.771387, zinb_loss:0.630823, adversial_loss:1.300245\n",
      "Pretrain epoch 117, recon_loss:0.771327, zinb_loss:0.630725, adversial_loss:1.300648\n",
      "Pretrain epoch 118, recon_loss:0.770896, zinb_loss:0.630538, adversial_loss:1.299725\n",
      "Pretrain epoch 119, recon_loss:0.771083, zinb_loss:0.630620, adversial_loss:1.299314\n",
      "Pretrain epoch 120, recon_loss:0.770678, zinb_loss:0.630338, adversial_loss:1.298796\n",
      "Pretrain epoch 121, recon_loss:0.770595, zinb_loss:0.630327, adversial_loss:1.299041\n",
      "Pretrain epoch 122, recon_loss:0.770426, zinb_loss:0.630228, adversial_loss:1.298796\n",
      "Pretrain epoch 123, recon_loss:0.770400, zinb_loss:0.630227, adversial_loss:1.297851\n",
      "Pretrain epoch 124, recon_loss:0.769996, zinb_loss:0.630014, adversial_loss:1.297896\n",
      "Pretrain epoch 125, recon_loss:0.770002, zinb_loss:0.630018, adversial_loss:1.297887\n",
      "Pretrain epoch 126, recon_loss:0.769764, zinb_loss:0.629883, adversial_loss:1.297321\n",
      "Pretrain epoch 127, recon_loss:0.769703, zinb_loss:0.629920, adversial_loss:1.297207\n",
      "Pretrain epoch 128, recon_loss:0.769471, zinb_loss:0.629776, adversial_loss:1.296510\n",
      "Pretrain epoch 129, recon_loss:0.769474, zinb_loss:0.629844, adversial_loss:1.296746\n",
      "Pretrain epoch 130, recon_loss:0.769316, zinb_loss:0.629907, adversial_loss:1.296185\n",
      "Pretrain epoch 131, recon_loss:0.769345, zinb_loss:0.630119, adversial_loss:1.296040\n",
      "Pretrain epoch 132, recon_loss:0.769921, zinb_loss:0.630883, adversial_loss:1.295672\n",
      "Pretrain epoch 133, recon_loss:0.769987, zinb_loss:0.631382, adversial_loss:1.295606\n",
      "Pretrain epoch 134, recon_loss:0.770319, zinb_loss:0.630958, adversial_loss:1.295570\n",
      "Pretrain epoch 135, recon_loss:0.769544, zinb_loss:0.630117, adversial_loss:1.293905\n",
      "Pretrain epoch 136, recon_loss:0.769243, zinb_loss:0.630311, adversial_loss:1.294124\n",
      "Pretrain epoch 137, recon_loss:0.769668, zinb_loss:0.630274, adversial_loss:1.294208\n",
      "Pretrain epoch 138, recon_loss:0.768874, zinb_loss:0.629551, adversial_loss:1.293011\n",
      "Pretrain epoch 139, recon_loss:0.769396, zinb_loss:0.630022, adversial_loss:1.292869\n",
      "Pretrain epoch 140, recon_loss:0.768710, zinb_loss:0.629450, adversial_loss:1.292766\n",
      "Pretrain epoch 141, recon_loss:0.768588, zinb_loss:0.629526, adversial_loss:1.292793\n",
      "Pretrain epoch 142, recon_loss:0.768435, zinb_loss:0.629493, adversial_loss:1.291975\n",
      "Pretrain epoch 143, recon_loss:0.768048, zinb_loss:0.629232, adversial_loss:1.291733\n",
      "Pretrain epoch 144, recon_loss:0.767937, zinb_loss:0.629348, adversial_loss:1.292000\n",
      "Pretrain epoch 145, recon_loss:0.767733, zinb_loss:0.629056, adversial_loss:1.291455\n",
      "Pretrain epoch 146, recon_loss:0.767740, zinb_loss:0.629110, adversial_loss:1.290631\n",
      "Pretrain epoch 147, recon_loss:0.767510, zinb_loss:0.628934, adversial_loss:1.290836\n",
      "Pretrain epoch 148, recon_loss:0.767375, zinb_loss:0.628849, adversial_loss:1.291117\n",
      "Pretrain epoch 149, recon_loss:0.767231, zinb_loss:0.628824, adversial_loss:1.290537\n",
      "Pretrain epoch 150, recon_loss:0.767011, zinb_loss:0.628681, adversial_loss:1.289755\n",
      "Pretrain epoch 151, recon_loss:0.766930, zinb_loss:0.628695, adversial_loss:1.289879\n",
      "Pretrain epoch 152, recon_loss:0.766698, zinb_loss:0.628563, adversial_loss:1.289621\n",
      "Pretrain epoch 153, recon_loss:0.766584, zinb_loss:0.628529, adversial_loss:1.289369\n",
      "Pretrain epoch 154, recon_loss:0.766406, zinb_loss:0.628483, adversial_loss:1.288932\n",
      "Pretrain epoch 155, recon_loss:0.766286, zinb_loss:0.628409, adversial_loss:1.288240\n",
      "Pretrain epoch 156, recon_loss:0.766156, zinb_loss:0.628336, adversial_loss:1.287966\n",
      "Pretrain epoch 157, recon_loss:0.765973, zinb_loss:0.628333, adversial_loss:1.288143\n",
      "Pretrain epoch 158, recon_loss:0.765870, zinb_loss:0.628206, adversial_loss:1.287627\n",
      "Pretrain epoch 159, recon_loss:0.765755, zinb_loss:0.628209, adversial_loss:1.287160\n",
      "Pretrain epoch 160, recon_loss:0.765601, zinb_loss:0.628156, adversial_loss:1.286916\n",
      "Pretrain epoch 161, recon_loss:0.765482, zinb_loss:0.628089, adversial_loss:1.286921\n",
      "Pretrain epoch 162, recon_loss:0.765553, zinb_loss:0.628178, adversial_loss:1.285903\n",
      "Pretrain epoch 163, recon_loss:0.765706, zinb_loss:0.628333, adversial_loss:1.286662\n",
      "Pretrain epoch 164, recon_loss:0.766546, zinb_loss:0.629138, adversial_loss:1.285251\n",
      "Pretrain epoch 165, recon_loss:0.767097, zinb_loss:0.629627, adversial_loss:1.285976\n",
      "Pretrain epoch 166, recon_loss:0.766208, zinb_loss:0.628702, adversial_loss:1.284390\n",
      "Pretrain epoch 167, recon_loss:0.765260, zinb_loss:0.628191, adversial_loss:1.283949\n",
      "Pretrain epoch 168, recon_loss:0.766244, zinb_loss:0.628701, adversial_loss:1.284614\n",
      "Pretrain epoch 169, recon_loss:0.765279, zinb_loss:0.628139, adversial_loss:1.283516\n",
      "Pretrain epoch 170, recon_loss:0.765658, zinb_loss:0.628244, adversial_loss:1.283159\n",
      "Pretrain epoch 171, recon_loss:0.765288, zinb_loss:0.628052, adversial_loss:1.283587\n",
      "Pretrain epoch 172, recon_loss:0.764857, zinb_loss:0.627844, adversial_loss:1.283251\n",
      "Pretrain epoch 173, recon_loss:0.765260, zinb_loss:0.628118, adversial_loss:1.282436\n",
      "Pretrain epoch 174, recon_loss:0.764392, zinb_loss:0.627717, adversial_loss:1.282285\n",
      "Pretrain epoch 175, recon_loss:0.764773, zinb_loss:0.627935, adversial_loss:1.282586\n",
      "Pretrain epoch 176, recon_loss:0.764257, zinb_loss:0.627658, adversial_loss:1.281763\n",
      "Pretrain epoch 177, recon_loss:0.764276, zinb_loss:0.627682, adversial_loss:1.281396\n",
      "Pretrain epoch 178, recon_loss:0.764062, zinb_loss:0.627596, adversial_loss:1.281657\n",
      "Pretrain epoch 179, recon_loss:0.763986, zinb_loss:0.627492, adversial_loss:1.281358\n",
      "Pretrain epoch 180, recon_loss:0.763848, zinb_loss:0.627575, adversial_loss:1.280781\n",
      "Pretrain epoch 181, recon_loss:0.763658, zinb_loss:0.627387, adversial_loss:1.280916\n",
      "Pretrain epoch 182, recon_loss:0.763669, zinb_loss:0.627456, adversial_loss:1.280822\n",
      "Pretrain epoch 183, recon_loss:0.763358, zinb_loss:0.627285, adversial_loss:1.280413\n",
      "Pretrain epoch 184, recon_loss:0.763430, zinb_loss:0.627338, adversial_loss:1.279923\n",
      "Pretrain epoch 185, recon_loss:0.763196, zinb_loss:0.627269, adversial_loss:1.279783\n",
      "Pretrain epoch 186, recon_loss:0.763281, zinb_loss:0.627313, adversial_loss:1.280205\n",
      "Pretrain epoch 187, recon_loss:0.763191, zinb_loss:0.627381, adversial_loss:1.279038\n",
      "Pretrain epoch 188, recon_loss:0.763060, zinb_loss:0.627429, adversial_loss:1.278970\n",
      "Pretrain epoch 189, recon_loss:0.763346, zinb_loss:0.627577, adversial_loss:1.279608\n",
      "Pretrain epoch 190, recon_loss:0.762973, zinb_loss:0.627280, adversial_loss:1.278903\n",
      "Pretrain epoch 191, recon_loss:0.763026, zinb_loss:0.627135, adversial_loss:1.278417\n",
      "Pretrain epoch 192, recon_loss:0.762511, zinb_loss:0.626993, adversial_loss:1.278164\n",
      "Pretrain epoch 193, recon_loss:0.762645, zinb_loss:0.627139, adversial_loss:1.278583\n",
      "Pretrain epoch 194, recon_loss:0.762589, zinb_loss:0.627038, adversial_loss:1.277658\n",
      "Pretrain epoch 195, recon_loss:0.762198, zinb_loss:0.626901, adversial_loss:1.277149\n",
      "Pretrain epoch 196, recon_loss:0.762336, zinb_loss:0.626877, adversial_loss:1.277972\n",
      "Pretrain epoch 197, recon_loss:0.762059, zinb_loss:0.626925, adversial_loss:1.276962\n",
      "Pretrain epoch 198, recon_loss:0.761929, zinb_loss:0.626830, adversial_loss:1.276672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 199, recon_loss:0.761676, zinb_loss:0.626703, adversial_loss:1.276777\n",
      "Pretrain epoch 200, recon_loss:0.761598, zinb_loss:0.626732, adversial_loss:1.276787\n",
      "Pretrain epoch 201, recon_loss:0.761551, zinb_loss:0.626729, adversial_loss:1.276282\n",
      "Pretrain epoch 202, recon_loss:0.761438, zinb_loss:0.626608, adversial_loss:1.275815\n",
      "Pretrain epoch 203, recon_loss:0.761329, zinb_loss:0.626583, adversial_loss:1.276529\n",
      "Pretrain epoch 204, recon_loss:0.761202, zinb_loss:0.626573, adversial_loss:1.275519\n",
      "Pretrain epoch 205, recon_loss:0.761257, zinb_loss:0.626585, adversial_loss:1.275652\n",
      "Pretrain epoch 206, recon_loss:0.761152, zinb_loss:0.626577, adversial_loss:1.275099\n",
      "Pretrain epoch 207, recon_loss:0.761234, zinb_loss:0.626690, adversial_loss:1.275626\n",
      "Pretrain epoch 208, recon_loss:0.761665, zinb_loss:0.627107, adversial_loss:1.273614\n",
      "Pretrain epoch 209, recon_loss:0.761999, zinb_loss:0.627263, adversial_loss:1.275154\n",
      "Pretrain epoch 210, recon_loss:0.761713, zinb_loss:0.627031, adversial_loss:1.273631\n",
      "Pretrain epoch 211, recon_loss:0.760860, zinb_loss:0.626554, adversial_loss:1.273066\n",
      "Pretrain epoch 212, recon_loss:0.761342, zinb_loss:0.626922, adversial_loss:1.274019\n",
      "Pretrain epoch 213, recon_loss:0.762381, zinb_loss:0.627192, adversial_loss:1.272925\n",
      "Pretrain epoch 214, recon_loss:0.761405, zinb_loss:0.626692, adversial_loss:1.273736\n",
      "Pretrain epoch 215, recon_loss:0.761456, zinb_loss:0.626911, adversial_loss:1.272705\n",
      "Pretrain epoch 216, recon_loss:0.760689, zinb_loss:0.626513, adversial_loss:1.272339\n",
      "Pretrain epoch 217, recon_loss:0.760589, zinb_loss:0.626463, adversial_loss:1.273166\n",
      "Pretrain epoch 218, recon_loss:0.760831, zinb_loss:0.626679, adversial_loss:1.272440\n",
      "Pretrain epoch 219, recon_loss:0.760364, zinb_loss:0.626378, adversial_loss:1.271747\n",
      "Pretrain epoch 220, recon_loss:0.760391, zinb_loss:0.626402, adversial_loss:1.272083\n",
      "Pretrain epoch 221, recon_loss:0.759937, zinb_loss:0.626285, adversial_loss:1.272450\n",
      "Pretrain epoch 222, recon_loss:0.760152, zinb_loss:0.626306, adversial_loss:1.271766\n",
      "Pretrain epoch 223, recon_loss:0.759920, zinb_loss:0.626314, adversial_loss:1.271065\n",
      "Pretrain epoch 224, recon_loss:0.759479, zinb_loss:0.626046, adversial_loss:1.271957\n",
      "Pretrain epoch 225, recon_loss:0.759670, zinb_loss:0.626167, adversial_loss:1.271603\n",
      "Pretrain epoch 226, recon_loss:0.759341, zinb_loss:0.626084, adversial_loss:1.270462\n",
      "Pretrain epoch 227, recon_loss:0.759355, zinb_loss:0.626023, adversial_loss:1.271136\n",
      "Pretrain epoch 228, recon_loss:0.758976, zinb_loss:0.625971, adversial_loss:1.271234\n",
      "Pretrain epoch 229, recon_loss:0.759075, zinb_loss:0.625943, adversial_loss:1.270373\n",
      "Pretrain epoch 230, recon_loss:0.758902, zinb_loss:0.625946, adversial_loss:1.270608\n",
      "Pretrain epoch 231, recon_loss:0.758847, zinb_loss:0.625828, adversial_loss:1.270542\n",
      "Pretrain epoch 232, recon_loss:0.758621, zinb_loss:0.625816, adversial_loss:1.270036\n",
      "Pretrain epoch 233, recon_loss:0.758484, zinb_loss:0.625754, adversial_loss:1.269886\n",
      "Pretrain epoch 234, recon_loss:0.758580, zinb_loss:0.625754, adversial_loss:1.269803\n",
      "Pretrain epoch 235, recon_loss:0.758282, zinb_loss:0.625766, adversial_loss:1.269494\n",
      "Pretrain epoch 236, recon_loss:0.758247, zinb_loss:0.625758, adversial_loss:1.269161\n",
      "Pretrain epoch 237, recon_loss:0.758274, zinb_loss:0.625919, adversial_loss:1.269464\n",
      "Pretrain epoch 238, recon_loss:0.758416, zinb_loss:0.626123, adversial_loss:1.269094\n",
      "Pretrain epoch 239, recon_loss:0.759425, zinb_loss:0.627000, adversial_loss:1.268666\n",
      "Pretrain epoch 240, recon_loss:0.759433, zinb_loss:0.627312, adversial_loss:1.269424\n",
      "Pretrain epoch 241, recon_loss:0.758946, zinb_loss:0.626515, adversial_loss:1.268044\n",
      "Pretrain epoch 242, recon_loss:0.758398, zinb_loss:0.625859, adversial_loss:1.267596\n",
      "Pretrain epoch 243, recon_loss:0.758400, zinb_loss:0.626189, adversial_loss:1.267879\n",
      "Pretrain epoch 244, recon_loss:0.758251, zinb_loss:0.625972, adversial_loss:1.267945\n",
      "Pretrain epoch 245, recon_loss:0.757926, zinb_loss:0.625754, adversial_loss:1.267376\n",
      "Pretrain epoch 246, recon_loss:0.758450, zinb_loss:0.625991, adversial_loss:1.267111\n",
      "Pretrain epoch 247, recon_loss:0.757817, zinb_loss:0.625638, adversial_loss:1.267705\n",
      "Pretrain epoch 248, recon_loss:0.757740, zinb_loss:0.625709, adversial_loss:1.267026\n",
      "Pretrain epoch 249, recon_loss:0.757581, zinb_loss:0.625628, adversial_loss:1.266837\n",
      "Pretrain epoch 250, recon_loss:0.757324, zinb_loss:0.625543, adversial_loss:1.267304\n",
      "Pretrain epoch 251, recon_loss:0.757314, zinb_loss:0.625634, adversial_loss:1.266793\n",
      "Pretrain epoch 252, recon_loss:0.756889, zinb_loss:0.625389, adversial_loss:1.266628\n",
      "Pretrain epoch 253, recon_loss:0.756935, zinb_loss:0.625487, adversial_loss:1.266536\n",
      "Pretrain epoch 254, recon_loss:0.756694, zinb_loss:0.625405, adversial_loss:1.266254\n",
      "Pretrain epoch 255, recon_loss:0.756546, zinb_loss:0.625338, adversial_loss:1.265859\n",
      "Pretrain epoch 256, recon_loss:0.756459, zinb_loss:0.625372, adversial_loss:1.265898\n",
      "Pretrain epoch 257, recon_loss:0.756208, zinb_loss:0.625240, adversial_loss:1.265420\n",
      "Pretrain epoch 258, recon_loss:0.756110, zinb_loss:0.625279, adversial_loss:1.265282\n",
      "Pretrain epoch 259, recon_loss:0.755972, zinb_loss:0.625202, adversial_loss:1.264920\n",
      "Pretrain epoch 260, recon_loss:0.755845, zinb_loss:0.625181, adversial_loss:1.264856\n",
      "Pretrain epoch 261, recon_loss:0.755776, zinb_loss:0.625212, adversial_loss:1.264385\n",
      "Pretrain epoch 262, recon_loss:0.755658, zinb_loss:0.625171, adversial_loss:1.264643\n",
      "Pretrain epoch 263, recon_loss:0.755740, zinb_loss:0.625268, adversial_loss:1.263762\n",
      "Pretrain epoch 264, recon_loss:0.755793, zinb_loss:0.625322, adversial_loss:1.264693\n",
      "Pretrain epoch 265, recon_loss:0.756047, zinb_loss:0.625576, adversial_loss:1.263003\n",
      "Pretrain epoch 266, recon_loss:0.756053, zinb_loss:0.625506, adversial_loss:1.264240\n",
      "Pretrain epoch 267, recon_loss:0.755714, zinb_loss:0.625297, adversial_loss:1.262850\n",
      "Pretrain epoch 268, recon_loss:0.755153, zinb_loss:0.625064, adversial_loss:1.263153\n",
      "Pretrain epoch 269, recon_loss:0.755243, zinb_loss:0.625092, adversial_loss:1.263150\n",
      "Pretrain epoch 270, recon_loss:0.755230, zinb_loss:0.625185, adversial_loss:1.262389\n",
      "Pretrain epoch 271, recon_loss:0.755256, zinb_loss:0.625203, adversial_loss:1.262950\n",
      "Pretrain epoch 272, recon_loss:0.755542, zinb_loss:0.625343, adversial_loss:1.262252\n",
      "Pretrain epoch 273, recon_loss:0.756221, zinb_loss:0.625609, adversial_loss:1.262175\n",
      "Pretrain epoch 274, recon_loss:0.757112, zinb_loss:0.625953, adversial_loss:1.262888\n",
      "Pretrain epoch 275, recon_loss:0.757585, zinb_loss:0.626125, adversial_loss:1.261826\n",
      "Pretrain epoch 276, recon_loss:0.755297, zinb_loss:0.625102, adversial_loss:1.261289\n",
      "Pretrain epoch 277, recon_loss:0.755384, zinb_loss:0.625368, adversial_loss:1.261689\n",
      "Pretrain epoch 278, recon_loss:0.756129, zinb_loss:0.625459, adversial_loss:1.260738\n",
      "Pretrain epoch 279, recon_loss:0.754724, zinb_loss:0.625007, adversial_loss:1.260598\n",
      "Pretrain epoch 280, recon_loss:0.755369, zinb_loss:0.625379, adversial_loss:1.261417\n",
      "Pretrain epoch 281, recon_loss:0.754782, zinb_loss:0.624991, adversial_loss:1.260518\n",
      "Pretrain epoch 282, recon_loss:0.754534, zinb_loss:0.625051, adversial_loss:1.259917\n",
      "Pretrain epoch 283, recon_loss:0.754491, zinb_loss:0.625008, adversial_loss:1.260628\n",
      "Pretrain epoch 284, recon_loss:0.754288, zinb_loss:0.624848, adversial_loss:1.260599\n",
      "Pretrain epoch 285, recon_loss:0.754080, zinb_loss:0.624923, adversial_loss:1.259742\n",
      "Pretrain epoch 286, recon_loss:0.753993, zinb_loss:0.624777, adversial_loss:1.259850\n",
      "Pretrain epoch 287, recon_loss:0.753858, zinb_loss:0.624777, adversial_loss:1.260215\n",
      "Pretrain epoch 288, recon_loss:0.753481, zinb_loss:0.624721, adversial_loss:1.259990\n",
      "Pretrain epoch 289, recon_loss:0.753600, zinb_loss:0.624683, adversial_loss:1.259371\n",
      "Pretrain epoch 290, recon_loss:0.753198, zinb_loss:0.624659, adversial_loss:1.259436\n",
      "Pretrain epoch 291, recon_loss:0.753150, zinb_loss:0.624620, adversial_loss:1.259738\n",
      "Pretrain epoch 292, recon_loss:0.753045, zinb_loss:0.624606, adversial_loss:1.259176\n",
      "Pretrain epoch 293, recon_loss:0.752695, zinb_loss:0.624578, adversial_loss:1.258881\n",
      "Pretrain epoch 294, recon_loss:0.752711, zinb_loss:0.624557, adversial_loss:1.259076\n",
      "Pretrain epoch 295, recon_loss:0.752386, zinb_loss:0.624516, adversial_loss:1.258735\n",
      "Pretrain epoch 296, recon_loss:0.752381, zinb_loss:0.624503, adversial_loss:1.258430\n",
      "Pretrain epoch 297, recon_loss:0.752095, zinb_loss:0.624473, adversial_loss:1.258393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 298, recon_loss:0.752020, zinb_loss:0.624451, adversial_loss:1.258269\n",
      "Pretrain epoch 299, recon_loss:0.751884, zinb_loss:0.624454, adversial_loss:1.257985\n",
      "Pretrain epoch 300, recon_loss:0.751720, zinb_loss:0.624407, adversial_loss:1.257759\n",
      "Pretrain epoch 301, recon_loss:0.751619, zinb_loss:0.624393, adversial_loss:1.257720\n",
      "Pretrain epoch 302, recon_loss:0.751603, zinb_loss:0.624389, adversial_loss:1.257465\n",
      "Pretrain epoch 303, recon_loss:0.751616, zinb_loss:0.624417, adversial_loss:1.257201\n",
      "Pretrain epoch 304, recon_loss:0.751976, zinb_loss:0.624573, adversial_loss:1.257134\n",
      "Pretrain epoch 305, recon_loss:0.753156, zinb_loss:0.625050, adversial_loss:1.257288\n",
      "Pretrain epoch 306, recon_loss:0.754855, zinb_loss:0.625786, adversial_loss:1.256867\n",
      "Pretrain epoch 307, recon_loss:0.754816, zinb_loss:0.626012, adversial_loss:1.256853\n",
      "Pretrain epoch 308, recon_loss:0.751676, zinb_loss:0.624669, adversial_loss:1.256409\n",
      "Pretrain epoch 309, recon_loss:0.752470, zinb_loss:0.624667, adversial_loss:1.256081\n",
      "Pretrain epoch 310, recon_loss:0.753001, zinb_loss:0.625094, adversial_loss:1.256042\n",
      "Pretrain epoch 311, recon_loss:0.751207, zinb_loss:0.624419, adversial_loss:1.255652\n",
      "Pretrain epoch 312, recon_loss:0.752243, zinb_loss:0.624646, adversial_loss:1.255806\n",
      "Pretrain epoch 313, recon_loss:0.751611, zinb_loss:0.624558, adversial_loss:1.255658\n",
      "Pretrain epoch 314, recon_loss:0.751132, zinb_loss:0.624370, adversial_loss:1.255069\n",
      "Pretrain epoch 315, recon_loss:0.751409, zinb_loss:0.624456, adversial_loss:1.255266\n",
      "Pretrain epoch 316, recon_loss:0.750799, zinb_loss:0.624303, adversial_loss:1.255428\n",
      "Pretrain epoch 317, recon_loss:0.750899, zinb_loss:0.624351, adversial_loss:1.254581\n",
      "Pretrain epoch 318, recon_loss:0.750526, zinb_loss:0.624260, adversial_loss:1.254706\n",
      "Pretrain epoch 319, recon_loss:0.750601, zinb_loss:0.624306, adversial_loss:1.255149\n",
      "Pretrain epoch 320, recon_loss:0.750154, zinb_loss:0.624208, adversial_loss:1.254608\n",
      "Pretrain epoch 321, recon_loss:0.750454, zinb_loss:0.624261, adversial_loss:1.253649\n",
      "Pretrain epoch 322, recon_loss:0.750311, zinb_loss:0.624279, adversial_loss:1.255018\n",
      "Pretrain epoch 323, recon_loss:0.750246, zinb_loss:0.624426, adversial_loss:1.253833\n",
      "Pretrain epoch 324, recon_loss:0.750702, zinb_loss:0.624627, adversial_loss:1.253942\n",
      "Pretrain epoch 325, recon_loss:0.750848, zinb_loss:0.624686, adversial_loss:1.253212\n",
      "Pretrain epoch 326, recon_loss:0.750545, zinb_loss:0.624489, adversial_loss:1.254483\n",
      "Pretrain epoch 327, recon_loss:0.749625, zinb_loss:0.624184, adversial_loss:1.252910\n",
      "Pretrain epoch 328, recon_loss:0.749357, zinb_loss:0.624110, adversial_loss:1.252911\n",
      "Pretrain epoch 329, recon_loss:0.749682, zinb_loss:0.624217, adversial_loss:1.253526\n",
      "Pretrain epoch 330, recon_loss:0.749361, zinb_loss:0.624243, adversial_loss:1.252541\n",
      "Pretrain epoch 331, recon_loss:0.749046, zinb_loss:0.624085, adversial_loss:1.252763\n",
      "Pretrain epoch 332, recon_loss:0.748671, zinb_loss:0.623996, adversial_loss:1.252374\n",
      "Pretrain epoch 333, recon_loss:0.748825, zinb_loss:0.624106, adversial_loss:1.252345\n",
      "Pretrain epoch 334, recon_loss:0.748693, zinb_loss:0.624035, adversial_loss:1.252432\n",
      "Pretrain epoch 335, recon_loss:0.748310, zinb_loss:0.623969, adversial_loss:1.251544\n",
      "Pretrain epoch 336, recon_loss:0.748171, zinb_loss:0.623935, adversial_loss:1.252011\n",
      "Pretrain epoch 337, recon_loss:0.748107, zinb_loss:0.623957, adversial_loss:1.252016\n",
      "Pretrain epoch 338, recon_loss:0.748038, zinb_loss:0.623936, adversial_loss:1.250951\n",
      "Pretrain epoch 339, recon_loss:0.747790, zinb_loss:0.623863, adversial_loss:1.251737\n",
      "Pretrain epoch 340, recon_loss:0.747788, zinb_loss:0.623857, adversial_loss:1.251112\n",
      "Pretrain epoch 341, recon_loss:0.747594, zinb_loss:0.623835, adversial_loss:1.250883\n",
      "Pretrain epoch 342, recon_loss:0.747351, zinb_loss:0.623804, adversial_loss:1.250976\n",
      "Pretrain epoch 343, recon_loss:0.747299, zinb_loss:0.623808, adversial_loss:1.250553\n",
      "Pretrain epoch 344, recon_loss:0.747291, zinb_loss:0.623843, adversial_loss:1.250751\n",
      "Pretrain epoch 345, recon_loss:0.747353, zinb_loss:0.623880, adversial_loss:1.250244\n",
      "Pretrain epoch 346, recon_loss:0.747712, zinb_loss:0.623967, adversial_loss:1.250278\n",
      "Pretrain epoch 347, recon_loss:0.748533, zinb_loss:0.624176, adversial_loss:1.250290\n",
      "Pretrain epoch 348, recon_loss:0.749864, zinb_loss:0.624498, adversial_loss:1.249875\n",
      "Pretrain epoch 349, recon_loss:0.749792, zinb_loss:0.624604, adversial_loss:1.249688\n",
      "Pretrain epoch 350, recon_loss:0.747748, zinb_loss:0.624104, adversial_loss:1.249543\n",
      "Pretrain epoch 351, recon_loss:0.747056, zinb_loss:0.623864, adversial_loss:1.248984\n",
      "Pretrain epoch 352, recon_loss:0.748128, zinb_loss:0.624133, adversial_loss:1.249011\n",
      "Pretrain epoch 353, recon_loss:0.747624, zinb_loss:0.624018, adversial_loss:1.249153\n",
      "Pretrain epoch 354, recon_loss:0.746567, zinb_loss:0.623769, adversial_loss:1.248775\n",
      "Pretrain epoch 355, recon_loss:0.746866, zinb_loss:0.623964, adversial_loss:1.248478\n",
      "Pretrain epoch 356, recon_loss:0.746527, zinb_loss:0.623889, adversial_loss:1.248987\n",
      "Pretrain epoch 357, recon_loss:0.746377, zinb_loss:0.623701, adversial_loss:1.248239\n",
      "Pretrain epoch 358, recon_loss:0.746282, zinb_loss:0.623742, adversial_loss:1.248244\n",
      "Pretrain epoch 359, recon_loss:0.746046, zinb_loss:0.623740, adversial_loss:1.248407\n",
      "Pretrain epoch 360, recon_loss:0.745941, zinb_loss:0.623657, adversial_loss:1.248133\n",
      "Pretrain epoch 361, recon_loss:0.745735, zinb_loss:0.623707, adversial_loss:1.247800\n",
      "Pretrain epoch 362, recon_loss:0.745730, zinb_loss:0.623680, adversial_loss:1.248000\n",
      "Pretrain epoch 363, recon_loss:0.745833, zinb_loss:0.623627, adversial_loss:1.247833\n",
      "Pretrain epoch 364, recon_loss:0.745583, zinb_loss:0.623639, adversial_loss:1.247455\n",
      "Pretrain epoch 365, recon_loss:0.745557, zinb_loss:0.623658, adversial_loss:1.247508\n",
      "Pretrain epoch 366, recon_loss:0.745358, zinb_loss:0.623622, adversial_loss:1.247385\n",
      "Pretrain epoch 367, recon_loss:0.745166, zinb_loss:0.623627, adversial_loss:1.247007\n",
      "Pretrain epoch 368, recon_loss:0.745114, zinb_loss:0.623602, adversial_loss:1.247067\n",
      "Pretrain epoch 369, recon_loss:0.745092, zinb_loss:0.623547, adversial_loss:1.246902\n",
      "Pretrain epoch 370, recon_loss:0.744758, zinb_loss:0.623542, adversial_loss:1.246693\n",
      "Pretrain epoch 371, recon_loss:0.745036, zinb_loss:0.623572, adversial_loss:1.246650\n",
      "Pretrain epoch 372, recon_loss:0.744833, zinb_loss:0.623528, adversial_loss:1.246337\n",
      "Pretrain epoch 373, recon_loss:0.744253, zinb_loss:0.623511, adversial_loss:1.246203\n",
      "Pretrain epoch 374, recon_loss:0.744213, zinb_loss:0.623551, adversial_loss:1.246197\n",
      "Pretrain epoch 375, recon_loss:0.744185, zinb_loss:0.623590, adversial_loss:1.245750\n",
      "Pretrain epoch 376, recon_loss:0.744927, zinb_loss:0.623700, adversial_loss:1.245964\n",
      "Pretrain epoch 377, recon_loss:0.745542, zinb_loss:0.623854, adversial_loss:1.245949\n",
      "Pretrain epoch 378, recon_loss:0.746172, zinb_loss:0.624132, adversial_loss:1.245143\n",
      "Pretrain epoch 379, recon_loss:0.745374, zinb_loss:0.624021, adversial_loss:1.246134\n",
      "Pretrain epoch 380, recon_loss:0.744613, zinb_loss:0.623714, adversial_loss:1.244378\n",
      "Pretrain epoch 381, recon_loss:0.745395, zinb_loss:0.623964, adversial_loss:1.245559\n",
      "Pretrain epoch 382, recon_loss:0.745601, zinb_loss:0.624205, adversial_loss:1.244211\n",
      "Pretrain epoch 383, recon_loss:0.744020, zinb_loss:0.623674, adversial_loss:1.245310\n",
      "Pretrain epoch 384, recon_loss:0.744484, zinb_loss:0.623604, adversial_loss:1.244278\n",
      "Pretrain epoch 385, recon_loss:0.744280, zinb_loss:0.623744, adversial_loss:1.244271\n",
      "Pretrain epoch 386, recon_loss:0.744052, zinb_loss:0.623587, adversial_loss:1.244613\n",
      "Pretrain epoch 387, recon_loss:0.743644, zinb_loss:0.623512, adversial_loss:1.244011\n",
      "Pretrain epoch 388, recon_loss:0.743463, zinb_loss:0.623467, adversial_loss:1.243936\n",
      "Pretrain epoch 389, recon_loss:0.743408, zinb_loss:0.623382, adversial_loss:1.244285\n",
      "Pretrain epoch 390, recon_loss:0.742802, zinb_loss:0.623369, adversial_loss:1.243493\n",
      "Pretrain epoch 391, recon_loss:0.742870, zinb_loss:0.623338, adversial_loss:1.243549\n",
      "Pretrain epoch 392, recon_loss:0.742498, zinb_loss:0.623299, adversial_loss:1.243875\n",
      "Pretrain epoch 393, recon_loss:0.742168, zinb_loss:0.623262, adversial_loss:1.243256\n",
      "Pretrain epoch 394, recon_loss:0.742424, zinb_loss:0.623246, adversial_loss:1.243256\n",
      "Pretrain epoch 395, recon_loss:0.741983, zinb_loss:0.623228, adversial_loss:1.243176\n",
      "Pretrain epoch 396, recon_loss:0.741756, zinb_loss:0.623253, adversial_loss:1.243132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 397, recon_loss:0.742005, zinb_loss:0.623249, adversial_loss:1.242527\n",
      "Pretrain epoch 398, recon_loss:0.742020, zinb_loss:0.623239, adversial_loss:1.243070\n",
      "Pretrain epoch 399, recon_loss:0.742514, zinb_loss:0.623436, adversial_loss:1.241964\n",
      "Pretrain epoch 400, recon_loss:0.743174, zinb_loss:0.623717, adversial_loss:1.243144\n"
     ]
    }
   ],
   "source": [
    "pretrain_latent = model.pretrain_autoencoder(\n",
    "                        X1=adata1.X, X2=adata2.X, X1_raw=adata1.raw.X, X2_raw=adata2.raw.X, \n",
    "                        epochs=400, file='GSM4949911_tea',ad_out=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886af0cf",
   "metadata": {},
   "source": [
    "When the parameter n_cluster is set to -1, scMAGCA will use GetCluster to estimate the number of clusters according to the resolution parameter and the potential representation obtained by pre-training, and use ASW, DB index and CH value to evaluate the clustering results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7835d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stage\n",
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: ASW= 0.4081, DB= 0.9771, CH= 4057.4425\n",
      "Training epoch 1, recon_loss:0.744567, zinb_loss:0.624116, cluster_loss:0.184134\n",
      "Clustering   1: ASW= 0.4081, DB= 0.9771, CH= 4057.4425\n",
      "Training epoch 2, recon_loss:0.781684, zinb_loss:0.635306, cluster_loss:0.192307\n",
      "Clustering   2: ASW= 0.5035, DB= 0.7872, CH= 7048.4868\n",
      "Training epoch 3, recon_loss:0.781879, zinb_loss:0.638056, cluster_loss:0.199501\n",
      "Clustering   3: ASW= 0.5203, DB= 0.7576, CH= 7181.4846\n",
      "Training epoch 4, recon_loss:0.802478, zinb_loss:0.644835, cluster_loss:0.204655\n",
      "Clustering   4: ASW= 0.5511, DB= 0.7052, CH= 9237.8673\n",
      "Training epoch 5, recon_loss:0.788960, zinb_loss:0.637543, cluster_loss:0.195610\n",
      "Clustering   5: ASW= 0.5629, DB= 0.6762, CH= 9559.8021\n",
      "Training epoch 6, recon_loss:0.791756, zinb_loss:0.640890, cluster_loss:0.195503\n",
      "Clustering   6: ASW= 0.5875, DB= 0.6388, CH= 10485.6587\n",
      "Training epoch 7, recon_loss:0.788115, zinb_loss:0.639390, cluster_loss:0.190574\n",
      "Clustering   7: ASW= 0.5878, DB= 0.6310, CH= 10706.3957\n",
      "Training epoch 8, recon_loss:0.795532, zinb_loss:0.643657, cluster_loss:0.196299\n",
      "Clustering   8: ASW= 0.6004, DB= 0.6067, CH= 11318.9830\n",
      "Training epoch 9, recon_loss:0.793502, zinb_loss:0.644986, cluster_loss:0.191464\n",
      "Clustering   9: ASW= 0.6066, DB= 0.5921, CH= 12165.7921\n",
      "Training epoch 10, recon_loss:0.795640, zinb_loss:0.645226, cluster_loss:0.193899\n",
      "Clustering   10: ASW= 0.6189, DB= 0.5795, CH= 12337.0980\n",
      "Training epoch 11, recon_loss:0.794627, zinb_loss:0.646246, cluster_loss:0.190530\n",
      "Clustering   11: ASW= 0.6209, DB= 0.5646, CH= 13181.4561\n",
      "Training epoch 12, recon_loss:0.793503, zinb_loss:0.643653, cluster_loss:0.191122\n",
      "Clustering   12: ASW= 0.6333, DB= 0.5539, CH= 13114.1243\n",
      "Training epoch 13, recon_loss:0.790501, zinb_loss:0.643247, cluster_loss:0.185939\n",
      "Clustering   13: ASW= 0.6322, DB= 0.5480, CH= 13909.9101\n",
      "Training epoch 14, recon_loss:0.791912, zinb_loss:0.644665, cluster_loss:0.186220\n",
      "Clustering   14: ASW= 0.6435, DB= 0.5346, CH= 13930.9664\n",
      "Training epoch 15, recon_loss:0.789298, zinb_loss:0.644028, cluster_loss:0.182875\n",
      "Clustering   15: ASW= 0.6452, DB= 0.5299, CH= 14541.9165\n",
      "Training epoch 16, recon_loss:0.790982, zinb_loss:0.644929, cluster_loss:0.183325\n",
      "Clustering   16: ASW= 0.6533, DB= 0.5175, CH= 14713.5949\n",
      "Training epoch 17, recon_loss:0.789246, zinb_loss:0.643808, cluster_loss:0.181418\n",
      "Clustering   17: ASW= 0.6554, DB= 0.5132, CH= 15224.9930\n",
      "Training epoch 18, recon_loss:0.791492, zinb_loss:0.645494, cluster_loss:0.181958\n",
      "Clustering   18: ASW= 0.6629, DB= 0.5014, CH= 15485.4823\n",
      "Training epoch 19, recon_loss:0.789743, zinb_loss:0.644231, cluster_loss:0.180452\n",
      "Clustering   19: ASW= 0.6651, DB= 0.4963, CH= 15993.7179\n",
      "Training epoch 20, recon_loss:0.791554, zinb_loss:0.645630, cluster_loss:0.180706\n",
      "Clustering   20: ASW= 0.6718, DB= 0.4855, CH= 16238.4130\n",
      "Training epoch 21, recon_loss:0.789927, zinb_loss:0.644363, cluster_loss:0.179470\n",
      "Clustering   21: ASW= 0.6741, DB= 0.4805, CH= 16741.5947\n",
      "Training epoch 22, recon_loss:0.791409, zinb_loss:0.645626, cluster_loss:0.179546\n",
      "Clustering   22: ASW= 0.6800, DB= 0.4708, CH= 16957.5644\n",
      "Training epoch 23, recon_loss:0.790026, zinb_loss:0.644476, cluster_loss:0.178504\n",
      "Clustering   23: ASW= 0.6824, DB= 0.4654, CH= 17467.1475\n",
      "Training epoch 24, recon_loss:0.791160, zinb_loss:0.645633, cluster_loss:0.178435\n",
      "Clustering   24: ASW= 0.6876, DB= 0.4572, CH= 17651.8670\n",
      "Training epoch 25, recon_loss:0.790033, zinb_loss:0.644636, cluster_loss:0.177479\n",
      "Clustering   25: ASW= 0.6899, DB= 0.4520, CH= 18159.1258\n",
      "Training epoch 26, recon_loss:0.790941, zinb_loss:0.645726, cluster_loss:0.177350\n",
      "Clustering   26: ASW= 0.6945, DB= 0.4448, CH= 18320.8315\n",
      "Training epoch 27, recon_loss:0.790051, zinb_loss:0.644885, cluster_loss:0.176457\n",
      "Clustering   27: ASW= 0.6965, DB= 0.4400, CH= 18799.5358\n",
      "Training epoch 28, recon_loss:0.790884, zinb_loss:0.645942, cluster_loss:0.176336\n",
      "Clustering   28: ASW= 0.7009, DB= 0.4337, CH= 18973.0495\n",
      "Training epoch 29, recon_loss:0.790154, zinb_loss:0.645214, cluster_loss:0.175504\n",
      "Clustering   29: ASW= 0.7021, DB= 0.4284, CH= 19374.2514\n",
      "Training epoch 30, recon_loss:0.790992, zinb_loss:0.646239, cluster_loss:0.175394\n",
      "Clustering   30: ASW= 0.7066, DB= 0.4239, CH= 19596.3244\n",
      "Training epoch 31, recon_loss:0.790411, zinb_loss:0.645577, cluster_loss:0.174676\n",
      "Clustering   31: ASW= 0.7069, DB= 0.4194, CH= 19914.3225\n",
      "Training epoch 32, recon_loss:0.791320, zinb_loss:0.646544, cluster_loss:0.174585\n",
      "Clustering   32: ASW= 0.7119, DB= 0.4148, CH= 20200.7676\n",
      "Training epoch 33, recon_loss:0.790896, zinb_loss:0.645896, cluster_loss:0.174084\n",
      "Clustering   33: ASW= 0.7108, DB= 0.4114, CH= 20389.9855\n",
      "Training epoch 34, recon_loss:0.791883, zinb_loss:0.646757, cluster_loss:0.173998\n",
      "Clustering   34: ASW= 0.7167, DB= 0.4066, CH= 20784.2287\n",
      "Training epoch 35, recon_loss:0.791580, zinb_loss:0.646077, cluster_loss:0.173861\n",
      "Clustering   35: ASW= 0.7142, DB= 0.4048, CH= 20818.7748\n",
      "Training epoch 36, recon_loss:0.792708, zinb_loss:0.646908, cluster_loss:0.173661\n",
      "Clustering   36: ASW= 0.7209, DB= 0.3997, CH= 21376.5842\n",
      "Training epoch 37, recon_loss:0.792308, zinb_loss:0.646198, cluster_loss:0.173958\n",
      "Clustering   37: ASW= 0.7178, DB= 0.3997, CH= 21227.3954\n",
      "Training epoch 38, recon_loss:0.793536, zinb_loss:0.647071, cluster_loss:0.173452\n",
      "Clustering   38: ASW= 0.7243, DB= 0.3938, CH= 21940.6146\n",
      "Training epoch 39, recon_loss:0.792650, zinb_loss:0.646334, cluster_loss:0.173853\n",
      "Clustering   39: ASW= 0.7216, DB= 0.3933, CH= 21605.7713\n",
      "Training epoch 40, recon_loss:0.793561, zinb_loss:0.647123, cluster_loss:0.172832\n",
      "Clustering   40: ASW= 0.7270, DB= 0.3889, CH= 22434.8069\n",
      "Training epoch 41, recon_loss:0.792455, zinb_loss:0.646422, cluster_loss:0.173081\n",
      "Clustering   41: ASW= 0.7253, DB= 0.3880, CH= 22016.1240\n",
      "Training epoch 42, recon_loss:0.793121, zinb_loss:0.647158, cluster_loss:0.171864\n",
      "Clustering   42: ASW= 0.7296, DB= 0.3841, CH= 22880.1381\n",
      "Training epoch 43, recon_loss:0.792161, zinb_loss:0.646511, cluster_loss:0.172056\n",
      "Clustering   43: ASW= 0.7285, DB= 0.3832, CH= 22441.1165\n",
      "Training epoch 44, recon_loss:0.792669, zinb_loss:0.647224, cluster_loss:0.170923\n",
      "Clustering   44: ASW= 0.7323, DB= 0.3801, CH= 23308.4009\n",
      "Training epoch 45, recon_loss:0.791936, zinb_loss:0.646629, cluster_loss:0.171171\n",
      "Clustering   45: ASW= 0.7311, DB= 0.3794, CH= 22830.8324\n",
      "Training epoch 46, recon_loss:0.792408, zinb_loss:0.647386, cluster_loss:0.170139\n",
      "Clustering   46: ASW= 0.7350, DB= 0.3758, CH= 23720.7644\n",
      "Training epoch 47, recon_loss:0.791735, zinb_loss:0.646757, cluster_loss:0.170424\n",
      "Clustering   47: ASW= 0.7334, DB= 0.3752, CH= 23184.3867\n",
      "Training epoch 48, recon_loss:0.792224, zinb_loss:0.647568, cluster_loss:0.169477\n",
      "Clustering   48: ASW= 0.7376, DB= 0.3716, CH= 24119.5970\n",
      "Training epoch 49, recon_loss:0.791572, zinb_loss:0.646885, cluster_loss:0.169838\n",
      "Clustering   49: ASW= 0.7355, DB= 0.3721, CH= 23523.7886\n",
      "Training epoch 50, recon_loss:0.792151, zinb_loss:0.647766, cluster_loss:0.168988\n",
      "Clustering   50: ASW= 0.7399, DB= 0.3682, CH= 24495.2883\n",
      "Training epoch 51, recon_loss:0.791514, zinb_loss:0.647032, cluster_loss:0.169469\n",
      "Clustering   51: ASW= 0.7374, DB= 0.3690, CH= 23838.4278\n",
      "Training epoch 52, recon_loss:0.792260, zinb_loss:0.647990, cluster_loss:0.168765\n",
      "Clustering   52: ASW= 0.7418, DB= 0.3664, CH= 24852.9703\n",
      "Training epoch 53, recon_loss:0.791620, zinb_loss:0.647189, cluster_loss:0.169355\n",
      "Clustering   53: ASW= 0.7392, DB= 0.3669, CH= 24137.3850\n",
      "Training epoch 54, recon_loss:0.792565, zinb_loss:0.648117, cluster_loss:0.168693\n",
      "Clustering   54: ASW= 0.7431, DB= 0.3626, CH= 25153.7847\n",
      "Training epoch 55, recon_loss:0.791842, zinb_loss:0.647240, cluster_loss:0.169230\n",
      "Clustering   55: ASW= 0.7412, DB= 0.3646, CH= 24445.4218\n",
      "Training epoch 56, recon_loss:0.792959, zinb_loss:0.648107, cluster_loss:0.168525\n",
      "Clustering   56: ASW= 0.7445, DB= 0.3594, CH= 25456.0063\n",
      "Training epoch 57, recon_loss:0.791962, zinb_loss:0.647209, cluster_loss:0.168900\n",
      "Clustering   57: ASW= 0.7435, DB= 0.3615, CH= 24805.0179\n",
      "Training epoch 58, recon_loss:0.793046, zinb_loss:0.647981, cluster_loss:0.168251\n",
      "Clustering   58: ASW= 0.7461, DB= 0.3563, CH= 25740.0570\n",
      "Training epoch 59, recon_loss:0.792017, zinb_loss:0.647157, cluster_loss:0.168451\n",
      "Clustering   59: ASW= 0.7455, DB= 0.3588, CH= 25160.9770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 60, recon_loss:0.793047, zinb_loss:0.647811, cluster_loss:0.168048\n",
      "Clustering   60: ASW= 0.7478, DB= 0.3540, CH= 26001.7672\n",
      "Training epoch 61, recon_loss:0.792132, zinb_loss:0.647105, cluster_loss:0.168134\n",
      "Clustering   61: ASW= 0.7472, DB= 0.3569, CH= 25502.1591\n",
      "Training epoch 62, recon_loss:0.793054, zinb_loss:0.647648, cluster_loss:0.167881\n",
      "Clustering   62: ASW= 0.7493, DB= 0.3515, CH= 26258.3489\n",
      "Training epoch 63, recon_loss:0.792143, zinb_loss:0.647033, cluster_loss:0.167827\n",
      "Clustering   63: ASW= 0.7487, DB= 0.3546, CH= 25811.4141\n",
      "Training epoch 64, recon_loss:0.792888, zinb_loss:0.647470, cluster_loss:0.167610\n",
      "Clustering   64: ASW= 0.7508, DB= 0.3495, CH= 26516.5546\n",
      "Training epoch 65, recon_loss:0.791956, zinb_loss:0.646935, cluster_loss:0.167434\n",
      "Clustering   65: ASW= 0.7502, DB= 0.3524, CH= 26101.7202\n",
      "Training epoch 66, recon_loss:0.792565, zinb_loss:0.647317, cluster_loss:0.167206\n",
      "Clustering   66: ASW= 0.7521, DB= 0.3474, CH= 26784.6218\n",
      "Training epoch 67, recon_loss:0.791658, zinb_loss:0.646850, cluster_loss:0.166972\n",
      "Clustering   67: ASW= 0.7517, DB= 0.3500, CH= 26380.8573\n",
      "Training epoch 68, recon_loss:0.792175, zinb_loss:0.647203, cluster_loss:0.166745\n",
      "Clustering   68: ASW= 0.7534, DB= 0.3456, CH= 27058.3477\n",
      "Training epoch 69, recon_loss:0.791458, zinb_loss:0.646813, cluster_loss:0.166587\n",
      "Clustering   69: ASW= 0.7532, DB= 0.3479, CH= 26629.1185\n",
      "Training epoch 70, recon_loss:0.791937, zinb_loss:0.647191, cluster_loss:0.166368\n",
      "Clustering   70: ASW= 0.7546, DB= 0.3439, CH= 27347.3599\n",
      "Training epoch 71, recon_loss:0.791464, zinb_loss:0.646864, cluster_loss:0.166331\n",
      "Clustering   71: ASW= 0.7546, DB= 0.3453, CH= 26882.8002\n",
      "Training epoch 72, recon_loss:0.791901, zinb_loss:0.647289, cluster_loss:0.166123\n",
      "Clustering   72: ASW= 0.7558, DB= 0.3417, CH= 27621.0542\n",
      "Training epoch 73, recon_loss:0.791706, zinb_loss:0.647042, cluster_loss:0.166227\n",
      "Clustering   73: ASW= 0.7559, DB= 0.3442, CH= 27139.2965\n",
      "Training epoch 74, recon_loss:0.792051, zinb_loss:0.647543, cluster_loss:0.166005\n",
      "Clustering   74: ASW= 0.7569, DB= 0.3395, CH= 27884.7212\n",
      "Training epoch 75, recon_loss:0.791942, zinb_loss:0.647306, cluster_loss:0.166171\n",
      "Clustering   75: ASW= 0.7572, DB= 0.3421, CH= 27372.6464\n",
      "Training epoch 76, recon_loss:0.792110, zinb_loss:0.647812, cluster_loss:0.165873\n",
      "Clustering   76: ASW= 0.7579, DB= 0.3380, CH= 28129.3803\n",
      "Training epoch 77, recon_loss:0.791903, zinb_loss:0.647530, cluster_loss:0.166023\n",
      "Clustering   77: ASW= 0.7583, DB= 0.3402, CH= 27610.8474\n",
      "Training epoch 78, recon_loss:0.791952, zinb_loss:0.647974, cluster_loss:0.165621\n",
      "Clustering   78: ASW= 0.7589, DB= 0.3367, CH= 28350.4636\n",
      "Training epoch 79, recon_loss:0.791602, zinb_loss:0.647666, cluster_loss:0.165760\n",
      "Clustering   79: ASW= 0.7593, DB= 0.3382, CH= 27862.7363\n",
      "Training epoch 80, recon_loss:0.791686, zinb_loss:0.648010, cluster_loss:0.165302\n",
      "Clustering   80: ASW= 0.7600, DB= 0.3357, CH= 28551.4303\n",
      "Training epoch 81, recon_loss:0.791312, zinb_loss:0.647766, cluster_loss:0.165492\n",
      "Clustering   81: ASW= 0.7602, DB= 0.3365, CH= 28115.4741\n",
      "Training epoch 82, recon_loss:0.791460, zinb_loss:0.647985, cluster_loss:0.165009\n",
      "Clustering   82: ASW= 0.7611, DB= 0.3345, CH= 28734.5324\n",
      "Training epoch 83, recon_loss:0.791164, zinb_loss:0.647894, cluster_loss:0.165249\n",
      "Clustering   83: ASW= 0.7610, DB= 0.3356, CH= 28402.8143\n",
      "Training epoch 84, recon_loss:0.791352, zinb_loss:0.647991, cluster_loss:0.164766\n",
      "Clustering   84: ASW= 0.7622, DB= 0.3332, CH= 28892.9929\n",
      "Training epoch 85, recon_loss:0.791217, zinb_loss:0.648101, cluster_loss:0.165010\n",
      "Clustering   85: ASW= 0.7617, DB= 0.3346, CH= 28710.5160\n",
      "Training epoch 86, recon_loss:0.791365, zinb_loss:0.648078, cluster_loss:0.164574\n",
      "Clustering   86: ASW= 0.7633, DB= 0.3321, CH= 29017.3921\n",
      "Training epoch 87, recon_loss:0.791659, zinb_loss:0.648519, cluster_loss:0.164832\n",
      "Clustering   87: ASW= 0.7624, DB= 0.3331, CH= 29044.2741\n",
      "Training epoch 88, recon_loss:0.791725, zinb_loss:0.648356, cluster_loss:0.164560\n",
      "Clustering   88: ASW= 0.7644, DB= 0.3316, CH= 29099.2268\n",
      "Training epoch 89, recon_loss:0.792552, zinb_loss:0.649082, cluster_loss:0.164823\n",
      "Clustering   89: ASW= 0.7631, DB= 0.3314, CH= 29388.3728\n",
      "Training epoch 90, recon_loss:0.792114, zinb_loss:0.648588, cluster_loss:0.164634\n",
      "Clustering   90: ASW= 0.7654, DB= 0.3309, CH= 29217.1969\n",
      "Training epoch 91, recon_loss:0.793110, zinb_loss:0.649289, cluster_loss:0.164880\n",
      "Clustering   91: ASW= 0.7637, DB= 0.3298, CH= 29646.5459\n",
      "Training epoch 92, recon_loss:0.792037, zinb_loss:0.648463, cluster_loss:0.164504\n",
      "Clustering   92: ASW= 0.7662, DB= 0.3299, CH= 29425.2856\n",
      "Training epoch 93, recon_loss:0.793016, zinb_loss:0.649034, cluster_loss:0.164729\n",
      "Clustering   93: ASW= 0.7647, DB= 0.3290, CH= 29855.6239\n",
      "Training epoch 94, recon_loss:0.791845, zinb_loss:0.648197, cluster_loss:0.164285\n",
      "Clustering   94: ASW= 0.7668, DB= 0.3292, CH= 29613.7612\n",
      "Training epoch 95, recon_loss:0.792960, zinb_loss:0.648805, cluster_loss:0.164481\n",
      "Clustering   95: ASW= 0.7659, DB= 0.3274, CH= 30072.6021\n",
      "Training epoch 96, recon_loss:0.791553, zinb_loss:0.647937, cluster_loss:0.163971\n",
      "Clustering   96: ASW= 0.7674, DB= 0.3287, CH= 29820.9324\n",
      "Training epoch 97, recon_loss:0.792576, zinb_loss:0.648543, cluster_loss:0.164059\n",
      "Clustering   97: ASW= 0.7671, DB= 0.3262, CH= 30270.7187\n",
      "Training epoch 98, recon_loss:0.791022, zinb_loss:0.647711, cluster_loss:0.163519\n",
      "Clustering   98: ASW= 0.7682, DB= 0.3276, CH= 30035.1755\n",
      "Training epoch 99, recon_loss:0.791844, zinb_loss:0.648285, cluster_loss:0.163517\n",
      "Clustering   99: ASW= 0.7680, DB= 0.3250, CH= 30463.0335\n",
      "Training epoch 100, recon_loss:0.790507, zinb_loss:0.647555, cluster_loss:0.163100\n",
      "Clustering   100: ASW= 0.7688, DB= 0.3264, CH= 30233.9740\n",
      "Training epoch 101, recon_loss:0.791299, zinb_loss:0.648140, cluster_loss:0.163104\n",
      "Clustering   101: ASW= 0.7689, DB= 0.3246, CH= 30648.5508\n",
      "Training epoch 102, recon_loss:0.790149, zinb_loss:0.647484, cluster_loss:0.162793\n",
      "Clustering   102: ASW= 0.7694, DB= 0.3248, CH= 30430.7393\n",
      "Training epoch 103, recon_loss:0.790937, zinb_loss:0.648067, cluster_loss:0.162810\n",
      "Clustering   103: ASW= 0.7697, DB= 0.3239, CH= 30831.6858\n",
      "Training epoch 104, recon_loss:0.789932, zinb_loss:0.647466, cluster_loss:0.162593\n",
      "Clustering   104: ASW= 0.7700, DB= 0.3237, CH= 30619.4572\n",
      "Training epoch 105, recon_loss:0.790782, zinb_loss:0.648062, cluster_loss:0.162627\n",
      "Clustering   105: ASW= 0.7705, DB= 0.3226, CH= 31014.7771\n",
      "Training epoch 106, recon_loss:0.789860, zinb_loss:0.647489, cluster_loss:0.162502\n",
      "Clustering   106: ASW= 0.7705, DB= 0.3236, CH= 30815.1758\n",
      "Training epoch 107, recon_loss:0.790819, zinb_loss:0.648092, cluster_loss:0.162557\n",
      "Clustering   107: ASW= 0.7713, DB= 0.3220, CH= 31183.3700\n",
      "Training epoch 108, recon_loss:0.789977, zinb_loss:0.647529, cluster_loss:0.162542\n",
      "Clustering   108: ASW= 0.7710, DB= 0.3228, CH= 30992.0955\n",
      "Training epoch 109, recon_loss:0.791109, zinb_loss:0.648135, cluster_loss:0.162631\n",
      "Clustering   109: ASW= 0.7719, DB= 0.3209, CH= 31325.5398\n",
      "Training epoch 110, recon_loss:0.790383, zinb_loss:0.647578, cluster_loss:0.162762\n",
      "Clustering   110: ASW= 0.7713, DB= 0.3228, CH= 31167.5080\n",
      "Training epoch 111, recon_loss:0.791713, zinb_loss:0.648188, cluster_loss:0.162856\n",
      "Clustering   111: ASW= 0.7726, DB= 0.3200, CH= 31440.4167\n",
      "Training epoch 112, recon_loss:0.790986, zinb_loss:0.647628, cluster_loss:0.163121\n",
      "Clustering   112: ASW= 0.7714, DB= 0.3222, CH= 31310.5559\n",
      "Training epoch 113, recon_loss:0.792351, zinb_loss:0.648193, cluster_loss:0.163090\n",
      "Clustering   113: ASW= 0.7733, DB= 0.3195, CH= 31541.7789\n",
      "Training epoch 114, recon_loss:0.791462, zinb_loss:0.647629, cluster_loss:0.163362\n",
      "Clustering   114: ASW= 0.7715, DB= 0.3215, CH= 31451.7911\n",
      "Training epoch 115, recon_loss:0.792591, zinb_loss:0.648087, cluster_loss:0.163118\n",
      "Clustering   115: ASW= 0.7739, DB= 0.3181, CH= 31619.3713\n",
      "Training epoch 116, recon_loss:0.791608, zinb_loss:0.647552, cluster_loss:0.163241\n",
      "Clustering   116: ASW= 0.7718, DB= 0.3206, CH= 31640.3609\n",
      "Training epoch 117, recon_loss:0.792424, zinb_loss:0.647879, cluster_loss:0.162888\n",
      "Clustering   117: ASW= 0.7744, DB= 0.3172, CH= 31687.6710\n",
      "Training epoch 118, recon_loss:0.791625, zinb_loss:0.647466, cluster_loss:0.162862\n",
      "Clustering   118: ASW= 0.7725, DB= 0.3195, CH= 31868.0086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 119, recon_loss:0.792284, zinb_loss:0.647676, cluster_loss:0.162608\n",
      "Clustering   119: ASW= 0.7748, DB= 0.3152, CH= 31723.7642\n",
      "Training epoch 120, recon_loss:0.791855, zinb_loss:0.647488, cluster_loss:0.162505\n",
      "Clustering   120: ASW= 0.7733, DB= 0.3188, CH= 32136.3572\n",
      "Training epoch 121, recon_loss:0.792293, zinb_loss:0.647569, cluster_loss:0.162416\n",
      "Clustering   121: ASW= 0.7750, DB= 0.3149, CH= 31745.1165\n",
      "Training epoch 122, recon_loss:0.792150, zinb_loss:0.647669, cluster_loss:0.162275\n",
      "Clustering   122: ASW= 0.7742, DB= 0.3166, CH= 32380.2880\n",
      "Training epoch 123, recon_loss:0.792139, zinb_loss:0.647593, cluster_loss:0.162282\n",
      "Clustering   123: ASW= 0.7750, DB= 0.3147, CH= 31769.1720\n",
      "Training epoch 124, recon_loss:0.792110, zinb_loss:0.647936, cluster_loss:0.162088\n",
      "Clustering   124: ASW= 0.7750, DB= 0.3154, CH= 32595.3393\n",
      "Training epoch 125, recon_loss:0.791530, zinb_loss:0.647629, cluster_loss:0.162002\n",
      "Clustering   125: ASW= 0.7752, DB= 0.3141, CH= 31880.2922\n",
      "Training epoch 126, recon_loss:0.791491, zinb_loss:0.648025, cluster_loss:0.161784\n",
      "Clustering   126: ASW= 0.7756, DB= 0.3147, CH= 32750.3761\n",
      "Training epoch 127, recon_loss:0.790674, zinb_loss:0.647611, cluster_loss:0.161643\n",
      "Clustering   127: ASW= 0.7757, DB= 0.3137, CH= 32057.4534\n",
      "Training epoch 128, recon_loss:0.790538, zinb_loss:0.647938, cluster_loss:0.161414\n",
      "Clustering   128: ASW= 0.7761, DB= 0.3131, CH= 32891.1686\n",
      "Training epoch 129, recon_loss:0.789994, zinb_loss:0.647598, cluster_loss:0.161360\n",
      "Clustering   129: ASW= 0.7763, DB= 0.3128, CH= 32224.6116\n",
      "Training epoch 130, recon_loss:0.789937, zinb_loss:0.647914, cluster_loss:0.161178\n",
      "Clustering   130: ASW= 0.7765, DB= 0.3134, CH= 33048.1193\n",
      "Training epoch 131, recon_loss:0.789634, zinb_loss:0.647636, cluster_loss:0.161213\n",
      "Clustering   131: ASW= 0.7768, DB= 0.3121, CH= 32376.5297\n",
      "Training epoch 132, recon_loss:0.789717, zinb_loss:0.647963, cluster_loss:0.161099\n",
      "Clustering   132: ASW= 0.7769, DB= 0.3128, CH= 33208.1892\n",
      "Training epoch 133, recon_loss:0.789611, zinb_loss:0.647707, cluster_loss:0.161209\n",
      "Clustering   133: ASW= 0.7774, DB= 0.3114, CH= 32509.1537\n",
      "Training epoch 134, recon_loss:0.789891, zinb_loss:0.648032, cluster_loss:0.161167\n",
      "Clustering   134: ASW= 0.7774, DB= 0.3121, CH= 33367.2963\n",
      "Training epoch 135, recon_loss:0.789930, zinb_loss:0.647751, cluster_loss:0.161295\n",
      "Clustering   135: ASW= 0.7779, DB= 0.3107, CH= 32634.3532\n",
      "Training epoch 136, recon_loss:0.790256, zinb_loss:0.648018, cluster_loss:0.161264\n",
      "Clustering   136: ASW= 0.7778, DB= 0.3106, CH= 33513.3374\n",
      "Training epoch 137, recon_loss:0.790208, zinb_loss:0.647672, cluster_loss:0.161322\n",
      "Clustering   137: ASW= 0.7784, DB= 0.3105, CH= 32745.6510\n",
      "Training epoch 138, recon_loss:0.790391, zinb_loss:0.647878, cluster_loss:0.161230\n",
      "Clustering   138: ASW= 0.7783, DB= 0.3100, CH= 33635.9323\n",
      "Training epoch 139, recon_loss:0.790088, zinb_loss:0.647482, cluster_loss:0.161231\n",
      "Clustering   139: ASW= 0.7788, DB= 0.3100, CH= 32859.0466\n",
      "Training epoch 140, recon_loss:0.790142, zinb_loss:0.647671, cluster_loss:0.161069\n",
      "Clustering   140: ASW= 0.7787, DB= 0.3103, CH= 33752.5650\n",
      "Training epoch 141, recon_loss:0.789717, zinb_loss:0.647284, cluster_loss:0.161141\n",
      "Clustering   141: ASW= 0.7790, DB= 0.3097, CH= 32956.8777\n",
      "Training epoch 142, recon_loss:0.789876, zinb_loss:0.647508, cluster_loss:0.160954\n",
      "Clustering   142: ASW= 0.7792, DB= 0.3089, CH= 33848.8049\n",
      "Training epoch 143, recon_loss:0.789464, zinb_loss:0.647152, cluster_loss:0.161189\n",
      "Clustering   143: ASW= 0.7792, DB= 0.3103, CH= 33052.8102\n",
      "Training epoch 144, recon_loss:0.789789, zinb_loss:0.647434, cluster_loss:0.160990\n",
      "Clustering   144: ASW= 0.7797, DB= 0.3078, CH= 33939.1097\n",
      "Training epoch 145, recon_loss:0.789447, zinb_loss:0.647129, cluster_loss:0.161428\n",
      "Clustering   145: ASW= 0.7793, DB= 0.3102, CH= 33124.0345\n",
      "Training epoch 146, recon_loss:0.789799, zinb_loss:0.647474, cluster_loss:0.161149\n",
      "Clustering   146: ASW= 0.7801, DB= 0.3071, CH= 34014.0696\n",
      "Training epoch 147, recon_loss:0.789473, zinb_loss:0.647211, cluster_loss:0.161685\n",
      "Clustering   147: ASW= 0.7793, DB= 0.3095, CH= 33217.4109\n",
      "Training epoch 148, recon_loss:0.789732, zinb_loss:0.647540, cluster_loss:0.161194\n",
      "Clustering   148: ASW= 0.7805, DB= 0.3067, CH= 34059.2656\n",
      "Training epoch 149, recon_loss:0.789302, zinb_loss:0.647296, cluster_loss:0.161626\n",
      "Clustering   149: ASW= 0.7795, DB= 0.3091, CH= 33365.1027\n",
      "Training epoch 150, recon_loss:0.789441, zinb_loss:0.647519, cluster_loss:0.161005\n",
      "Clustering   150: ASW= 0.7809, DB= 0.3061, CH= 34072.2818\n",
      "Training epoch 151, recon_loss:0.789131, zinb_loss:0.647338, cluster_loss:0.161321\n",
      "Clustering   151: ASW= 0.7797, DB= 0.3085, CH= 33545.2856\n",
      "Training epoch 152, recon_loss:0.789247, zinb_loss:0.647456, cluster_loss:0.160789\n",
      "Clustering   152: ASW= 0.7814, DB= 0.3055, CH= 34118.0969\n",
      "Training epoch 153, recon_loss:0.789350, zinb_loss:0.647390, cluster_loss:0.161075\n",
      "Clustering   153: ASW= 0.7799, DB= 0.3079, CH= 33737.9179\n",
      "Training epoch 154, recon_loss:0.789473, zinb_loss:0.647395, cluster_loss:0.160680\n",
      "Clustering   154: ASW= 0.7819, DB= 0.3049, CH= 34205.6265\n",
      "Training epoch 155, recon_loss:0.789800, zinb_loss:0.647415, cluster_loss:0.160970\n",
      "Clustering   155: ASW= 0.7801, DB= 0.3070, CH= 33895.7064\n",
      "Training epoch 156, recon_loss:0.789937, zinb_loss:0.647308, cluster_loss:0.160674\n",
      "Clustering   156: ASW= 0.7824, DB= 0.3045, CH= 34303.7215\n",
      "Training epoch 157, recon_loss:0.790202, zinb_loss:0.647368, cluster_loss:0.160925\n",
      "Clustering   157: ASW= 0.7802, DB= 0.3069, CH= 34041.6637\n",
      "Training epoch 158, recon_loss:0.790305, zinb_loss:0.647213, cluster_loss:0.160733\n",
      "Clustering   158: ASW= 0.7828, DB= 0.3043, CH= 34377.2423\n",
      "Training epoch 159, recon_loss:0.790305, zinb_loss:0.647324, cluster_loss:0.160886\n",
      "Clustering   159: ASW= 0.7803, DB= 0.3065, CH= 34148.2796\n",
      "Training epoch 160, recon_loss:0.790137, zinb_loss:0.647136, cluster_loss:0.160722\n",
      "Clustering   160: ASW= 0.7831, DB= 0.3042, CH= 34433.4541\n",
      "Training epoch 161, recon_loss:0.789960, zinb_loss:0.647286, cluster_loss:0.160749\n",
      "Clustering   161: ASW= 0.7806, DB= 0.3059, CH= 34264.2217\n",
      "Training epoch 162, recon_loss:0.789572, zinb_loss:0.647085, cluster_loss:0.160563\n",
      "Clustering   162: ASW= 0.7833, DB= 0.3049, CH= 34495.2659\n",
      "Training epoch 163, recon_loss:0.789475, zinb_loss:0.647276, cluster_loss:0.160534\n",
      "Clustering   163: ASW= 0.7811, DB= 0.3051, CH= 34416.2714\n",
      "Training epoch 164, recon_loss:0.789002, zinb_loss:0.647072, cluster_loss:0.160331\n",
      "Clustering   164: ASW= 0.7835, DB= 0.3048, CH= 34554.4926\n",
      "Training epoch 165, recon_loss:0.789022, zinb_loss:0.647278, cluster_loss:0.160316\n",
      "Clustering   165: ASW= 0.7817, DB= 0.3050, CH= 34582.6276\n",
      "Training epoch 166, recon_loss:0.788554, zinb_loss:0.647097, cluster_loss:0.160096\n",
      "Clustering   166: ASW= 0.7837, DB= 0.3050, CH= 34625.7116\n",
      "Training epoch 167, recon_loss:0.788675, zinb_loss:0.647311, cluster_loss:0.160132\n",
      "Clustering   167: ASW= 0.7822, DB= 0.3042, CH= 34739.4302\n",
      "Training epoch 168, recon_loss:0.788257, zinb_loss:0.647164, cluster_loss:0.159888\n",
      "Clustering   168: ASW= 0.7839, DB= 0.3046, CH= 34704.9376\n",
      "Training epoch 169, recon_loss:0.788426, zinb_loss:0.647373, cluster_loss:0.159976\n",
      "Clustering   169: ASW= 0.7828, DB= 0.3030, CH= 34898.1016\n",
      "Training epoch 170, recon_loss:0.788167, zinb_loss:0.647290, cluster_loss:0.159731\n",
      "Clustering   170: ASW= 0.7840, DB= 0.3030, CH= 34765.7245\n",
      "Training epoch 171, recon_loss:0.788394, zinb_loss:0.647511, cluster_loss:0.159876\n",
      "Clustering   171: ASW= 0.7832, DB= 0.3018, CH= 35037.9415\n",
      "Training epoch 172, recon_loss:0.788393, zinb_loss:0.647513, cluster_loss:0.159655\n",
      "Clustering   172: ASW= 0.7842, DB= 0.3026, CH= 34835.7089\n",
      "Training epoch 173, recon_loss:0.788721, zinb_loss:0.647767, cluster_loss:0.159879\n",
      "Clustering   173: ASW= 0.7835, DB= 0.3022, CH= 35174.6043\n",
      "Training epoch 174, recon_loss:0.789079, zinb_loss:0.647865, cluster_loss:0.159703\n",
      "Clustering   174: ASW= 0.7843, DB= 0.3020, CH= 34887.7394\n",
      "Training epoch 175, recon_loss:0.789534, zinb_loss:0.648131, cluster_loss:0.160043\n",
      "Clustering   175: ASW= 0.7837, DB= 0.3014, CH= 35251.4131\n",
      "Training epoch 176, recon_loss:0.790042, zinb_loss:0.648247, cluster_loss:0.159865\n",
      "Clustering   176: ASW= 0.7843, DB= 0.3019, CH= 34915.0024\n",
      "Training epoch 177, recon_loss:0.790381, zinb_loss:0.648361, cluster_loss:0.160277\n",
      "Clustering   177: ASW= 0.7839, DB= 0.3007, CH= 35289.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 178, recon_loss:0.790610, zinb_loss:0.648408, cluster_loss:0.159975\n",
      "Clustering   178: ASW= 0.7844, DB= 0.3010, CH= 34942.9135\n",
      "Training epoch 179, recon_loss:0.790428, zinb_loss:0.648308, cluster_loss:0.160384\n",
      "Clustering   179: ASW= 0.7840, DB= 0.3003, CH= 35323.6997\n",
      "Training epoch 180, recon_loss:0.790463, zinb_loss:0.648340, cluster_loss:0.160018\n",
      "Clustering   180: ASW= 0.7846, DB= 0.3009, CH= 35017.0617\n",
      "Training epoch 181, recon_loss:0.789932, zinb_loss:0.648164, cluster_loss:0.160367\n",
      "Clustering   181: ASW= 0.7842, DB= 0.3002, CH= 35387.8759\n",
      "Training epoch 182, recon_loss:0.789944, zinb_loss:0.648180, cluster_loss:0.160067\n",
      "Clustering   182: ASW= 0.7849, DB= 0.3004, CH= 35101.0235\n",
      "Training epoch 183, recon_loss:0.789397, zinb_loss:0.648016, cluster_loss:0.160332\n",
      "Clustering   183: ASW= 0.7844, DB= 0.3004, CH= 35452.8007\n",
      "Training epoch 184, recon_loss:0.789369, zinb_loss:0.648022, cluster_loss:0.160072\n",
      "Clustering   184: ASW= 0.7852, DB= 0.2998, CH= 35199.1717\n",
      "Training epoch 185, recon_loss:0.788864, zinb_loss:0.647886, cluster_loss:0.160190\n",
      "Clustering   185: ASW= 0.7846, DB= 0.3006, CH= 35537.0317\n",
      "Training epoch 186, recon_loss:0.788791, zinb_loss:0.647859, cluster_loss:0.159958\n",
      "Clustering   186: ASW= 0.7855, DB= 0.2992, CH= 35305.9777\n",
      "Training epoch 187, recon_loss:0.788381, zinb_loss:0.647764, cluster_loss:0.159952\n",
      "Clustering   187: ASW= 0.7849, DB= 0.3005, CH= 35640.0452\n",
      "Training epoch 188, recon_loss:0.788266, zinb_loss:0.647711, cluster_loss:0.159772\n",
      "Clustering   188: ASW= 0.7858, DB= 0.2988, CH= 35424.6203\n",
      "Training epoch 189, recon_loss:0.787956, zinb_loss:0.647653, cluster_loss:0.159684\n",
      "Clustering   189: ASW= 0.7851, DB= 0.3004, CH= 35743.0840\n",
      "Training epoch 190, recon_loss:0.787825, zinb_loss:0.647595, cluster_loss:0.159570\n",
      "Clustering   190: ASW= 0.7862, DB= 0.2982, CH= 35539.2503\n",
      "Training epoch 191, recon_loss:0.787674, zinb_loss:0.647595, cluster_loss:0.159453\n",
      "Clustering   191: ASW= 0.7855, DB= 0.3002, CH= 35852.2894\n",
      "Training epoch 192, recon_loss:0.787610, zinb_loss:0.647543, cluster_loss:0.159421\n",
      "Clustering   192: ASW= 0.7864, DB= 0.2994, CH= 35674.4264\n",
      "Training epoch 193, recon_loss:0.787740, zinb_loss:0.647604, cluster_loss:0.159327\n",
      "Clustering   193: ASW= 0.7858, DB= 0.3002, CH= 35961.0214\n",
      "Training epoch 194, recon_loss:0.787893, zinb_loss:0.647555, cluster_loss:0.159423\n",
      "Clustering   194: ASW= 0.7867, DB= 0.2989, CH= 35770.2504\n",
      "Training epoch 195, recon_loss:0.788319, zinb_loss:0.647650, cluster_loss:0.159362\n",
      "Clustering   195: ASW= 0.7862, DB= 0.2991, CH= 36035.4665\n",
      "Training epoch 196, recon_loss:0.788633, zinb_loss:0.647557, cluster_loss:0.159580\n",
      "Clustering   196: ASW= 0.7869, DB= 0.2986, CH= 35848.0341\n",
      "Training epoch 197, recon_loss:0.788916, zinb_loss:0.647615, cluster_loss:0.159423\n",
      "Clustering   197: ASW= 0.7865, DB= 0.2998, CH= 36116.0539\n",
      "Training epoch 198, recon_loss:0.788901, zinb_loss:0.647423, cluster_loss:0.159595\n",
      "Clustering   198: ASW= 0.7870, DB= 0.2984, CH= 35923.0315\n",
      "Training epoch 199, recon_loss:0.788959, zinb_loss:0.647498, cluster_loss:0.159343\n",
      "Clustering   199: ASW= 0.7869, DB= 0.2989, CH= 36158.5533\n",
      "Training epoch 200, recon_loss:0.788627, zinb_loss:0.647291, cluster_loss:0.159448\n",
      "Clustering   200: ASW= 0.7872, DB= 0.2981, CH= 36022.3588\n",
      "Training epoch 201, recon_loss:0.788765, zinb_loss:0.647442, cluster_loss:0.159228\n",
      "Clustering   201: ASW= 0.7872, DB= 0.2979, CH= 36191.0196\n",
      "Training epoch 202, recon_loss:0.788382, zinb_loss:0.647327, cluster_loss:0.159301\n",
      "Clustering   202: ASW= 0.7874, DB= 0.2974, CH= 36161.7040\n",
      "Training epoch 203, recon_loss:0.788552, zinb_loss:0.647442, cluster_loss:0.159153\n",
      "Clustering   203: ASW= 0.7875, DB= 0.2971, CH= 36218.5402\n",
      "Training epoch 204, recon_loss:0.788152, zinb_loss:0.647406, cluster_loss:0.159190\n",
      "Clustering   204: ASW= 0.7875, DB= 0.2978, CH= 36271.4412\n",
      "Training epoch 205, recon_loss:0.788121, zinb_loss:0.647354, cluster_loss:0.159051\n",
      "Clustering   205: ASW= 0.7878, DB= 0.2965, CH= 36263.0611\n",
      "Training epoch 206, recon_loss:0.787602, zinb_loss:0.647322, cluster_loss:0.159021\n",
      "Clustering   206: ASW= 0.7875, DB= 0.2978, CH= 36338.8404\n",
      "Training epoch 207, recon_loss:0.787642, zinb_loss:0.647239, cluster_loss:0.158942\n",
      "Clustering   207: ASW= 0.7880, DB= 0.2964, CH= 36324.8044\n",
      "Training epoch 208, recon_loss:0.787179, zinb_loss:0.647245, cluster_loss:0.158869\n",
      "Clustering   208: ASW= 0.7876, DB= 0.2979, CH= 36391.1172\n",
      "Training epoch 209, recon_loss:0.787342, zinb_loss:0.647176, cluster_loss:0.158860\n",
      "Clustering   209: ASW= 0.7883, DB= 0.2958, CH= 36403.2179\n",
      "Training epoch 210, recon_loss:0.786963, zinb_loss:0.647211, cluster_loss:0.158753\n",
      "Clustering   210: ASW= 0.7877, DB= 0.2975, CH= 36436.7100\n",
      "Training epoch 211, recon_loss:0.787193, zinb_loss:0.647157, cluster_loss:0.158781\n",
      "Clustering   211: ASW= 0.7886, DB= 0.2954, CH= 36500.2965\n",
      "Training epoch 212, recon_loss:0.786908, zinb_loss:0.647200, cluster_loss:0.158673\n",
      "Clustering   212: ASW= 0.7879, DB= 0.2967, CH= 36485.3514\n",
      "Training epoch 213, recon_loss:0.787201, zinb_loss:0.647175, cluster_loss:0.158714\n",
      "Clustering   213: ASW= 0.7889, DB= 0.2947, CH= 36611.0354\n",
      "Training epoch 214, recon_loss:0.787015, zinb_loss:0.647202, cluster_loss:0.158642\n",
      "Clustering   214: ASW= 0.7881, DB= 0.2964, CH= 36506.4855\n",
      "Training epoch 215, recon_loss:0.787364, zinb_loss:0.647219, cluster_loss:0.158673\n",
      "Clustering   215: ASW= 0.7891, DB= 0.2941, CH= 36734.6814\n",
      "Training epoch 216, recon_loss:0.787332, zinb_loss:0.647222, cluster_loss:0.158700\n",
      "Clustering   216: ASW= 0.7882, DB= 0.2958, CH= 36499.5096\n",
      "Training epoch 217, recon_loss:0.787727, zinb_loss:0.647284, cluster_loss:0.158714\n",
      "Clustering   217: ASW= 0.7894, DB= 0.2938, CH= 36848.7984\n",
      "Training epoch 218, recon_loss:0.787858, zinb_loss:0.647252, cluster_loss:0.158845\n",
      "Clustering   218: ASW= 0.7882, DB= 0.2950, CH= 36471.9399\n",
      "Training epoch 219, recon_loss:0.788038, zinb_loss:0.647318, cluster_loss:0.158831\n",
      "Clustering   219: ASW= 0.7896, DB= 0.2932, CH= 36922.4594\n",
      "Training epoch 220, recon_loss:0.787745, zinb_loss:0.647208, cluster_loss:0.158859\n",
      "Clustering   220: ASW= 0.7882, DB= 0.2958, CH= 36516.8361\n",
      "Training epoch 221, recon_loss:0.787484, zinb_loss:0.647207, cluster_loss:0.158799\n",
      "Clustering   221: ASW= 0.7898, DB= 0.2926, CH= 36977.5866\n",
      "Training epoch 222, recon_loss:0.787212, zinb_loss:0.647100, cluster_loss:0.158749\n",
      "Clustering   222: ASW= 0.7883, DB= 0.2952, CH= 36576.9621\n",
      "Training epoch 223, recon_loss:0.786801, zinb_loss:0.647046, cluster_loss:0.158685\n",
      "Clustering   223: ASW= 0.7900, DB= 0.2922, CH= 37038.2019\n",
      "Training epoch 224, recon_loss:0.786777, zinb_loss:0.647009, cluster_loss:0.158652\n",
      "Clustering   224: ASW= 0.7884, DB= 0.2944, CH= 36631.9032\n",
      "Training epoch 225, recon_loss:0.786375, zinb_loss:0.646925, cluster_loss:0.158598\n",
      "Clustering   225: ASW= 0.7902, DB= 0.2921, CH= 37113.1379\n",
      "Training epoch 226, recon_loss:0.786578, zinb_loss:0.646966, cluster_loss:0.158660\n",
      "Clustering   226: ASW= 0.7886, DB= 0.2934, CH= 36695.3175\n",
      "Training epoch 227, recon_loss:0.786220, zinb_loss:0.646852, cluster_loss:0.158616\n",
      "Clustering   227: ASW= 0.7903, DB= 0.2918, CH= 37180.3474\n",
      "Training epoch 228, recon_loss:0.786608, zinb_loss:0.646981, cluster_loss:0.158775\n",
      "Clustering   228: ASW= 0.7887, DB= 0.2930, CH= 36737.8528\n",
      "Training epoch 229, recon_loss:0.786241, zinb_loss:0.646830, cluster_loss:0.158712\n",
      "Clustering   229: ASW= 0.7904, DB= 0.2909, CH= 37238.2187\n",
      "Training epoch 230, recon_loss:0.786718, zinb_loss:0.647037, cluster_loss:0.158915\n",
      "Clustering   230: ASW= 0.7888, DB= 0.2927, CH= 36764.7003\n",
      "Training epoch 231, recon_loss:0.786432, zinb_loss:0.646858, cluster_loss:0.158820\n",
      "Clustering   231: ASW= 0.7905, DB= 0.2905, CH= 37291.1512\n",
      "Training epoch 232, recon_loss:0.787145, zinb_loss:0.647165, cluster_loss:0.159045\n",
      "Clustering   232: ASW= 0.7889, DB= 0.2923, CH= 36787.9435\n",
      "Training epoch 233, recon_loss:0.787190, zinb_loss:0.647023, cluster_loss:0.158948\n",
      "Clustering   233: ASW= 0.7906, DB= 0.2904, CH= 37338.3803\n",
      "Training epoch 234, recon_loss:0.788264, zinb_loss:0.647473, cluster_loss:0.159215\n",
      "Clustering   234: ASW= 0.7892, DB= 0.2917, CH= 36818.7903\n",
      "Training epoch 235, recon_loss:0.788489, zinb_loss:0.647391, cluster_loss:0.159046\n",
      "Clustering   235: ASW= 0.7908, DB= 0.2904, CH= 37401.5963\n",
      "Training epoch 236, recon_loss:0.789089, zinb_loss:0.647701, cluster_loss:0.159242\n",
      "Clustering   236: ASW= 0.7896, DB= 0.2911, CH= 36908.4564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 237, recon_loss:0.788512, zinb_loss:0.647421, cluster_loss:0.158794\n",
      "Clustering   237: ASW= 0.7910, DB= 0.2885, CH= 37468.2002\n",
      "Training epoch 238, recon_loss:0.788288, zinb_loss:0.647432, cluster_loss:0.158921\n",
      "Clustering   238: ASW= 0.7900, DB= 0.2906, CH= 37013.7336\n",
      "Training epoch 239, recon_loss:0.787527, zinb_loss:0.647129, cluster_loss:0.158382\n",
      "Clustering   239: ASW= 0.7910, DB= 0.2882, CH= 37499.3674\n",
      "Training epoch 240, recon_loss:0.787364, zinb_loss:0.647135, cluster_loss:0.158593\n",
      "Clustering   240: ASW= 0.7904, DB= 0.2899, CH= 37140.7009\n",
      "Training epoch 241, recon_loss:0.786973, zinb_loss:0.646926, cluster_loss:0.158108\n",
      "Clustering   241: ASW= 0.7911, DB= 0.2887, CH= 37554.2132\n",
      "Training epoch 242, recon_loss:0.786919, zinb_loss:0.646965, cluster_loss:0.158426\n",
      "Clustering   242: ASW= 0.7909, DB= 0.2894, CH= 37275.9489\n",
      "Training epoch 243, recon_loss:0.786982, zinb_loss:0.646846, cluster_loss:0.158012\n",
      "Clustering   243: ASW= 0.7910, DB= 0.2889, CH= 37611.8044\n",
      "Training epoch 244, recon_loss:0.786894, zinb_loss:0.646879, cluster_loss:0.158428\n",
      "Clustering   244: ASW= 0.7914, DB= 0.2889, CH= 37402.4978\n",
      "Training epoch 245, recon_loss:0.787138, zinb_loss:0.646849, cluster_loss:0.158086\n",
      "Clustering   245: ASW= 0.7909, DB= 0.2891, CH= 37655.5011\n",
      "Training epoch 246, recon_loss:0.786808, zinb_loss:0.646853, cluster_loss:0.158535\n",
      "Clustering   246: ASW= 0.7919, DB= 0.2883, CH= 37516.8700\n",
      "Training epoch 247, recon_loss:0.786954, zinb_loss:0.646879, cluster_loss:0.158256\n",
      "Clustering   247: ASW= 0.7908, DB= 0.2892, CH= 37681.2233\n",
      "Training epoch 248, recon_loss:0.786418, zinb_loss:0.646841, cluster_loss:0.158598\n",
      "Clustering   248: ASW= 0.7924, DB= 0.2879, CH= 37637.0881\n",
      "Training epoch 249, recon_loss:0.786700, zinb_loss:0.646874, cluster_loss:0.158397\n",
      "Clustering   249: ASW= 0.7907, DB= 0.2897, CH= 37701.8988\n",
      "Training epoch 250, recon_loss:0.786047, zinb_loss:0.646830, cluster_loss:0.158618\n",
      "Clustering   250: ASW= 0.7928, DB= 0.2873, CH= 37753.0344\n",
      "Training epoch 251, recon_loss:0.786658, zinb_loss:0.646857, cluster_loss:0.158539\n",
      "Clustering   251: ASW= 0.7907, DB= 0.2899, CH= 37706.5189\n",
      "Training epoch 252, recon_loss:0.785958, zinb_loss:0.646834, cluster_loss:0.158678\n",
      "Clustering   252: ASW= 0.7932, DB= 0.2870, CH= 37865.8206\n",
      "Training epoch 253, recon_loss:0.786892, zinb_loss:0.646847, cluster_loss:0.158700\n",
      "Clustering   253: ASW= 0.7907, DB= 0.2902, CH= 37693.9245\n",
      "Training epoch 254, recon_loss:0.786172, zinb_loss:0.646860, cluster_loss:0.158787\n",
      "Clustering   254: ASW= 0.7934, DB= 0.2866, CH= 37961.4993\n",
      "Training epoch 255, recon_loss:0.787088, zinb_loss:0.646855, cluster_loss:0.158807\n",
      "Clustering   255: ASW= 0.7908, DB= 0.2899, CH= 37687.3160\n",
      "Training epoch 256, recon_loss:0.786275, zinb_loss:0.646892, cluster_loss:0.158833\n",
      "Clustering   256: ASW= 0.7936, DB= 0.2860, CH= 38040.6100\n",
      "Training epoch 257, recon_loss:0.787086, zinb_loss:0.646881, cluster_loss:0.158807\n",
      "Clustering   257: ASW= 0.7911, DB= 0.2897, CH= 37697.1560\n",
      "Training epoch 258, recon_loss:0.786307, zinb_loss:0.646957, cluster_loss:0.158836\n",
      "Clustering   258: ASW= 0.7936, DB= 0.2858, CH= 38114.2956\n",
      "Training epoch 259, recon_loss:0.787124, zinb_loss:0.646951, cluster_loss:0.158853\n",
      "Clustering   259: ASW= 0.7915, DB= 0.2893, CH= 37732.2314\n",
      "Training epoch 260, recon_loss:0.786496, zinb_loss:0.647079, cluster_loss:0.158938\n",
      "Clustering   260: ASW= 0.7934, DB= 0.2857, CH= 38163.7264\n",
      "Training epoch 261, recon_loss:0.787307, zinb_loss:0.647069, cluster_loss:0.159027\n",
      "Clustering   261: ASW= 0.7920, DB= 0.2884, CH= 37771.1374\n",
      "Training epoch 262, recon_loss:0.786772, zinb_loss:0.647206, cluster_loss:0.159157\n",
      "Clustering   262: ASW= 0.7932, DB= 0.2856, CH= 38196.7233\n",
      "Training epoch 263, recon_loss:0.787382, zinb_loss:0.647124, cluster_loss:0.159247\n",
      "Clustering   263: ASW= 0.7924, DB= 0.2883, CH= 37814.1218\n",
      "Training epoch 264, recon_loss:0.786832, zinb_loss:0.647251, cluster_loss:0.159289\n",
      "Clustering   264: ASW= 0.7930, DB= 0.2863, CH= 38259.8555\n",
      "Training epoch 265, recon_loss:0.787051, zinb_loss:0.647068, cluster_loss:0.159267\n",
      "Clustering   265: ASW= 0.7928, DB= 0.2876, CH= 37874.0528\n",
      "Training epoch 266, recon_loss:0.786587, zinb_loss:0.647170, cluster_loss:0.159177\n",
      "Clustering   266: ASW= 0.7930, DB= 0.2863, CH= 38342.9050\n",
      "Training epoch 267, recon_loss:0.786597, zinb_loss:0.646949, cluster_loss:0.159081\n",
      "Clustering   267: ASW= 0.7933, DB= 0.2867, CH= 37964.8956\n",
      "Training epoch 268, recon_loss:0.786359, zinb_loss:0.647032, cluster_loss:0.158946\n",
      "Clustering   268: ASW= 0.7931, DB= 0.2860, CH= 38431.3046\n",
      "Training epoch 269, recon_loss:0.786446, zinb_loss:0.646833, cluster_loss:0.158870\n",
      "Clustering   269: ASW= 0.7937, DB= 0.2860, CH= 38059.9492\n",
      "Training epoch 270, recon_loss:0.786403, zinb_loss:0.646913, cluster_loss:0.158752\n",
      "Clustering   270: ASW= 0.7932, DB= 0.2856, CH= 38525.2092\n",
      "Training epoch 271, recon_loss:0.786725, zinb_loss:0.646734, cluster_loss:0.158752\n",
      "Clustering   271: ASW= 0.7940, DB= 0.2856, CH= 38133.0860\n",
      "Training epoch 272, recon_loss:0.786713, zinb_loss:0.646807, cluster_loss:0.158641\n",
      "Clustering   272: ASW= 0.7935, DB= 0.2858, CH= 38609.2539\n",
      "Training epoch 273, recon_loss:0.787074, zinb_loss:0.646615, cluster_loss:0.158680\n",
      "Clustering   273: ASW= 0.7942, DB= 0.2850, CH= 38191.5029\n",
      "Training epoch 274, recon_loss:0.786879, zinb_loss:0.646687, cluster_loss:0.158559\n",
      "Clustering   274: ASW= 0.7937, DB= 0.2849, CH= 38650.2062\n",
      "Training epoch 275, recon_loss:0.787080, zinb_loss:0.646483, cluster_loss:0.158592\n",
      "Clustering   275: ASW= 0.7943, DB= 0.2846, CH= 38265.2667\n",
      "Training epoch 276, recon_loss:0.786802, zinb_loss:0.646569, cluster_loss:0.158478\n",
      "Clustering   276: ASW= 0.7940, DB= 0.2844, CH= 38676.9603\n",
      "Training epoch 277, recon_loss:0.786903, zinb_loss:0.646390, cluster_loss:0.158524\n",
      "Clustering   277: ASW= 0.7944, DB= 0.2845, CH= 38340.0552\n",
      "Training epoch 278, recon_loss:0.786682, zinb_loss:0.646510, cluster_loss:0.158427\n",
      "Clustering   278: ASW= 0.7942, DB= 0.2841, CH= 38692.6829\n",
      "Training epoch 279, recon_loss:0.786608, zinb_loss:0.646394, cluster_loss:0.158468\n",
      "Clustering   279: ASW= 0.7945, DB= 0.2841, CH= 38425.6102\n",
      "Training epoch 280, recon_loss:0.786587, zinb_loss:0.646562, cluster_loss:0.158385\n",
      "Clustering   280: ASW= 0.7944, DB= 0.2838, CH= 38691.1363\n",
      "Training epoch 281, recon_loss:0.786411, zinb_loss:0.646533, cluster_loss:0.158444\n",
      "Clustering   281: ASW= 0.7946, DB= 0.2837, CH= 38528.6757\n",
      "Training epoch 282, recon_loss:0.786670, zinb_loss:0.646774, cluster_loss:0.158411\n",
      "Clustering   282: ASW= 0.7946, DB= 0.2836, CH= 38678.7303\n",
      "Training epoch 283, recon_loss:0.786470, zinb_loss:0.646825, cluster_loss:0.158500\n",
      "Clustering   283: ASW= 0.7947, DB= 0.2836, CH= 38648.4502\n",
      "Training epoch 284, recon_loss:0.786706, zinb_loss:0.647098, cluster_loss:0.158449\n",
      "Clustering   284: ASW= 0.7948, DB= 0.2832, CH= 38672.7606\n",
      "Training epoch 285, recon_loss:0.786450, zinb_loss:0.647113, cluster_loss:0.158513\n",
      "Clustering   285: ASW= 0.7949, DB= 0.2836, CH= 38754.3535\n",
      "Training epoch 286, recon_loss:0.786486, zinb_loss:0.647346, cluster_loss:0.158366\n",
      "Clustering   286: ASW= 0.7950, DB= 0.2837, CH= 38703.0693\n",
      "Training epoch 287, recon_loss:0.786154, zinb_loss:0.647228, cluster_loss:0.158382\n",
      "Clustering   287: ASW= 0.7951, DB= 0.2830, CH= 38846.0095\n",
      "Training epoch 288, recon_loss:0.786063, zinb_loss:0.647402, cluster_loss:0.158175\n",
      "Clustering   288: ASW= 0.7952, DB= 0.2835, CH= 38762.8936\n",
      "Training epoch 289, recon_loss:0.785666, zinb_loss:0.647175, cluster_loss:0.158158\n",
      "Clustering   289: ASW= 0.7954, DB= 0.2828, CH= 38958.1805\n",
      "Training epoch 290, recon_loss:0.785566, zinb_loss:0.647319, cluster_loss:0.157980\n",
      "Clustering   290: ASW= 0.7954, DB= 0.2827, CH= 38841.4063\n",
      "Training epoch 291, recon_loss:0.785194, zinb_loss:0.647073, cluster_loss:0.157953\n",
      "Clustering   291: ASW= 0.7958, DB= 0.2826, CH= 39073.1084\n",
      "Training epoch 292, recon_loss:0.785157, zinb_loss:0.647207, cluster_loss:0.157845\n",
      "Clustering   292: ASW= 0.7956, DB= 0.2823, CH= 38907.0059\n",
      "Training epoch 293, recon_loss:0.784849, zinb_loss:0.646977, cluster_loss:0.157809\n",
      "Clustering   293: ASW= 0.7961, DB= 0.2822, CH= 39176.9252\n",
      "Training epoch 294, recon_loss:0.784885, zinb_loss:0.647112, cluster_loss:0.157785\n",
      "Clustering   294: ASW= 0.7957, DB= 0.2820, CH= 38971.1153\n",
      "Training epoch 295, recon_loss:0.784659, zinb_loss:0.646913, cluster_loss:0.157729\n",
      "Clustering   295: ASW= 0.7965, DB= 0.2813, CH= 39267.3010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 296, recon_loss:0.784774, zinb_loss:0.647065, cluster_loss:0.157790\n",
      "Clustering   296: ASW= 0.7958, DB= 0.2818, CH= 39031.8003\n",
      "Training epoch 297, recon_loss:0.784628, zinb_loss:0.646877, cluster_loss:0.157712\n",
      "Clustering   297: ASW= 0.7968, DB= 0.2810, CH= 39346.3105\n",
      "Training epoch 298, recon_loss:0.784813, zinb_loss:0.647039, cluster_loss:0.157856\n",
      "Clustering   298: ASW= 0.7958, DB= 0.2814, CH= 39075.4216\n",
      "Training epoch 299, recon_loss:0.784744, zinb_loss:0.646857, cluster_loss:0.157740\n",
      "Clustering   299: ASW= 0.7971, DB= 0.2805, CH= 39409.4129\n",
      "Training epoch 300, recon_loss:0.784963, zinb_loss:0.647021, cluster_loss:0.157959\n",
      "Clustering   300: ASW= 0.7958, DB= 0.2809, CH= 39105.4821\n",
      "Training epoch 301, recon_loss:0.784951, zinb_loss:0.646833, cluster_loss:0.157789\n",
      "Clustering   301: ASW= 0.7973, DB= 0.2801, CH= 39453.4958\n",
      "Training epoch 302, recon_loss:0.785138, zinb_loss:0.646986, cluster_loss:0.158054\n",
      "Clustering   302: ASW= 0.7957, DB= 0.2801, CH= 39148.1492\n",
      "Training epoch 303, recon_loss:0.785166, zinb_loss:0.646791, cluster_loss:0.157838\n",
      "Clustering   303: ASW= 0.7976, DB= 0.2799, CH= 39481.3289\n",
      "Training epoch 304, recon_loss:0.785283, zinb_loss:0.646926, cluster_loss:0.158131\n",
      "Clustering   304: ASW= 0.7956, DB= 0.2797, CH= 39185.3458\n",
      "Training epoch 305, recon_loss:0.785348, zinb_loss:0.646732, cluster_loss:0.157872\n",
      "Clustering   305: ASW= 0.7978, DB= 0.2795, CH= 39500.4129\n",
      "Training epoch 306, recon_loss:0.785432, zinb_loss:0.646834, cluster_loss:0.158187\n",
      "Clustering   306: ASW= 0.7955, DB= 0.2799, CH= 39262.2548\n",
      "Training epoch 307, recon_loss:0.785535, zinb_loss:0.646657, cluster_loss:0.157918\n",
      "Clustering   307: ASW= 0.7981, DB= 0.2793, CH= 39515.7214\n",
      "Training epoch 308, recon_loss:0.785671, zinb_loss:0.646717, cluster_loss:0.158265\n",
      "Clustering   308: ASW= 0.7954, DB= 0.2799, CH= 39334.8905\n",
      "Training epoch 309, recon_loss:0.785794, zinb_loss:0.646585, cluster_loss:0.158013\n",
      "Clustering   309: ASW= 0.7983, DB= 0.2791, CH= 39528.2604\n",
      "Training epoch 310, recon_loss:0.786066, zinb_loss:0.646601, cluster_loss:0.158389\n",
      "Clustering   310: ASW= 0.7953, DB= 0.2801, CH= 39408.4300\n",
      "Training epoch 311, recon_loss:0.785987, zinb_loss:0.646495, cluster_loss:0.158117\n",
      "Clustering   311: ASW= 0.7986, DB= 0.2789, CH= 39538.2086\n",
      "Training epoch 312, recon_loss:0.786186, zinb_loss:0.646461, cluster_loss:0.158411\n",
      "Clustering   312: ASW= 0.7954, DB= 0.2803, CH= 39498.6042\n",
      "Training epoch 313, recon_loss:0.785747, zinb_loss:0.646367, cluster_loss:0.158097\n",
      "Clustering   313: ASW= 0.7986, DB= 0.2789, CH= 39550.1381\n",
      "Training epoch 314, recon_loss:0.785798, zinb_loss:0.646298, cluster_loss:0.158266\n",
      "Clustering   314: ASW= 0.7956, DB= 0.2804, CH= 39624.6997\n",
      "Training epoch 315, recon_loss:0.785159, zinb_loss:0.646244, cluster_loss:0.157953\n",
      "Clustering   315: ASW= 0.7986, DB= 0.2792, CH= 39563.3367\n",
      "Training epoch 316, recon_loss:0.785254, zinb_loss:0.646180, cluster_loss:0.158050\n",
      "Clustering   316: ASW= 0.7962, DB= 0.2800, CH= 39775.2930\n",
      "Training epoch 317, recon_loss:0.784736, zinb_loss:0.646198, cluster_loss:0.157841\n",
      "Clustering   317: ASW= 0.7984, DB= 0.2793, CH= 39561.8870\n",
      "Training epoch 318, recon_loss:0.785061, zinb_loss:0.646195, cluster_loss:0.157938\n",
      "Clustering   318: ASW= 0.7969, DB= 0.2793, CH= 39948.4093\n",
      "Training epoch 319, recon_loss:0.784805, zinb_loss:0.646258, cluster_loss:0.157859\n",
      "Clustering   319: ASW= 0.7982, DB= 0.2791, CH= 39533.0317\n",
      "Training epoch 320, recon_loss:0.785335, zinb_loss:0.646327, cluster_loss:0.157975\n",
      "Clustering   320: ASW= 0.7975, DB= 0.2784, CH= 40100.7149\n",
      "Training epoch 321, recon_loss:0.785166, zinb_loss:0.646330, cluster_loss:0.157830\n",
      "Clustering   321: ASW= 0.7981, DB= 0.2800, CH= 39540.4951\n",
      "Training epoch 322, recon_loss:0.785838, zinb_loss:0.646426, cluster_loss:0.157942\n",
      "Clustering   322: ASW= 0.7980, DB= 0.2784, CH= 40207.6920\n",
      "Training epoch 323, recon_loss:0.786024, zinb_loss:0.646382, cluster_loss:0.157724\n",
      "Clustering   323: ASW= 0.7980, DB= 0.2803, CH= 39585.8443\n",
      "Training epoch 324, recon_loss:0.787200, zinb_loss:0.646475, cluster_loss:0.157926\n",
      "Clustering   324: ASW= 0.7982, DB= 0.2784, CH= 40239.6111\n",
      "Training epoch 325, recon_loss:0.787723, zinb_loss:0.646466, cluster_loss:0.157706\n",
      "Clustering   325: ASW= 0.7982, DB= 0.2803, CH= 39649.3508\n",
      "Training epoch 326, recon_loss:0.788941, zinb_loss:0.646481, cluster_loss:0.158034\n",
      "Clustering   326: ASW= 0.7982, DB= 0.2782, CH= 40175.1510\n",
      "Training epoch 327, recon_loss:0.788127, zinb_loss:0.646453, cluster_loss:0.157632\n",
      "Clustering   327: ASW= 0.7985, DB= 0.2799, CH= 39732.8075\n",
      "Training epoch 328, recon_loss:0.788174, zinb_loss:0.646293, cluster_loss:0.157867\n",
      "Clustering   328: ASW= 0.7980, DB= 0.2780, CH= 40093.6703\n",
      "Training epoch 329, recon_loss:0.787187, zinb_loss:0.646260, cluster_loss:0.157376\n",
      "Clustering   329: ASW= 0.7989, DB= 0.2792, CH= 39842.3204\n",
      "Training epoch 330, recon_loss:0.787006, zinb_loss:0.646065, cluster_loss:0.157621\n",
      "Clustering   330: ASW= 0.7978, DB= 0.2787, CH= 40089.6482\n",
      "Training epoch 331, recon_loss:0.786532, zinb_loss:0.646061, cluster_loss:0.157240\n",
      "Clustering   331: ASW= 0.7994, DB= 0.2783, CH= 39963.9266\n",
      "Training epoch 332, recon_loss:0.786244, zinb_loss:0.645890, cluster_loss:0.157563\n",
      "Clustering   332: ASW= 0.7976, DB= 0.2783, CH= 40074.4025\n",
      "Training epoch 333, recon_loss:0.786043, zinb_loss:0.645923, cluster_loss:0.157236\n",
      "Clustering   333: ASW= 0.7997, DB= 0.2775, CH= 40057.8489\n",
      "Training epoch 334, recon_loss:0.785571, zinb_loss:0.645771, cluster_loss:0.157590\n",
      "Clustering   334: ASW= 0.7974, DB= 0.2780, CH= 40065.8502\n",
      "Training epoch 335, recon_loss:0.785604, zinb_loss:0.645851, cluster_loss:0.157250\n",
      "Clustering   335: ASW= 0.8001, DB= 0.2771, CH= 40163.3839\n",
      "Training epoch 336, recon_loss:0.785193, zinb_loss:0.645705, cluster_loss:0.157648\n",
      "Clustering   336: ASW= 0.7974, DB= 0.2782, CH= 40081.0488\n",
      "Training epoch 337, recon_loss:0.785358, zinb_loss:0.645820, cluster_loss:0.157300\n",
      "Clustering   337: ASW= 0.8003, DB= 0.2771, CH= 40284.0330\n",
      "Training epoch 338, recon_loss:0.785027, zinb_loss:0.645683, cluster_loss:0.157771\n",
      "Clustering   338: ASW= 0.7973, DB= 0.2774, CH= 40072.7514\n",
      "Training epoch 339, recon_loss:0.785255, zinb_loss:0.645821, cluster_loss:0.157430\n",
      "Clustering   339: ASW= 0.8006, DB= 0.2767, CH= 40381.5408\n",
      "Training epoch 340, recon_loss:0.784987, zinb_loss:0.645698, cluster_loss:0.157981\n",
      "Clustering   340: ASW= 0.7973, DB= 0.2775, CH= 40062.0131\n",
      "Training epoch 341, recon_loss:0.785221, zinb_loss:0.645849, cluster_loss:0.157647\n",
      "Clustering   341: ASW= 0.8008, DB= 0.2761, CH= 40453.8745\n",
      "Training epoch 342, recon_loss:0.784933, zinb_loss:0.645746, cluster_loss:0.158185\n",
      "Clustering   342: ASW= 0.7974, DB= 0.2773, CH= 40033.8847\n",
      "Training epoch 343, recon_loss:0.785092, zinb_loss:0.645890, cluster_loss:0.157806\n",
      "Clustering   343: ASW= 0.8009, DB= 0.2760, CH= 40508.9414\n",
      "Training epoch 344, recon_loss:0.784587, zinb_loss:0.645782, cluster_loss:0.158124\n",
      "Clustering   344: ASW= 0.7976, DB= 0.2772, CH= 40052.8884\n",
      "Training epoch 345, recon_loss:0.784670, zinb_loss:0.645877, cluster_loss:0.157720\n",
      "Clustering   345: ASW= 0.8009, DB= 0.2759, CH= 40546.2951\n",
      "Training epoch 346, recon_loss:0.784024, zinb_loss:0.645771, cluster_loss:0.157836\n",
      "Clustering   346: ASW= 0.7980, DB= 0.2769, CH= 40128.2500\n",
      "Training epoch 347, recon_loss:0.784118, zinb_loss:0.645820, cluster_loss:0.157466\n",
      "Clustering   347: ASW= 0.8009, DB= 0.2762, CH= 40603.3594\n",
      "Training epoch 348, recon_loss:0.783497, zinb_loss:0.645740, cluster_loss:0.157521\n",
      "Clustering   348: ASW= 0.7984, DB= 0.2768, CH= 40243.0112\n",
      "Training epoch 349, recon_loss:0.783688, zinb_loss:0.645762, cluster_loss:0.157234\n",
      "Clustering   349: ASW= 0.8009, DB= 0.2757, CH= 40664.2492\n",
      "Training epoch 350, recon_loss:0.783155, zinb_loss:0.645719, cluster_loss:0.157285\n",
      "Clustering   350: ASW= 0.7988, DB= 0.2762, CH= 40344.5852\n",
      "Training epoch 351, recon_loss:0.783432, zinb_loss:0.645727, cluster_loss:0.157060\n",
      "Clustering   351: ASW= 0.8011, DB= 0.2757, CH= 40735.4760\n",
      "Training epoch 352, recon_loss:0.782982, zinb_loss:0.645710, cluster_loss:0.157115\n",
      "Clustering   352: ASW= 0.7991, DB= 0.2758, CH= 40446.2223\n",
      "Training epoch 353, recon_loss:0.783337, zinb_loss:0.645714, cluster_loss:0.156937\n",
      "Clustering   353: ASW= 0.8012, DB= 0.2753, CH= 40803.3542\n",
      "Training epoch 354, recon_loss:0.782993, zinb_loss:0.645717, cluster_loss:0.156994\n",
      "Clustering   354: ASW= 0.7994, DB= 0.2753, CH= 40537.9631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 355, recon_loss:0.783435, zinb_loss:0.645735, cluster_loss:0.156861\n",
      "Clustering   355: ASW= 0.8013, DB= 0.2754, CH= 40868.4653\n",
      "Training epoch 356, recon_loss:0.783249, zinb_loss:0.645768, cluster_loss:0.156935\n",
      "Clustering   356: ASW= 0.7996, DB= 0.2753, CH= 40612.2424\n",
      "Training epoch 357, recon_loss:0.783817, zinb_loss:0.645836, cluster_loss:0.156846\n",
      "Clustering   357: ASW= 0.8015, DB= 0.2749, CH= 40919.6528\n",
      "Training epoch 358, recon_loss:0.783946, zinb_loss:0.645938, cluster_loss:0.156968\n",
      "Clustering   358: ASW= 0.7997, DB= 0.2752, CH= 40670.7753\n",
      "Training epoch 359, recon_loss:0.784643, zinb_loss:0.646123, cluster_loss:0.156934\n",
      "Clustering   359: ASW= 0.8016, DB= 0.2745, CH= 40952.6551\n",
      "Training epoch 360, recon_loss:0.785131, zinb_loss:0.646333, cluster_loss:0.157133\n",
      "Clustering   360: ASW= 0.7997, DB= 0.2753, CH= 40709.6457\n",
      "Training epoch 361, recon_loss:0.785187, zinb_loss:0.646537, cluster_loss:0.157089\n",
      "Clustering   361: ASW= 0.8017, DB= 0.2741, CH= 40957.8316\n",
      "Training epoch 362, recon_loss:0.785484, zinb_loss:0.646701, cluster_loss:0.157191\n",
      "Clustering   362: ASW= 0.7997, DB= 0.2756, CH= 40744.2463\n",
      "Training epoch 363, recon_loss:0.784484, zinb_loss:0.646685, cluster_loss:0.157090\n",
      "Clustering   363: ASW= 0.8018, DB= 0.2738, CH= 40976.7268\n",
      "Training epoch 364, recon_loss:0.784661, zinb_loss:0.646711, cluster_loss:0.157001\n",
      "Clustering   364: ASW= 0.7999, DB= 0.2756, CH= 40819.1409\n",
      "Training epoch 365, recon_loss:0.783616, zinb_loss:0.646592, cluster_loss:0.156990\n",
      "Clustering   365: ASW= 0.8019, DB= 0.2735, CH= 41000.1584\n",
      "Training epoch 366, recon_loss:0.783898, zinb_loss:0.646617, cluster_loss:0.156852\n",
      "Clustering   366: ASW= 0.8001, DB= 0.2755, CH= 40914.5288\n",
      "Training epoch 367, recon_loss:0.783135, zinb_loss:0.646513, cluster_loss:0.156948\n",
      "Clustering   367: ASW= 0.8019, DB= 0.2733, CH= 41027.9324\n",
      "Training epoch 368, recon_loss:0.783522, zinb_loss:0.646562, cluster_loss:0.156804\n",
      "Clustering   368: ASW= 0.8003, DB= 0.2753, CH= 41011.6800\n",
      "Training epoch 369, recon_loss:0.782932, zinb_loss:0.646469, cluster_loss:0.156962\n",
      "Clustering   369: ASW= 0.8020, DB= 0.2732, CH= 41067.3155\n",
      "Training epoch 370, recon_loss:0.783347, zinb_loss:0.646534, cluster_loss:0.156801\n",
      "Clustering   370: ASW= 0.8005, DB= 0.2751, CH= 41097.3987\n",
      "Training epoch 371, recon_loss:0.782807, zinb_loss:0.646430, cluster_loss:0.156972\n",
      "Clustering   371: ASW= 0.8020, DB= 0.2729, CH= 41109.3090\n",
      "Training epoch 372, recon_loss:0.783209, zinb_loss:0.646492, cluster_loss:0.156778\n",
      "Clustering   372: ASW= 0.8008, DB= 0.2744, CH= 41171.1664\n",
      "Training epoch 373, recon_loss:0.782648, zinb_loss:0.646365, cluster_loss:0.156949\n",
      "Clustering   373: ASW= 0.8021, DB= 0.2729, CH= 41160.4034\n",
      "Training epoch 374, recon_loss:0.783032, zinb_loss:0.646417, cluster_loss:0.156736\n",
      "Clustering   374: ASW= 0.8010, DB= 0.2740, CH= 41250.9416\n",
      "Training epoch 375, recon_loss:0.782515, zinb_loss:0.646286, cluster_loss:0.156919\n",
      "Clustering   375: ASW= 0.8022, DB= 0.2728, CH= 41216.0965\n",
      "Training epoch 376, recon_loss:0.782903, zinb_loss:0.646333, cluster_loss:0.156712\n",
      "Clustering   376: ASW= 0.8013, DB= 0.2736, CH= 41324.4518\n",
      "Training epoch 377, recon_loss:0.782488, zinb_loss:0.646203, cluster_loss:0.156925\n",
      "Clustering   377: ASW= 0.8022, DB= 0.2725, CH= 41264.1971\n",
      "Training epoch 378, recon_loss:0.782908, zinb_loss:0.646238, cluster_loss:0.156751\n",
      "Clustering   378: ASW= 0.8016, DB= 0.2731, CH= 41389.2264\n",
      "Training epoch 379, recon_loss:0.782692, zinb_loss:0.646116, cluster_loss:0.157020\n",
      "Clustering   379: ASW= 0.8022, DB= 0.2727, CH= 41290.6948\n",
      "Training epoch 380, recon_loss:0.783153, zinb_loss:0.646126, cluster_loss:0.156918\n",
      "Clustering   380: ASW= 0.8018, DB= 0.2726, CH= 41435.4682\n",
      "Training epoch 381, recon_loss:0.783252, zinb_loss:0.646026, cluster_loss:0.157251\n",
      "Clustering   381: ASW= 0.8020, DB= 0.2726, CH= 41275.4083\n",
      "Training epoch 382, recon_loss:0.783548, zinb_loss:0.645969, cluster_loss:0.157223\n",
      "Clustering   382: ASW= 0.8019, DB= 0.2727, CH= 41434.1137\n",
      "Training epoch 383, recon_loss:0.783832, zinb_loss:0.645914, cluster_loss:0.157466\n",
      "Clustering   383: ASW= 0.8020, DB= 0.2727, CH= 41281.0251\n",
      "Training epoch 384, recon_loss:0.783618, zinb_loss:0.645785, cluster_loss:0.157459\n",
      "Clustering   384: ASW= 0.8021, DB= 0.2723, CH= 41414.9984\n",
      "Training epoch 385, recon_loss:0.784032, zinb_loss:0.645786, cluster_loss:0.157538\n",
      "Clustering   385: ASW= 0.8019, DB= 0.2727, CH= 41290.4623\n",
      "Training epoch 386, recon_loss:0.783367, zinb_loss:0.645607, cluster_loss:0.157531\n",
      "Clustering   386: ASW= 0.8021, DB= 0.2723, CH= 41392.3360\n",
      "Training epoch 387, recon_loss:0.783855, zinb_loss:0.645688, cluster_loss:0.157443\n",
      "Clustering   387: ASW= 0.8020, DB= 0.2727, CH= 41327.3356\n",
      "Training epoch 388, recon_loss:0.782986, zinb_loss:0.645506, cluster_loss:0.157428\n",
      "Clustering   388: ASW= 0.8023, DB= 0.2721, CH= 41412.7919\n",
      "Training epoch 389, recon_loss:0.783655, zinb_loss:0.645650, cluster_loss:0.157300\n",
      "Clustering   389: ASW= 0.8021, DB= 0.2723, CH= 41349.7366\n",
      "Training epoch 390, recon_loss:0.782935, zinb_loss:0.645518, cluster_loss:0.157319\n",
      "Clustering   390: ASW= 0.8024, DB= 0.2721, CH= 41447.6609\n",
      "Training epoch 391, recon_loss:0.783760, zinb_loss:0.645714, cluster_loss:0.157195\n",
      "Clustering   391: ASW= 0.8023, DB= 0.2721, CH= 41368.2446\n",
      "Training epoch 392, recon_loss:0.783295, zinb_loss:0.645631, cluster_loss:0.157276\n",
      "Clustering   392: ASW= 0.8025, DB= 0.2721, CH= 41467.0872\n",
      "Training epoch 393, recon_loss:0.783961, zinb_loss:0.645831, cluster_loss:0.157075\n",
      "Clustering   393: ASW= 0.8026, DB= 0.2714, CH= 41391.1979\n",
      "Training epoch 394, recon_loss:0.783588, zinb_loss:0.645717, cluster_loss:0.157175\n",
      "Clustering   394: ASW= 0.8026, DB= 0.2716, CH= 41504.7855\n",
      "Training epoch 395, recon_loss:0.783971, zinb_loss:0.645875, cluster_loss:0.156943\n",
      "Clustering   395: ASW= 0.8027, DB= 0.2711, CH= 41413.4835\n",
      "Training epoch 396, recon_loss:0.783731, zinb_loss:0.645721, cluster_loss:0.157071\n",
      "Clustering   396: ASW= 0.8026, DB= 0.2714, CH= 41567.0369\n",
      "Training epoch 397, recon_loss:0.783928, zinb_loss:0.645854, cluster_loss:0.156864\n",
      "Clustering   397: ASW= 0.8028, DB= 0.2709, CH= 41421.8283\n",
      "Training epoch 398, recon_loss:0.783817, zinb_loss:0.645672, cluster_loss:0.157010\n",
      "Clustering   398: ASW= 0.8029, DB= 0.2710, CH= 41623.6251\n",
      "Training epoch 399, recon_loss:0.783943, zinb_loss:0.645799, cluster_loss:0.156887\n",
      "Clustering   399: ASW= 0.8028, DB= 0.2708, CH= 41409.6842\n",
      "Training epoch 400, recon_loss:0.783811, zinb_loss:0.645585, cluster_loss:0.156979\n",
      "Clustering   400: ASW= 0.8031, DB= 0.2707, CH= 41701.9741\n",
      "Training epoch 401, recon_loss:0.783887, zinb_loss:0.645714, cluster_loss:0.156941\n",
      "Clustering   401: ASW= 0.8029, DB= 0.2702, CH= 41391.6178\n",
      "Training epoch 402, recon_loss:0.783523, zinb_loss:0.645475, cluster_loss:0.156873\n",
      "Clustering   402: ASW= 0.8032, DB= 0.2705, CH= 41779.6771\n",
      "Training epoch 403, recon_loss:0.783517, zinb_loss:0.645600, cluster_loss:0.156855\n",
      "Clustering   403: ASW= 0.8031, DB= 0.2699, CH= 41439.2827\n",
      "Training epoch 404, recon_loss:0.782992, zinb_loss:0.645342, cluster_loss:0.156709\n",
      "Clustering   404: ASW= 0.8032, DB= 0.2705, CH= 41849.7804\n",
      "Training epoch 405, recon_loss:0.783020, zinb_loss:0.645486, cluster_loss:0.156713\n",
      "Clustering   405: ASW= 0.8033, DB= 0.2696, CH= 41521.0471\n",
      "Training epoch 406, recon_loss:0.782500, zinb_loss:0.645229, cluster_loss:0.156569\n",
      "Clustering   406: ASW= 0.8033, DB= 0.2706, CH= 41913.7505\n",
      "Training epoch 407, recon_loss:0.782619, zinb_loss:0.645398, cluster_loss:0.156615\n",
      "Clustering   407: ASW= 0.8035, DB= 0.2692, CH= 41600.9177\n",
      "Training epoch 408, recon_loss:0.782161, zinb_loss:0.645145, cluster_loss:0.156476\n",
      "Clustering   408: ASW= 0.8033, DB= 0.2708, CH= 41979.3412\n",
      "Training epoch 409, recon_loss:0.782339, zinb_loss:0.645332, cluster_loss:0.156558\n",
      "Clustering   409: ASW= 0.8037, DB= 0.2689, CH= 41673.6319\n",
      "Training epoch 410, recon_loss:0.781991, zinb_loss:0.645082, cluster_loss:0.156423\n",
      "Clustering   410: ASW= 0.8034, DB= 0.2708, CH= 42039.2813\n",
      "Training epoch 411, recon_loss:0.782239, zinb_loss:0.645280, cluster_loss:0.156537\n",
      "Clustering   411: ASW= 0.8039, DB= 0.2689, CH= 41743.5516\n",
      "Training epoch 412, recon_loss:0.782046, zinb_loss:0.645029, cluster_loss:0.156411\n",
      "Clustering   412: ASW= 0.8034, DB= 0.2708, CH= 42094.1068\n",
      "Training epoch 413, recon_loss:0.782291, zinb_loss:0.645225, cluster_loss:0.156547\n",
      "Clustering   413: ASW= 0.8040, DB= 0.2688, CH= 41804.2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 414, recon_loss:0.782210, zinb_loss:0.644979, cluster_loss:0.156436\n",
      "Clustering   414: ASW= 0.8035, DB= 0.2709, CH= 42138.9944\n",
      "Training epoch 415, recon_loss:0.782168, zinb_loss:0.645153, cluster_loss:0.156560\n",
      "Clustering   415: ASW= 0.8041, DB= 0.2686, CH= 41862.9103\n",
      "Training epoch 416, recon_loss:0.782156, zinb_loss:0.644922, cluster_loss:0.156448\n",
      "Clustering   416: ASW= 0.8035, DB= 0.2707, CH= 42174.9479\n",
      "Training epoch 417, recon_loss:0.782045, zinb_loss:0.645084, cluster_loss:0.156555\n",
      "Clustering   417: ASW= 0.8041, DB= 0.2684, CH= 41912.7899\n",
      "Training epoch 418, recon_loss:0.782157, zinb_loss:0.644887, cluster_loss:0.156476\n",
      "Clustering   418: ASW= 0.8036, DB= 0.2706, CH= 42212.3904\n",
      "Training epoch 419, recon_loss:0.782355, zinb_loss:0.645083, cluster_loss:0.156609\n",
      "Clustering   419: ASW= 0.8041, DB= 0.2681, CH= 41932.8975\n",
      "Training epoch 420, recon_loss:0.782541, zinb_loss:0.644936, cluster_loss:0.156559\n",
      "Clustering   420: ASW= 0.8037, DB= 0.2705, CH= 42242.9432\n",
      "Training epoch 421, recon_loss:0.783104, zinb_loss:0.645197, cluster_loss:0.156689\n",
      "Clustering   421: ASW= 0.8041, DB= 0.2681, CH= 41947.9524\n",
      "Training epoch 422, recon_loss:0.782922, zinb_loss:0.645078, cluster_loss:0.156609\n",
      "Clustering   422: ASW= 0.8038, DB= 0.2701, CH= 42261.6981\n",
      "Training epoch 423, recon_loss:0.783453, zinb_loss:0.645352, cluster_loss:0.156645\n",
      "Clustering   423: ASW= 0.8041, DB= 0.2681, CH= 41964.3545\n",
      "Training epoch 424, recon_loss:0.782791, zinb_loss:0.645249, cluster_loss:0.156517\n",
      "Clustering   424: ASW= 0.8041, DB= 0.2697, CH= 42304.6991\n",
      "Training epoch 425, recon_loss:0.783186, zinb_loss:0.645497, cluster_loss:0.156497\n",
      "Clustering   425: ASW= 0.8040, DB= 0.2682, CH= 41990.7505\n",
      "Training epoch 426, recon_loss:0.782374, zinb_loss:0.645421, cluster_loss:0.156370\n",
      "Clustering   426: ASW= 0.8044, DB= 0.2693, CH= 42360.4523\n",
      "Training epoch 427, recon_loss:0.782788, zinb_loss:0.645659, cluster_loss:0.156373\n",
      "Clustering   427: ASW= 0.8040, DB= 0.2682, CH= 42025.9546\n",
      "Training epoch 428, recon_loss:0.781998, zinb_loss:0.645603, cluster_loss:0.156259\n",
      "Clustering   428: ASW= 0.8046, DB= 0.2689, CH= 42428.8099\n",
      "Training epoch 429, recon_loss:0.782397, zinb_loss:0.645810, cluster_loss:0.156301\n",
      "Clustering   429: ASW= 0.8039, DB= 0.2686, CH= 42063.8559\n",
      "Training epoch 430, recon_loss:0.781668, zinb_loss:0.645776, cluster_loss:0.156176\n",
      "Clustering   430: ASW= 0.8050, DB= 0.2683, CH= 42502.7234\n",
      "Training epoch 431, recon_loss:0.782070, zinb_loss:0.645920, cluster_loss:0.156266\n",
      "Clustering   431: ASW= 0.8039, DB= 0.2686, CH= 42095.0676\n",
      "Training epoch 432, recon_loss:0.781397, zinb_loss:0.645904, cluster_loss:0.156120\n",
      "Clustering   432: ASW= 0.8053, DB= 0.2678, CH= 42573.0110\n",
      "Training epoch 433, recon_loss:0.781764, zinb_loss:0.645975, cluster_loss:0.156248\n",
      "Clustering   433: ASW= 0.8039, DB= 0.2686, CH= 42122.1112\n",
      "Training epoch 434, recon_loss:0.781163, zinb_loss:0.645964, cluster_loss:0.156066\n",
      "Clustering   434: ASW= 0.8056, DB= 0.2672, CH= 42643.4703\n",
      "Training epoch 435, recon_loss:0.781488, zinb_loss:0.645980, cluster_loss:0.156224\n",
      "Clustering   435: ASW= 0.8040, DB= 0.2686, CH= 42157.9828\n",
      "Training epoch 436, recon_loss:0.781028, zinb_loss:0.645971, cluster_loss:0.156016\n",
      "Clustering   436: ASW= 0.8059, DB= 0.2669, CH= 42706.4407\n",
      "Training epoch 437, recon_loss:0.781396, zinb_loss:0.646009, cluster_loss:0.156229\n",
      "Clustering   437: ASW= 0.8040, DB= 0.2685, CH= 42189.7299\n",
      "Training epoch 438, recon_loss:0.781202, zinb_loss:0.646021, cluster_loss:0.156027\n",
      "Clustering   438: ASW= 0.8061, DB= 0.2669, CH= 42757.8668\n",
      "Training epoch 439, recon_loss:0.781787, zinb_loss:0.646141, cluster_loss:0.156332\n",
      "Clustering   439: ASW= 0.8041, DB= 0.2683, CH= 42213.2693\n",
      "Training epoch 440, recon_loss:0.781872, zinb_loss:0.646158, cluster_loss:0.156149\n",
      "Clustering   440: ASW= 0.8063, DB= 0.2667, CH= 42792.5731\n",
      "Training epoch 441, recon_loss:0.782709, zinb_loss:0.646264, cluster_loss:0.156557\n",
      "Clustering   441: ASW= 0.8041, DB= 0.2680, CH= 42226.4646\n",
      "Training epoch 442, recon_loss:0.782786, zinb_loss:0.646134, cluster_loss:0.156328\n",
      "Clustering   442: ASW= 0.8064, DB= 0.2671, CH= 42785.2483\n",
      "Training epoch 443, recon_loss:0.783539, zinb_loss:0.646057, cluster_loss:0.156781\n",
      "Clustering   443: ASW= 0.8040, DB= 0.2681, CH= 42225.7401\n",
      "Training epoch 444, recon_loss:0.783658, zinb_loss:0.645823, cluster_loss:0.156513\n",
      "Clustering   444: ASW= 0.8063, DB= 0.2669, CH= 42714.9748\n",
      "Training epoch 445, recon_loss:0.784057, zinb_loss:0.645686, cluster_loss:0.156963\n",
      "Clustering   445: ASW= 0.8039, DB= 0.2681, CH= 42212.4398\n",
      "Training epoch 446, recon_loss:0.784271, zinb_loss:0.645471, cluster_loss:0.156662\n",
      "Clustering   446: ASW= 0.8063, DB= 0.2670, CH= 42647.6379\n",
      "Training epoch 447, recon_loss:0.784022, zinb_loss:0.645337, cluster_loss:0.157054\n",
      "Clustering   447: ASW= 0.8039, DB= 0.2679, CH= 42207.8667\n",
      "Training epoch 448, recon_loss:0.784171, zinb_loss:0.645173, cluster_loss:0.156692\n",
      "Clustering   448: ASW= 0.8062, DB= 0.2677, CH= 42635.0963\n",
      "Training epoch 449, recon_loss:0.783479, zinb_loss:0.645044, cluster_loss:0.156997\n",
      "Clustering   449: ASW= 0.8041, DB= 0.2681, CH= 42247.4413\n",
      "Training epoch 450, recon_loss:0.783575, zinb_loss:0.644946, cluster_loss:0.156609\n",
      "Clustering   450: ASW= 0.8062, DB= 0.2675, CH= 42667.0624\n",
      "Training epoch 451, recon_loss:0.782723, zinb_loss:0.644834, cluster_loss:0.156819\n",
      "Clustering   451: ASW= 0.8043, DB= 0.2678, CH= 42296.4157\n",
      "Training epoch 452, recon_loss:0.782901, zinb_loss:0.644782, cluster_loss:0.156461\n",
      "Clustering   452: ASW= 0.8062, DB= 0.2674, CH= 42713.3822\n",
      "Training epoch 453, recon_loss:0.782083, zinb_loss:0.644685, cluster_loss:0.156610\n",
      "Clustering   453: ASW= 0.8046, DB= 0.2674, CH= 42357.9854\n",
      "Training epoch 454, recon_loss:0.782394, zinb_loss:0.644658, cluster_loss:0.156294\n",
      "Clustering   454: ASW= 0.8062, DB= 0.2674, CH= 42766.5342\n",
      "Training epoch 455, recon_loss:0.781705, zinb_loss:0.644589, cluster_loss:0.156417\n",
      "Clustering   455: ASW= 0.8049, DB= 0.2668, CH= 42433.6958\n",
      "Training epoch 456, recon_loss:0.782064, zinb_loss:0.644581, cluster_loss:0.156152\n",
      "Clustering   456: ASW= 0.8062, DB= 0.2675, CH= 42826.2027\n",
      "Training epoch 457, recon_loss:0.781430, zinb_loss:0.644544, cluster_loss:0.156243\n",
      "Clustering   457: ASW= 0.8052, DB= 0.2666, CH= 42521.7148\n",
      "Training epoch 458, recon_loss:0.781874, zinb_loss:0.644533, cluster_loss:0.156021\n",
      "Clustering   458: ASW= 0.8062, DB= 0.2669, CH= 42869.8353\n",
      "Training epoch 459, recon_loss:0.781307, zinb_loss:0.644540, cluster_loss:0.156071\n",
      "Clustering   459: ASW= 0.8056, DB= 0.2665, CH= 42632.2811\n",
      "Training epoch 460, recon_loss:0.781876, zinb_loss:0.644515, cluster_loss:0.155935\n",
      "Clustering   460: ASW= 0.8062, DB= 0.2664, CH= 42906.1827\n",
      "Training epoch 461, recon_loss:0.781477, zinb_loss:0.644597, cluster_loss:0.155954\n",
      "Clustering   461: ASW= 0.8059, DB= 0.2662, CH= 42738.7006\n",
      "Training epoch 462, recon_loss:0.782171, zinb_loss:0.644557, cluster_loss:0.155950\n",
      "Clustering   462: ASW= 0.8063, DB= 0.2657, CH= 42927.0080\n",
      "Training epoch 463, recon_loss:0.781933, zinb_loss:0.644721, cluster_loss:0.155947\n",
      "Clustering   463: ASW= 0.8061, DB= 0.2660, CH= 42803.4954\n",
      "Training epoch 464, recon_loss:0.782661, zinb_loss:0.644673, cluster_loss:0.156098\n",
      "Clustering   464: ASW= 0.8063, DB= 0.2657, CH= 42963.6026\n",
      "Training epoch 465, recon_loss:0.782513, zinb_loss:0.644900, cluster_loss:0.156089\n",
      "Clustering   465: ASW= 0.8062, DB= 0.2656, CH= 42832.2117\n",
      "Training epoch 466, recon_loss:0.783129, zinb_loss:0.644843, cluster_loss:0.156380\n",
      "Clustering   466: ASW= 0.8064, DB= 0.2656, CH= 42975.8083\n",
      "Training epoch 467, recon_loss:0.782888, zinb_loss:0.645055, cluster_loss:0.156330\n",
      "Clustering   467: ASW= 0.8062, DB= 0.2656, CH= 42820.7894\n",
      "Training epoch 468, recon_loss:0.783332, zinb_loss:0.644984, cluster_loss:0.156721\n",
      "Clustering   468: ASW= 0.8065, DB= 0.2657, CH= 42953.0430\n",
      "Training epoch 469, recon_loss:0.782964, zinb_loss:0.645104, cluster_loss:0.156572\n",
      "Clustering   469: ASW= 0.8060, DB= 0.2657, CH= 42802.0896\n",
      "Training epoch 470, recon_loss:0.783184, zinb_loss:0.645021, cluster_loss:0.157003\n",
      "Clustering   470: ASW= 0.8065, DB= 0.2661, CH= 42880.7125\n",
      "Training epoch 471, recon_loss:0.782867, zinb_loss:0.645058, cluster_loss:0.156684\n",
      "Clustering   471: ASW= 0.8059, DB= 0.2660, CH= 42818.4602\n",
      "Training epoch 472, recon_loss:0.782944, zinb_loss:0.644980, cluster_loss:0.157168\n",
      "Clustering   472: ASW= 0.8064, DB= 0.2656, CH= 42774.3863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 473, recon_loss:0.782769, zinb_loss:0.645017, cluster_loss:0.156714\n",
      "Clustering   473: ASW= 0.8058, DB= 0.2670, CH= 42854.1509\n",
      "Training epoch 474, recon_loss:0.782834, zinb_loss:0.644981, cluster_loss:0.157267\n",
      "Clustering   474: ASW= 0.8062, DB= 0.2654, CH= 42676.7876\n",
      "Training epoch 475, recon_loss:0.782649, zinb_loss:0.645059, cluster_loss:0.156579\n",
      "Clustering   475: ASW= 0.8058, DB= 0.2667, CH= 42905.8273\n",
      "Training epoch 476, recon_loss:0.782543, zinb_loss:0.645011, cluster_loss:0.157114\n",
      "Clustering   476: ASW= 0.8062, DB= 0.2645, CH= 42656.7664\n",
      "Training epoch 477, recon_loss:0.782328, zinb_loss:0.645115, cluster_loss:0.156375\n",
      "Clustering   477: ASW= 0.8059, DB= 0.2669, CH= 42958.6983\n",
      "Training epoch 478, recon_loss:0.782134, zinb_loss:0.645056, cluster_loss:0.156864\n",
      "Clustering   478: ASW= 0.8065, DB= 0.2643, CH= 42694.5783\n",
      "Training epoch 479, recon_loss:0.781886, zinb_loss:0.645164, cluster_loss:0.156161\n",
      "Clustering   479: ASW= 0.8060, DB= 0.2670, CH= 43047.9461\n",
      "Training epoch 480, recon_loss:0.781694, zinb_loss:0.645106, cluster_loss:0.156595\n",
      "Clustering   480: ASW= 0.8067, DB= 0.2641, CH= 42773.1816\n",
      "Training epoch 481, recon_loss:0.781385, zinb_loss:0.645189, cluster_loss:0.155970\n",
      "Clustering   481: ASW= 0.8063, DB= 0.2663, CH= 43161.7936\n",
      "Training epoch 482, recon_loss:0.781314, zinb_loss:0.645156, cluster_loss:0.156376\n",
      "Clustering   482: ASW= 0.8069, DB= 0.2636, CH= 42849.7210\n",
      "Training epoch 483, recon_loss:0.780963, zinb_loss:0.645234, cluster_loss:0.155834\n",
      "Clustering   483: ASW= 0.8066, DB= 0.2658, CH= 43280.6560\n",
      "Training epoch 484, recon_loss:0.781117, zinb_loss:0.645257, cluster_loss:0.156239\n",
      "Clustering   484: ASW= 0.8071, DB= 0.2635, CH= 42892.1872\n",
      "Training epoch 485, recon_loss:0.780792, zinb_loss:0.645387, cluster_loss:0.155761\n",
      "Clustering   485: ASW= 0.8069, DB= 0.2653, CH= 43410.5354\n",
      "Training epoch 486, recon_loss:0.781168, zinb_loss:0.645461, cluster_loss:0.156198\n",
      "Clustering   486: ASW= 0.8071, DB= 0.2635, CH= 42908.3689\n",
      "Training epoch 487, recon_loss:0.780824, zinb_loss:0.645700, cluster_loss:0.155735\n",
      "Clustering   487: ASW= 0.8072, DB= 0.2649, CH= 43553.9411\n",
      "Training epoch 488, recon_loss:0.781349, zinb_loss:0.645780, cluster_loss:0.156197\n",
      "Clustering   488: ASW= 0.8070, DB= 0.2637, CH= 42891.1891\n",
      "Training epoch 489, recon_loss:0.781043, zinb_loss:0.646115, cluster_loss:0.155759\n",
      "Clustering   489: ASW= 0.8076, DB= 0.2643, CH= 43661.8192\n",
      "Training epoch 490, recon_loss:0.781533, zinb_loss:0.646003, cluster_loss:0.156187\n",
      "Clustering   490: ASW= 0.8071, DB= 0.2643, CH= 42930.4356\n",
      "Training epoch 491, recon_loss:0.781008, zinb_loss:0.646146, cluster_loss:0.155823\n",
      "Clustering   491: ASW= 0.8077, DB= 0.2639, CH= 43670.5296\n",
      "Training epoch 492, recon_loss:0.781421, zinb_loss:0.645903, cluster_loss:0.156165\n",
      "Clustering   492: ASW= 0.8071, DB= 0.2643, CH= 42999.8136\n",
      "Training epoch 493, recon_loss:0.780909, zinb_loss:0.646012, cluster_loss:0.155929\n",
      "Clustering   493: ASW= 0.8078, DB= 0.2637, CH= 43674.0917\n",
      "Training epoch 494, recon_loss:0.781362, zinb_loss:0.645672, cluster_loss:0.156194\n",
      "Clustering   494: ASW= 0.8072, DB= 0.2647, CH= 43064.4736\n",
      "Training epoch 495, recon_loss:0.780889, zinb_loss:0.645789, cluster_loss:0.156084\n",
      "Clustering   495: ASW= 0.8078, DB= 0.2635, CH= 43654.7972\n",
      "Training epoch 496, recon_loss:0.781393, zinb_loss:0.645433, cluster_loss:0.156236\n",
      "Clustering   496: ASW= 0.8074, DB= 0.2647, CH= 43135.9342\n",
      "Training epoch 497, recon_loss:0.780949, zinb_loss:0.645567, cluster_loss:0.156249\n",
      "Clustering   497: ASW= 0.8077, DB= 0.2633, CH= 43636.3722\n",
      "Training epoch 498, recon_loss:0.781443, zinb_loss:0.645212, cluster_loss:0.156266\n",
      "Clustering   498: ASW= 0.8077, DB= 0.2647, CH= 43218.7986\n",
      "Training epoch 499, recon_loss:0.780995, zinb_loss:0.645362, cluster_loss:0.156389\n",
      "Clustering   499: ASW= 0.8075, DB= 0.2627, CH= 43614.6982\n",
      "Training epoch 500, recon_loss:0.781478, zinb_loss:0.645033, cluster_loss:0.156240\n",
      "Clustering   500: ASW= 0.8079, DB= 0.2645, CH= 43295.4937\n",
      "Training epoch 501, recon_loss:0.781003, zinb_loss:0.645158, cluster_loss:0.156424\n",
      "Clustering   501: ASW= 0.8075, DB= 0.2620, CH= 43604.4781\n",
      "Training epoch 502, recon_loss:0.781499, zinb_loss:0.644866, cluster_loss:0.156133\n",
      "Clustering   502: ASW= 0.8080, DB= 0.2645, CH= 43366.2404\n",
      "Training epoch 503, recon_loss:0.780951, zinb_loss:0.644941, cluster_loss:0.156316\n",
      "Clustering   503: ASW= 0.8076, DB= 0.2619, CH= 43646.0121\n",
      "Training epoch 504, recon_loss:0.781585, zinb_loss:0.644680, cluster_loss:0.156014\n",
      "Clustering   504: ASW= 0.8082, DB= 0.2643, CH= 43432.1042\n",
      "Training epoch 505, recon_loss:0.781119, zinb_loss:0.644731, cluster_loss:0.156250\n",
      "Clustering   505: ASW= 0.8077, DB= 0.2618, CH= 43690.5528\n",
      "Training epoch 506, recon_loss:0.781912, zinb_loss:0.644498, cluster_loss:0.155966\n",
      "Clustering   506: ASW= 0.8083, DB= 0.2642, CH= 43462.9658\n",
      "Training epoch 507, recon_loss:0.781417, zinb_loss:0.644560, cluster_loss:0.156240\n",
      "Clustering   507: ASW= 0.8077, DB= 0.2620, CH= 43714.3784\n",
      "Training epoch 508, recon_loss:0.781772, zinb_loss:0.644329, cluster_loss:0.155905\n",
      "Clustering   508: ASW= 0.8084, DB= 0.2639, CH= 43493.2263\n",
      "Training epoch 509, recon_loss:0.781109, zinb_loss:0.644385, cluster_loss:0.156155\n",
      "Clustering   509: ASW= 0.8077, DB= 0.2619, CH= 43725.5124\n",
      "Training epoch 510, recon_loss:0.781332, zinb_loss:0.644181, cluster_loss:0.155804\n",
      "Clustering   510: ASW= 0.8087, DB= 0.2638, CH= 43564.7812\n",
      "Training epoch 511, recon_loss:0.780725, zinb_loss:0.644224, cluster_loss:0.156108\n",
      "Clustering   511: ASW= 0.8076, DB= 0.2620, CH= 43714.6881\n",
      "Training epoch 512, recon_loss:0.781039, zinb_loss:0.644077, cluster_loss:0.155776\n",
      "Clustering   512: ASW= 0.8088, DB= 0.2638, CH= 43618.2890\n",
      "Training epoch 513, recon_loss:0.780609, zinb_loss:0.644134, cluster_loss:0.156076\n",
      "Clustering   513: ASW= 0.8075, DB= 0.2618, CH= 43715.4106\n",
      "Training epoch 514, recon_loss:0.780862, zinb_loss:0.644052, cluster_loss:0.155765\n",
      "Clustering   514: ASW= 0.8090, DB= 0.2631, CH= 43722.3490\n",
      "Training epoch 515, recon_loss:0.780496, zinb_loss:0.644112, cluster_loss:0.156101\n",
      "Clustering   515: ASW= 0.8074, DB= 0.2621, CH= 43716.2648\n",
      "Training epoch 516, recon_loss:0.780753, zinb_loss:0.644062, cluster_loss:0.155805\n",
      "Clustering   516: ASW= 0.8090, DB= 0.2631, CH= 43748.7848\n",
      "Training epoch 517, recon_loss:0.780328, zinb_loss:0.644140, cluster_loss:0.156029\n",
      "Clustering   517: ASW= 0.8074, DB= 0.2615, CH= 43733.7683\n",
      "Training epoch 518, recon_loss:0.780543, zinb_loss:0.644125, cluster_loss:0.155714\n",
      "Clustering   518: ASW= 0.8092, DB= 0.2628, CH= 43848.0465\n",
      "Training epoch 519, recon_loss:0.780233, zinb_loss:0.644224, cluster_loss:0.155912\n",
      "Clustering   519: ASW= 0.8076, DB= 0.2615, CH= 43790.9650\n",
      "Training epoch 520, recon_loss:0.780553, zinb_loss:0.644206, cluster_loss:0.155683\n",
      "Clustering   520: ASW= 0.8092, DB= 0.2626, CH= 43908.3510\n",
      "Training epoch 521, recon_loss:0.780473, zinb_loss:0.644381, cluster_loss:0.155870\n",
      "Clustering   521: ASW= 0.8078, DB= 0.2617, CH= 43872.0880\n",
      "Training epoch 522, recon_loss:0.780958, zinb_loss:0.644316, cluster_loss:0.155730\n",
      "Clustering   522: ASW= 0.8091, DB= 0.2622, CH= 43907.3297\n",
      "Training epoch 523, recon_loss:0.780957, zinb_loss:0.644573, cluster_loss:0.155873\n",
      "Clustering   523: ASW= 0.8082, DB= 0.2612, CH= 43958.4974\n",
      "Training epoch 524, recon_loss:0.781487, zinb_loss:0.644431, cluster_loss:0.155802\n",
      "Clustering   524: ASW= 0.8089, DB= 0.2622, CH= 43921.3709\n",
      "Training epoch 525, recon_loss:0.781397, zinb_loss:0.644716, cluster_loss:0.155900\n",
      "Clustering   525: ASW= 0.8086, DB= 0.2605, CH= 44028.3166\n",
      "Training epoch 526, recon_loss:0.781711, zinb_loss:0.644469, cluster_loss:0.155880\n",
      "Clustering   526: ASW= 0.8086, DB= 0.2627, CH= 43887.2726\n",
      "Training epoch 527, recon_loss:0.781568, zinb_loss:0.644754, cluster_loss:0.155896\n",
      "Clustering   527: ASW= 0.8091, DB= 0.2604, CH= 44131.1273\n",
      "Training epoch 528, recon_loss:0.781582, zinb_loss:0.644455, cluster_loss:0.155926\n",
      "Clustering   528: ASW= 0.8082, DB= 0.2629, CH= 43867.1461\n",
      "Training epoch 529, recon_loss:0.781701, zinb_loss:0.644750, cluster_loss:0.155894\n",
      "Clustering   529: ASW= 0.8096, DB= 0.2601, CH= 44193.9407\n",
      "Training epoch 530, recon_loss:0.781560, zinb_loss:0.644439, cluster_loss:0.156010\n",
      "Clustering   530: ASW= 0.8079, DB= 0.2637, CH= 43846.5786\n",
      "Training epoch 531, recon_loss:0.782124, zinb_loss:0.644764, cluster_loss:0.155918\n",
      "Clustering   531: ASW= 0.8099, DB= 0.2598, CH= 44233.6913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 532, recon_loss:0.781639, zinb_loss:0.644465, cluster_loss:0.156129\n",
      "Clustering   532: ASW= 0.8074, DB= 0.2639, CH= 43786.8876\n",
      "Training epoch 533, recon_loss:0.782126, zinb_loss:0.644795, cluster_loss:0.155941\n",
      "Clustering   533: ASW= 0.8102, DB= 0.2606, CH= 44258.9145\n",
      "Training epoch 534, recon_loss:0.781268, zinb_loss:0.644469, cluster_loss:0.156163\n",
      "Clustering   534: ASW= 0.8072, DB= 0.2644, CH= 43786.2120\n",
      "Training epoch 535, recon_loss:0.781620, zinb_loss:0.644782, cluster_loss:0.155873\n",
      "Clustering   535: ASW= 0.8103, DB= 0.2605, CH= 44253.5166\n",
      "Training epoch 536, recon_loss:0.780965, zinb_loss:0.644442, cluster_loss:0.156071\n",
      "Clustering   536: ASW= 0.8072, DB= 0.2641, CH= 43828.0405\n",
      "Training epoch 537, recon_loss:0.781209, zinb_loss:0.644735, cluster_loss:0.155799\n",
      "Clustering   537: ASW= 0.8102, DB= 0.2605, CH= 44236.6775\n",
      "Training epoch 538, recon_loss:0.780710, zinb_loss:0.644404, cluster_loss:0.155909\n",
      "Clustering   538: ASW= 0.8074, DB= 0.2638, CH= 43926.4304\n",
      "Training epoch 539, recon_loss:0.780764, zinb_loss:0.644591, cluster_loss:0.155703\n",
      "Clustering   539: ASW= 0.8102, DB= 0.2610, CH= 44236.4976\n",
      "Training epoch 540, recon_loss:0.780322, zinb_loss:0.644306, cluster_loss:0.155708\n",
      "Clustering   540: ASW= 0.8078, DB= 0.2633, CH= 44051.4582\n",
      "Training epoch 541, recon_loss:0.780304, zinb_loss:0.644400, cluster_loss:0.155580\n",
      "Clustering   541: ASW= 0.8101, DB= 0.2608, CH= 44259.5912\n",
      "Training epoch 542, recon_loss:0.779960, zinb_loss:0.644208, cluster_loss:0.155538\n",
      "Clustering   542: ASW= 0.8081, DB= 0.2628, CH= 44157.6921\n",
      "Training epoch 543, recon_loss:0.779965, zinb_loss:0.644250, cluster_loss:0.155475\n",
      "Clustering   543: ASW= 0.8101, DB= 0.2604, CH= 44293.5226\n",
      "Training epoch 544, recon_loss:0.779716, zinb_loss:0.644152, cluster_loss:0.155395\n",
      "Clustering   544: ASW= 0.8085, DB= 0.2622, CH= 44268.6185\n",
      "Training epoch 545, recon_loss:0.779738, zinb_loss:0.644150, cluster_loss:0.155389\n",
      "Clustering   545: ASW= 0.8101, DB= 0.2603, CH= 44337.2439\n",
      "Training epoch 546, recon_loss:0.779611, zinb_loss:0.644139, cluster_loss:0.155293\n",
      "Clustering   546: ASW= 0.8088, DB= 0.2618, CH= 44360.3563\n",
      "Training epoch 547, recon_loss:0.779602, zinb_loss:0.644111, cluster_loss:0.155321\n",
      "Clustering   547: ASW= 0.8102, DB= 0.2604, CH= 44387.6416\n",
      "Training epoch 548, recon_loss:0.779616, zinb_loss:0.644168, cluster_loss:0.155215\n",
      "Clustering   548: ASW= 0.8091, DB= 0.2615, CH= 44444.3837\n",
      "Training epoch 549, recon_loss:0.779568, zinb_loss:0.644131, cluster_loss:0.155278\n",
      "Clustering   549: ASW= 0.8102, DB= 0.2603, CH= 44442.0928\n",
      "Training epoch 550, recon_loss:0.779770, zinb_loss:0.644243, cluster_loss:0.155183\n",
      "Clustering   550: ASW= 0.8094, DB= 0.2612, CH= 44510.0034\n",
      "Training epoch 551, recon_loss:0.779762, zinb_loss:0.644250, cluster_loss:0.155291\n",
      "Clustering   551: ASW= 0.8103, DB= 0.2599, CH= 44493.5821\n",
      "Training epoch 552, recon_loss:0.780256, zinb_loss:0.644387, cluster_loss:0.155213\n",
      "Clustering   552: ASW= 0.8096, DB= 0.2604, CH= 44546.4168\n",
      "Training epoch 553, recon_loss:0.780334, zinb_loss:0.644457, cluster_loss:0.155398\n",
      "Clustering   553: ASW= 0.8104, DB= 0.2601, CH= 44542.7050\n",
      "Training epoch 554, recon_loss:0.781065, zinb_loss:0.644579, cluster_loss:0.155348\n",
      "Clustering   554: ASW= 0.8097, DB= 0.2603, CH= 44572.9343\n",
      "Training epoch 555, recon_loss:0.780900, zinb_loss:0.644676, cluster_loss:0.155614\n",
      "Clustering   555: ASW= 0.8104, DB= 0.2606, CH= 44570.2040\n",
      "Training epoch 556, recon_loss:0.781378, zinb_loss:0.644686, cluster_loss:0.155497\n",
      "Clustering   556: ASW= 0.8098, DB= 0.2602, CH= 44588.7587\n",
      "Training epoch 557, recon_loss:0.781050, zinb_loss:0.644688, cluster_loss:0.155802\n",
      "Clustering   557: ASW= 0.8105, DB= 0.2606, CH= 44575.7605\n",
      "Training epoch 558, recon_loss:0.781319, zinb_loss:0.644619, cluster_loss:0.155588\n",
      "Clustering   558: ASW= 0.8097, DB= 0.2601, CH= 44599.4676\n",
      "Training epoch 559, recon_loss:0.780849, zinb_loss:0.644492, cluster_loss:0.155919\n",
      "Clustering   559: ASW= 0.8105, DB= 0.2605, CH= 44566.4428\n",
      "Training epoch 560, recon_loss:0.781081, zinb_loss:0.644398, cluster_loss:0.155652\n",
      "Clustering   560: ASW= 0.8096, DB= 0.2597, CH= 44555.8493\n",
      "Training epoch 561, recon_loss:0.780511, zinb_loss:0.644249, cluster_loss:0.155969\n",
      "Clustering   561: ASW= 0.8105, DB= 0.2603, CH= 44574.0596\n",
      "Training epoch 562, recon_loss:0.780675, zinb_loss:0.644134, cluster_loss:0.155664\n",
      "Clustering   562: ASW= 0.8095, DB= 0.2596, CH= 44530.2719\n",
      "Training epoch 563, recon_loss:0.780114, zinb_loss:0.644012, cluster_loss:0.155955\n",
      "Clustering   563: ASW= 0.8106, DB= 0.2596, CH= 44570.6584\n",
      "Training epoch 564, recon_loss:0.780267, zinb_loss:0.643910, cluster_loss:0.155615\n",
      "Clustering   564: ASW= 0.8096, DB= 0.2596, CH= 44578.2046\n",
      "Training epoch 565, recon_loss:0.779836, zinb_loss:0.643820, cluster_loss:0.155943\n",
      "Clustering   565: ASW= 0.8106, DB= 0.2600, CH= 44535.6738\n",
      "Training epoch 566, recon_loss:0.780137, zinb_loss:0.643812, cluster_loss:0.155584\n",
      "Clustering   566: ASW= 0.8099, DB= 0.2589, CH= 44644.5828\n",
      "Training epoch 567, recon_loss:0.779757, zinb_loss:0.643773, cluster_loss:0.155926\n",
      "Clustering   567: ASW= 0.8106, DB= 0.2600, CH= 44492.0582\n",
      "Training epoch 568, recon_loss:0.779920, zinb_loss:0.643830, cluster_loss:0.155548\n",
      "Clustering   568: ASW= 0.8101, DB= 0.2587, CH= 44747.4730\n",
      "Training epoch 569, recon_loss:0.779741, zinb_loss:0.643841, cluster_loss:0.155910\n",
      "Clustering   569: ASW= 0.8104, DB= 0.2604, CH= 44434.5430\n",
      "Training epoch 570, recon_loss:0.779863, zinb_loss:0.643956, cluster_loss:0.155537\n",
      "Clustering   570: ASW= 0.8105, DB= 0.2588, CH= 44861.7968\n",
      "Training epoch 571, recon_loss:0.779666, zinb_loss:0.643933, cluster_loss:0.155828\n",
      "Clustering   571: ASW= 0.8103, DB= 0.2604, CH= 44412.1202\n",
      "Training epoch 572, recon_loss:0.779611, zinb_loss:0.644006, cluster_loss:0.155444\n",
      "Clustering   572: ASW= 0.8107, DB= 0.2586, CH= 44960.0115\n",
      "Training epoch 573, recon_loss:0.779459, zinb_loss:0.643924, cluster_loss:0.155664\n",
      "Clustering   573: ASW= 0.8104, DB= 0.2605, CH= 44457.4602\n",
      "Training epoch 574, recon_loss:0.779400, zinb_loss:0.643964, cluster_loss:0.155354\n",
      "Clustering   574: ASW= 0.8110, DB= 0.2583, CH= 45074.2866\n",
      "Training epoch 575, recon_loss:0.779589, zinb_loss:0.643899, cluster_loss:0.155644\n",
      "Clustering   575: ASW= 0.8104, DB= 0.2602, CH= 44477.7655\n",
      "Training epoch 576, recon_loss:0.779616, zinb_loss:0.643937, cluster_loss:0.155405\n",
      "Clustering   576: ASW= 0.8111, DB= 0.2584, CH= 45163.3471\n",
      "Training epoch 577, recon_loss:0.779986, zinb_loss:0.643890, cluster_loss:0.155754\n",
      "Clustering   577: ASW= 0.8104, DB= 0.2599, CH= 44473.0167\n",
      "Training epoch 578, recon_loss:0.779858, zinb_loss:0.643922, cluster_loss:0.155492\n",
      "Clustering   578: ASW= 0.8112, DB= 0.2583, CH= 45208.1166\n",
      "Training epoch 579, recon_loss:0.780190, zinb_loss:0.643871, cluster_loss:0.155848\n",
      "Clustering   579: ASW= 0.8104, DB= 0.2598, CH= 44460.5708\n",
      "Training epoch 580, recon_loss:0.779888, zinb_loss:0.643903, cluster_loss:0.155502\n",
      "Clustering   580: ASW= 0.8113, DB= 0.2581, CH= 45227.9453\n",
      "Training epoch 581, recon_loss:0.780053, zinb_loss:0.643794, cluster_loss:0.155811\n",
      "Clustering   581: ASW= 0.8104, DB= 0.2594, CH= 44473.9500\n",
      "Training epoch 582, recon_loss:0.779590, zinb_loss:0.643846, cluster_loss:0.155399\n",
      "Clustering   582: ASW= 0.8114, DB= 0.2583, CH= 45254.8656\n",
      "Training epoch 583, recon_loss:0.779654, zinb_loss:0.643672, cluster_loss:0.155681\n",
      "Clustering   583: ASW= 0.8104, DB= 0.2593, CH= 44516.5553\n",
      "Training epoch 584, recon_loss:0.779198, zinb_loss:0.643770, cluster_loss:0.155252\n",
      "Clustering   584: ASW= 0.8116, DB= 0.2580, CH= 45286.3452\n",
      "Training epoch 585, recon_loss:0.779274, zinb_loss:0.643570, cluster_loss:0.155553\n",
      "Clustering   585: ASW= 0.8104, DB= 0.2592, CH= 44573.8160\n",
      "Training epoch 586, recon_loss:0.779026, zinb_loss:0.643756, cluster_loss:0.155140\n",
      "Clustering   586: ASW= 0.8118, DB= 0.2579, CH= 45337.5574\n",
      "Training epoch 587, recon_loss:0.779299, zinb_loss:0.643565, cluster_loss:0.155518\n",
      "Clustering   587: ASW= 0.8103, DB= 0.2592, CH= 44617.2740\n",
      "Training epoch 588, recon_loss:0.779417, zinb_loss:0.643859, cluster_loss:0.155128\n",
      "Clustering   588: ASW= 0.8122, DB= 0.2576, CH= 45384.2594\n",
      "Training epoch 589, recon_loss:0.779754, zinb_loss:0.643700, cluster_loss:0.155634\n",
      "Clustering   589: ASW= 0.8101, DB= 0.2595, CH= 44638.3082\n",
      "Training epoch 590, recon_loss:0.780169, zinb_loss:0.644096, cluster_loss:0.155266\n",
      "Clustering   590: ASW= 0.8126, DB= 0.2574, CH= 45404.3221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 591, recon_loss:0.780172, zinb_loss:0.643955, cluster_loss:0.155839\n",
      "Clustering   591: ASW= 0.8099, DB= 0.2585, CH= 44691.1306\n",
      "Training epoch 592, recon_loss:0.780529, zinb_loss:0.644264, cluster_loss:0.155436\n",
      "Clustering   592: ASW= 0.8127, DB= 0.2574, CH= 45350.0267\n",
      "Training epoch 593, recon_loss:0.780281, zinb_loss:0.644100, cluster_loss:0.155915\n",
      "Clustering   593: ASW= 0.8100, DB= 0.2579, CH= 44779.0745\n",
      "Training epoch 594, recon_loss:0.780412, zinb_loss:0.644187, cluster_loss:0.155406\n",
      "Clustering   594: ASW= 0.8124, DB= 0.2575, CH= 45275.9918\n",
      "Training epoch 595, recon_loss:0.780148, zinb_loss:0.644089, cluster_loss:0.155744\n",
      "Clustering   595: ASW= 0.8102, DB= 0.2576, CH= 44839.5361\n",
      "Training epoch 596, recon_loss:0.780226, zinb_loss:0.644087, cluster_loss:0.155358\n",
      "Clustering   596: ASW= 0.8122, DB= 0.2582, CH= 45228.0594\n",
      "Training epoch 597, recon_loss:0.780186, zinb_loss:0.644096, cluster_loss:0.155618\n",
      "Clustering   597: ASW= 0.8105, DB= 0.2573, CH= 44903.7156\n",
      "Training epoch 598, recon_loss:0.780114, zinb_loss:0.644071, cluster_loss:0.155342\n",
      "Clustering   598: ASW= 0.8120, DB= 0.2583, CH= 45227.3790\n",
      "Training epoch 599, recon_loss:0.780177, zinb_loss:0.644115, cluster_loss:0.155575\n",
      "Clustering   599: ASW= 0.8106, DB= 0.2574, CH= 44917.6847\n",
      "Training epoch 600, recon_loss:0.779871, zinb_loss:0.644093, cluster_loss:0.155316\n",
      "Clustering   600: ASW= 0.8120, DB= 0.2582, CH= 45278.4881\n",
      "Training epoch 601, recon_loss:0.780074, zinb_loss:0.644150, cluster_loss:0.155532\n",
      "Clustering   601: ASW= 0.8109, DB= 0.2570, CH= 44968.4103\n",
      "Training epoch 602, recon_loss:0.779644, zinb_loss:0.644147, cluster_loss:0.155275\n",
      "Clustering   602: ASW= 0.8119, DB= 0.2580, CH= 45325.4166\n",
      "Training epoch 603, recon_loss:0.779885, zinb_loss:0.644182, cluster_loss:0.155513\n",
      "Clustering   603: ASW= 0.8109, DB= 0.2569, CH= 44963.3408\n",
      "Training epoch 604, recon_loss:0.779539, zinb_loss:0.644206, cluster_loss:0.155252\n",
      "Clustering   604: ASW= 0.8120, DB= 0.2578, CH= 45389.3617\n",
      "Training epoch 605, recon_loss:0.779861, zinb_loss:0.644250, cluster_loss:0.155534\n",
      "Clustering   605: ASW= 0.8110, DB= 0.2570, CH= 44987.3507\n",
      "Training epoch 606, recon_loss:0.779782, zinb_loss:0.644377, cluster_loss:0.155290\n",
      "Clustering   606: ASW= 0.8120, DB= 0.2578, CH= 45449.1004\n",
      "Training epoch 607, recon_loss:0.780254, zinb_loss:0.644470, cluster_loss:0.155681\n",
      "Clustering   607: ASW= 0.8110, DB= 0.2569, CH= 44970.5108\n",
      "Training epoch 608, recon_loss:0.780501, zinb_loss:0.644767, cluster_loss:0.155456\n",
      "Clustering   608: ASW= 0.8121, DB= 0.2576, CH= 45496.2287\n",
      "Training epoch 609, recon_loss:0.781070, zinb_loss:0.644817, cluster_loss:0.155961\n",
      "Clustering   609: ASW= 0.8109, DB= 0.2572, CH= 44881.7386\n",
      "Training epoch 610, recon_loss:0.781091, zinb_loss:0.645018, cluster_loss:0.155601\n",
      "Clustering   610: ASW= 0.8121, DB= 0.2577, CH= 45534.6271\n",
      "Training epoch 611, recon_loss:0.781349, zinb_loss:0.644752, cluster_loss:0.156029\n",
      "Clustering   611: ASW= 0.8110, DB= 0.2574, CH= 44885.5987\n",
      "Training epoch 612, recon_loss:0.780716, zinb_loss:0.644738, cluster_loss:0.155497\n",
      "Clustering   612: ASW= 0.8120, DB= 0.2577, CH= 45539.3884\n",
      "Training epoch 613, recon_loss:0.780712, zinb_loss:0.644317, cluster_loss:0.155810\n",
      "Clustering   613: ASW= 0.8113, DB= 0.2571, CH= 44953.3667\n",
      "Training epoch 614, recon_loss:0.779934, zinb_loss:0.644289, cluster_loss:0.155257\n",
      "Clustering   614: ASW= 0.8120, DB= 0.2577, CH= 45555.5730\n",
      "Training epoch 615, recon_loss:0.779892, zinb_loss:0.643909, cluster_loss:0.155541\n",
      "Clustering   615: ASW= 0.8115, DB= 0.2570, CH= 45032.9228\n",
      "Training epoch 616, recon_loss:0.779302, zinb_loss:0.643939, cluster_loss:0.155064\n",
      "Clustering   616: ASW= 0.8122, DB= 0.2573, CH= 45593.9422\n",
      "Training epoch 617, recon_loss:0.779354, zinb_loss:0.643635, cluster_loss:0.155360\n",
      "Clustering   617: ASW= 0.8117, DB= 0.2571, CH= 45118.8111\n",
      "Training epoch 618, recon_loss:0.778943, zinb_loss:0.643714, cluster_loss:0.154949\n",
      "Clustering   618: ASW= 0.8123, DB= 0.2572, CH= 45658.5656\n",
      "Training epoch 619, recon_loss:0.779048, zinb_loss:0.643465, cluster_loss:0.155241\n",
      "Clustering   619: ASW= 0.8119, DB= 0.2569, CH= 45192.1463\n",
      "Training epoch 620, recon_loss:0.778766, zinb_loss:0.643574, cluster_loss:0.154897\n",
      "Clustering   620: ASW= 0.8124, DB= 0.2574, CH= 45727.7503\n",
      "Training epoch 621, recon_loss:0.778904, zinb_loss:0.643371, cluster_loss:0.155152\n",
      "Clustering   621: ASW= 0.8121, DB= 0.2567, CH= 45275.0496\n",
      "Training epoch 622, recon_loss:0.778715, zinb_loss:0.643481, cluster_loss:0.154887\n",
      "Clustering   622: ASW= 0.8125, DB= 0.2575, CH= 45781.0486\n",
      "Training epoch 623, recon_loss:0.778906, zinb_loss:0.643325, cluster_loss:0.155087\n",
      "Clustering   623: ASW= 0.8123, DB= 0.2564, CH= 45345.1516\n",
      "Training epoch 624, recon_loss:0.778799, zinb_loss:0.643415, cluster_loss:0.154909\n",
      "Clustering   624: ASW= 0.8125, DB= 0.2574, CH= 45828.8004\n",
      "Training epoch 625, recon_loss:0.778948, zinb_loss:0.643295, cluster_loss:0.155038\n",
      "Clustering   625: ASW= 0.8125, DB= 0.2566, CH= 45414.2683\n",
      "Training epoch 626, recon_loss:0.778839, zinb_loss:0.643373, cluster_loss:0.154931\n",
      "Clustering   626: ASW= 0.8126, DB= 0.2577, CH= 45889.1364\n",
      "Training epoch 627, recon_loss:0.778906, zinb_loss:0.643268, cluster_loss:0.155004\n",
      "Clustering   627: ASW= 0.8127, DB= 0.2567, CH= 45448.2311\n",
      "Training epoch 628, recon_loss:0.778695, zinb_loss:0.643359, cluster_loss:0.154924\n",
      "Clustering   628: ASW= 0.8127, DB= 0.2576, CH= 45969.5222\n",
      "Training epoch 629, recon_loss:0.778753, zinb_loss:0.643264, cluster_loss:0.154970\n",
      "Clustering   629: ASW= 0.8128, DB= 0.2570, CH= 45486.5367\n",
      "Training epoch 630, recon_loss:0.778469, zinb_loss:0.643391, cluster_loss:0.154932\n",
      "Clustering   630: ASW= 0.8128, DB= 0.2577, CH= 46042.2409\n",
      "Training epoch 631, recon_loss:0.778628, zinb_loss:0.643313, cluster_loss:0.154984\n",
      "Clustering   631: ASW= 0.8129, DB= 0.2571, CH= 45490.5853\n",
      "Training epoch 632, recon_loss:0.778464, zinb_loss:0.643528, cluster_loss:0.155019\n",
      "Clustering   632: ASW= 0.8128, DB= 0.2575, CH= 46073.0330\n",
      "Training epoch 633, recon_loss:0.778854, zinb_loss:0.643483, cluster_loss:0.155110\n",
      "Clustering   633: ASW= 0.8130, DB= 0.2569, CH= 45466.2708\n",
      "Training epoch 634, recon_loss:0.778808, zinb_loss:0.643761, cluster_loss:0.155187\n",
      "Clustering   634: ASW= 0.8128, DB= 0.2576, CH= 46074.6084\n",
      "Training epoch 635, recon_loss:0.779661, zinb_loss:0.643759, cluster_loss:0.155224\n",
      "Clustering   635: ASW= 0.8129, DB= 0.2573, CH= 45420.8440\n",
      "Training epoch 636, recon_loss:0.779147, zinb_loss:0.643916, cluster_loss:0.155295\n",
      "Clustering   636: ASW= 0.8126, DB= 0.2576, CH= 46045.3832\n",
      "Training epoch 637, recon_loss:0.780143, zinb_loss:0.643879, cluster_loss:0.155275\n",
      "Clustering   637: ASW= 0.8132, DB= 0.2572, CH= 45468.4731\n",
      "Training epoch 638, recon_loss:0.779234, zinb_loss:0.643869, cluster_loss:0.155395\n",
      "Clustering   638: ASW= 0.8124, DB= 0.2579, CH= 45930.5185\n",
      "Training epoch 639, recon_loss:0.780603, zinb_loss:0.643804, cluster_loss:0.155244\n",
      "Clustering   639: ASW= 0.8135, DB= 0.2562, CH= 45527.8077\n",
      "Training epoch 640, recon_loss:0.779197, zinb_loss:0.643699, cluster_loss:0.155385\n",
      "Clustering   640: ASW= 0.8123, DB= 0.2572, CH= 45925.8813\n",
      "Training epoch 641, recon_loss:0.780247, zinb_loss:0.643587, cluster_loss:0.155176\n",
      "Clustering   641: ASW= 0.8140, DB= 0.2559, CH= 45687.7344\n",
      "Training epoch 642, recon_loss:0.778833, zinb_loss:0.643461, cluster_loss:0.155491\n",
      "Clustering   642: ASW= 0.8119, DB= 0.2569, CH= 45827.4094\n",
      "Training epoch 643, recon_loss:0.779603, zinb_loss:0.643329, cluster_loss:0.155149\n",
      "Clustering   643: ASW= 0.8140, DB= 0.2562, CH= 45735.2778\n",
      "Training epoch 644, recon_loss:0.778496, zinb_loss:0.643321, cluster_loss:0.155468\n",
      "Clustering   644: ASW= 0.8119, DB= 0.2560, CH= 45823.4928\n",
      "Training epoch 645, recon_loss:0.779218, zinb_loss:0.643208, cluster_loss:0.155015\n",
      "Clustering   645: ASW= 0.8140, DB= 0.2558, CH= 45819.4702\n",
      "Training epoch 646, recon_loss:0.778248, zinb_loss:0.643196, cluster_loss:0.155308\n",
      "Clustering   646: ASW= 0.8120, DB= 0.2559, CH= 45863.8867\n",
      "Training epoch 647, recon_loss:0.778884, zinb_loss:0.643091, cluster_loss:0.154901\n",
      "Clustering   647: ASW= 0.8140, DB= 0.2558, CH= 45884.1937\n",
      "Training epoch 648, recon_loss:0.778124, zinb_loss:0.643097, cluster_loss:0.155182\n",
      "Clustering   648: ASW= 0.8122, DB= 0.2557, CH= 45897.9540\n",
      "Training epoch 649, recon_loss:0.778705, zinb_loss:0.643025, cluster_loss:0.154817\n",
      "Clustering   649: ASW= 0.8141, DB= 0.2557, CH= 45963.7538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 650, recon_loss:0.778107, zinb_loss:0.643033, cluster_loss:0.155092\n",
      "Clustering   650: ASW= 0.8122, DB= 0.2554, CH= 45924.6186\n",
      "Training epoch 651, recon_loss:0.778690, zinb_loss:0.643006, cluster_loss:0.154796\n",
      "Clustering   651: ASW= 0.8142, DB= 0.2552, CH= 46018.5789\n",
      "Training epoch 652, recon_loss:0.778243, zinb_loss:0.643031, cluster_loss:0.155043\n",
      "Clustering   652: ASW= 0.8124, DB= 0.2553, CH= 45964.8776\n",
      "Training epoch 653, recon_loss:0.778872, zinb_loss:0.643057, cluster_loss:0.154833\n",
      "Clustering   653: ASW= 0.8142, DB= 0.2555, CH= 46079.7586\n",
      "Training epoch 654, recon_loss:0.778560, zinb_loss:0.643091, cluster_loss:0.155050\n",
      "Clustering   654: ASW= 0.8125, DB= 0.2552, CH= 45989.8558\n",
      "Training epoch 655, recon_loss:0.779242, zinb_loss:0.643167, cluster_loss:0.155024\n",
      "Clustering   655: ASW= 0.8142, DB= 0.2551, CH= 46103.8140\n",
      "Training epoch 656, recon_loss:0.779103, zinb_loss:0.643213, cluster_loss:0.155182\n",
      "Clustering   656: ASW= 0.8125, DB= 0.2551, CH= 45961.2954\n",
      "Training epoch 657, recon_loss:0.779602, zinb_loss:0.643325, cluster_loss:0.155302\n",
      "Clustering   657: ASW= 0.8141, DB= 0.2560, CH= 46109.7265\n",
      "Training epoch 658, recon_loss:0.779604, zinb_loss:0.643361, cluster_loss:0.155346\n",
      "Clustering   658: ASW= 0.8125, DB= 0.2549, CH= 45954.9612\n",
      "Training epoch 659, recon_loss:0.779723, zinb_loss:0.643470, cluster_loss:0.155545\n",
      "Clustering   659: ASW= 0.8141, DB= 0.2561, CH= 46158.1466\n",
      "Training epoch 660, recon_loss:0.779932, zinb_loss:0.643498, cluster_loss:0.155542\n",
      "Clustering   660: ASW= 0.8124, DB= 0.2548, CH= 45891.4763\n",
      "Training epoch 661, recon_loss:0.779578, zinb_loss:0.643465, cluster_loss:0.155658\n",
      "Clustering   661: ASW= 0.8139, DB= 0.2559, CH= 46129.4767\n",
      "Training epoch 662, recon_loss:0.779792, zinb_loss:0.643494, cluster_loss:0.155491\n",
      "Clustering   662: ASW= 0.8127, DB= 0.2544, CH= 45972.8724\n",
      "Training epoch 663, recon_loss:0.779059, zinb_loss:0.643351, cluster_loss:0.155534\n",
      "Clustering   663: ASW= 0.8140, DB= 0.2559, CH= 46168.9750\n",
      "Training epoch 664, recon_loss:0.779199, zinb_loss:0.643385, cluster_loss:0.155254\n",
      "Clustering   664: ASW= 0.8130, DB= 0.2544, CH= 46095.6254\n",
      "Training epoch 665, recon_loss:0.778309, zinb_loss:0.643074, cluster_loss:0.155305\n",
      "Clustering   665: ASW= 0.8139, DB= 0.2563, CH= 46148.6770\n",
      "Training epoch 666, recon_loss:0.778481, zinb_loss:0.643155, cluster_loss:0.154932\n",
      "Clustering   666: ASW= 0.8134, DB= 0.2545, CH= 46252.4600\n",
      "Training epoch 667, recon_loss:0.777918, zinb_loss:0.642933, cluster_loss:0.155062\n",
      "Clustering   667: ASW= 0.8141, DB= 0.2559, CH= 46260.1339\n",
      "Training epoch 668, recon_loss:0.778191, zinb_loss:0.643055, cluster_loss:0.154723\n",
      "Clustering   668: ASW= 0.8136, DB= 0.2541, CH= 46334.1467\n",
      "Training epoch 669, recon_loss:0.777863, zinb_loss:0.642852, cluster_loss:0.154928\n",
      "Clustering   669: ASW= 0.8141, DB= 0.2559, CH= 46338.1977\n",
      "Training epoch 670, recon_loss:0.778222, zinb_loss:0.643019, cluster_loss:0.154581\n",
      "Clustering   670: ASW= 0.8139, DB= 0.2537, CH= 46419.8440\n",
      "Training epoch 671, recon_loss:0.778079, zinb_loss:0.642846, cluster_loss:0.154857\n",
      "Clustering   671: ASW= 0.8142, DB= 0.2560, CH= 46407.3440\n",
      "Training epoch 672, recon_loss:0.778468, zinb_loss:0.643025, cluster_loss:0.154485\n",
      "Clustering   672: ASW= 0.8141, DB= 0.2542, CH= 46490.7059\n",
      "Training epoch 673, recon_loss:0.778338, zinb_loss:0.642851, cluster_loss:0.154799\n",
      "Clustering   673: ASW= 0.8142, DB= 0.2555, CH= 46440.3844\n",
      "Training epoch 674, recon_loss:0.778714, zinb_loss:0.643021, cluster_loss:0.154398\n",
      "Clustering   674: ASW= 0.8142, DB= 0.2541, CH= 46530.2675\n",
      "Training epoch 675, recon_loss:0.778520, zinb_loss:0.642832, cluster_loss:0.154742\n",
      "Clustering   675: ASW= 0.8141, DB= 0.2556, CH= 46463.8192\n",
      "Training epoch 676, recon_loss:0.778898, zinb_loss:0.642989, cluster_loss:0.154327\n",
      "Clustering   676: ASW= 0.8142, DB= 0.2541, CH= 46551.0367\n",
      "Training epoch 677, recon_loss:0.778592, zinb_loss:0.642767, cluster_loss:0.154707\n",
      "Clustering   677: ASW= 0.8140, DB= 0.2556, CH= 46449.3458\n",
      "Training epoch 678, recon_loss:0.778952, zinb_loss:0.642918, cluster_loss:0.154287\n",
      "Clustering   678: ASW= 0.8142, DB= 0.2539, CH= 46559.4259\n",
      "Training epoch 679, recon_loss:0.778521, zinb_loss:0.642660, cluster_loss:0.154701\n",
      "Clustering   679: ASW= 0.8138, DB= 0.2559, CH= 46413.8801\n",
      "Training epoch 680, recon_loss:0.778830, zinb_loss:0.642816, cluster_loss:0.154281\n",
      "Clustering   680: ASW= 0.8142, DB= 0.2539, CH= 46582.0812\n",
      "Training epoch 681, recon_loss:0.778301, zinb_loss:0.642551, cluster_loss:0.154719\n",
      "Clustering   681: ASW= 0.8135, DB= 0.2561, CH= 46363.5061\n",
      "Training epoch 682, recon_loss:0.778553, zinb_loss:0.642714, cluster_loss:0.154291\n",
      "Clustering   682: ASW= 0.8144, DB= 0.2539, CH= 46621.6832\n",
      "Training epoch 683, recon_loss:0.778053, zinb_loss:0.642472, cluster_loss:0.154773\n",
      "Clustering   683: ASW= 0.8132, DB= 0.2560, CH= 46290.9516\n",
      "Training epoch 684, recon_loss:0.778334, zinb_loss:0.642665, cluster_loss:0.154350\n",
      "Clustering   684: ASW= 0.8146, DB= 0.2541, CH= 46680.9755\n",
      "Training epoch 685, recon_loss:0.778035, zinb_loss:0.642466, cluster_loss:0.154950\n",
      "Clustering   685: ASW= 0.8128, DB= 0.2559, CH= 46159.7453\n",
      "Training epoch 686, recon_loss:0.778376, zinb_loss:0.642728, cluster_loss:0.154539\n",
      "Clustering   686: ASW= 0.8147, DB= 0.2542, CH= 46705.1483\n",
      "Training epoch 687, recon_loss:0.778331, zinb_loss:0.642698, cluster_loss:0.155236\n",
      "Clustering   687: ASW= 0.8123, DB= 0.2554, CH= 46010.4483\n",
      "Training epoch 688, recon_loss:0.778654, zinb_loss:0.643076, cluster_loss:0.154769\n",
      "Clustering   688: ASW= 0.8146, DB= 0.2537, CH= 46667.7891\n",
      "Training epoch 689, recon_loss:0.778960, zinb_loss:0.643261, cluster_loss:0.155450\n",
      "Clustering   689: ASW= 0.8121, DB= 0.2550, CH= 45859.9627\n",
      "Training epoch 690, recon_loss:0.779204, zinb_loss:0.643671, cluster_loss:0.154960\n",
      "Clustering   690: ASW= 0.8143, DB= 0.2548, CH= 46637.5341\n",
      "Training epoch 691, recon_loss:0.779466, zinb_loss:0.643745, cluster_loss:0.155420\n",
      "Clustering   691: ASW= 0.8128, DB= 0.2543, CH= 45939.9380\n",
      "Training epoch 692, recon_loss:0.779247, zinb_loss:0.643721, cluster_loss:0.154902\n",
      "Clustering   692: ASW= 0.8145, DB= 0.2547, CH= 46684.2038\n",
      "Training epoch 693, recon_loss:0.779477, zinb_loss:0.643743, cluster_loss:0.155122\n",
      "Clustering   693: ASW= 0.8138, DB= 0.2528, CH= 46308.1874\n",
      "Training epoch 694, recon_loss:0.778943, zinb_loss:0.643501, cluster_loss:0.154745\n",
      "Clustering   694: ASW= 0.8146, DB= 0.2553, CH= 46758.6592\n",
      "Training epoch 695, recon_loss:0.779277, zinb_loss:0.643511, cluster_loss:0.155040\n",
      "Clustering   695: ASW= 0.8141, DB= 0.2531, CH= 46291.7327\n",
      "Training epoch 696, recon_loss:0.779168, zinb_loss:0.643279, cluster_loss:0.154750\n",
      "Clustering   696: ASW= 0.8146, DB= 0.2552, CH= 46754.3705\n",
      "Training epoch 697, recon_loss:0.779291, zinb_loss:0.643358, cluster_loss:0.154886\n",
      "Clustering   697: ASW= 0.8149, DB= 0.2524, CH= 46554.2697\n",
      "Training epoch 698, recon_loss:0.778885, zinb_loss:0.643145, cluster_loss:0.154585\n",
      "Clustering   698: ASW= 0.8149, DB= 0.2550, CH= 46884.2579\n",
      "Training epoch 699, recon_loss:0.779008, zinb_loss:0.643128, cluster_loss:0.154762\n",
      "Clustering   699: ASW= 0.8151, DB= 0.2521, CH= 46651.5204\n",
      "Training epoch 700, recon_loss:0.778591, zinb_loss:0.642866, cluster_loss:0.154573\n",
      "Clustering   700: ASW= 0.8147, DB= 0.2549, CH= 46854.4308\n",
      "Training epoch 701, recon_loss:0.778886, zinb_loss:0.642988, cluster_loss:0.154722\n",
      "Clustering   701: ASW= 0.8155, DB= 0.2524, CH= 46714.4321\n",
      "Training epoch 702, recon_loss:0.778507, zinb_loss:0.642817, cluster_loss:0.154476\n",
      "Clustering   702: ASW= 0.8148, DB= 0.2543, CH= 46985.8439\n",
      "Training epoch 703, recon_loss:0.778516, zinb_loss:0.642859, cluster_loss:0.154665\n",
      "Clustering   703: ASW= 0.8157, DB= 0.2525, CH= 46747.6141\n",
      "Training epoch 704, recon_loss:0.778098, zinb_loss:0.642716, cluster_loss:0.154412\n",
      "Clustering   704: ASW= 0.8148, DB= 0.2542, CH= 47041.9454\n",
      "Training epoch 705, recon_loss:0.778243, zinb_loss:0.642762, cluster_loss:0.154617\n",
      "Clustering   705: ASW= 0.8158, DB= 0.2527, CH= 46798.1154\n",
      "Training epoch 706, recon_loss:0.777888, zinb_loss:0.642665, cluster_loss:0.154378\n",
      "Clustering   706: ASW= 0.8148, DB= 0.2539, CH= 47120.0528\n",
      "Training epoch 707, recon_loss:0.778087, zinb_loss:0.642735, cluster_loss:0.154629\n",
      "Clustering   707: ASW= 0.8159, DB= 0.2528, CH= 46818.5047\n",
      "Training epoch 708, recon_loss:0.777859, zinb_loss:0.642722, cluster_loss:0.154401\n",
      "Clustering   708: ASW= 0.8149, DB= 0.2539, CH= 47221.0879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 709, recon_loss:0.778074, zinb_loss:0.642790, cluster_loss:0.154725\n",
      "Clustering   709: ASW= 0.8158, DB= 0.2530, CH= 46777.9486\n",
      "Training epoch 710, recon_loss:0.777951, zinb_loss:0.642915, cluster_loss:0.154469\n",
      "Clustering   710: ASW= 0.8150, DB= 0.2536, CH= 47345.5181\n",
      "Training epoch 711, recon_loss:0.778174, zinb_loss:0.643005, cluster_loss:0.154892\n",
      "Clustering   711: ASW= 0.8157, DB= 0.2529, CH= 46715.3361\n",
      "Training epoch 712, recon_loss:0.778256, zinb_loss:0.643272, cluster_loss:0.154632\n",
      "Clustering   712: ASW= 0.8152, DB= 0.2536, CH= 47443.6459\n",
      "Training epoch 713, recon_loss:0.778531, zinb_loss:0.643366, cluster_loss:0.155186\n",
      "Clustering   713: ASW= 0.8154, DB= 0.2533, CH= 46578.3952\n",
      "Training epoch 714, recon_loss:0.778820, zinb_loss:0.643772, cluster_loss:0.154919\n",
      "Clustering   714: ASW= 0.8152, DB= 0.2538, CH= 47489.9950\n",
      "Training epoch 715, recon_loss:0.779109, zinb_loss:0.643819, cluster_loss:0.155592\n",
      "Clustering   715: ASW= 0.8152, DB= 0.2524, CH= 46404.5132\n",
      "Training epoch 716, recon_loss:0.779317, zinb_loss:0.644271, cluster_loss:0.155242\n",
      "Clustering   716: ASW= 0.8152, DB= 0.2538, CH= 47404.3629\n",
      "Training epoch 717, recon_loss:0.779436, zinb_loss:0.644071, cluster_loss:0.155811\n",
      "Clustering   717: ASW= 0.8148, DB= 0.2524, CH= 46274.4754\n",
      "Training epoch 718, recon_loss:0.778766, zinb_loss:0.644258, cluster_loss:0.155210\n",
      "Clustering   718: ASW= 0.8153, DB= 0.2540, CH= 47273.7566\n",
      "Training epoch 719, recon_loss:0.778815, zinb_loss:0.643840, cluster_loss:0.155524\n",
      "Clustering   719: ASW= 0.8145, DB= 0.2524, CH= 46314.4062\n",
      "Training epoch 720, recon_loss:0.778028, zinb_loss:0.643948, cluster_loss:0.154941\n",
      "Clustering   720: ASW= 0.8152, DB= 0.2540, CH= 47183.4404\n",
      "Training epoch 721, recon_loss:0.778078, zinb_loss:0.643497, cluster_loss:0.155191\n",
      "Clustering   721: ASW= 0.8145, DB= 0.2532, CH= 46429.2643\n",
      "Training epoch 722, recon_loss:0.777466, zinb_loss:0.643652, cluster_loss:0.154664\n",
      "Clustering   722: ASW= 0.8154, DB= 0.2537, CH= 47163.5072\n",
      "Training epoch 723, recon_loss:0.777766, zinb_loss:0.643245, cluster_loss:0.154889\n",
      "Clustering   723: ASW= 0.8145, DB= 0.2527, CH= 46547.4448\n",
      "Training epoch 724, recon_loss:0.777325, zinb_loss:0.643434, cluster_loss:0.154423\n",
      "Clustering   724: ASW= 0.8157, DB= 0.2538, CH= 47274.7167\n",
      "Training epoch 725, recon_loss:0.777768, zinb_loss:0.643120, cluster_loss:0.154753\n",
      "Clustering   725: ASW= 0.8146, DB= 0.2530, CH= 46646.5248\n",
      "Training epoch 726, recon_loss:0.777500, zinb_loss:0.643337, cluster_loss:0.154298\n",
      "Clustering   726: ASW= 0.8158, DB= 0.2537, CH= 47291.3223\n",
      "Training epoch 727, recon_loss:0.778230, zinb_loss:0.643104, cluster_loss:0.154717\n",
      "Clustering   727: ASW= 0.8145, DB= 0.2533, CH= 46688.4826\n",
      "Training epoch 728, recon_loss:0.777669, zinb_loss:0.643242, cluster_loss:0.154247\n",
      "Clustering   728: ASW= 0.8158, DB= 0.2538, CH= 47312.7544\n",
      "Training epoch 729, recon_loss:0.778311, zinb_loss:0.643057, cluster_loss:0.154645\n",
      "Clustering   729: ASW= 0.8146, DB= 0.2534, CH= 46782.4242\n",
      "Training epoch 730, recon_loss:0.777685, zinb_loss:0.643163, cluster_loss:0.154194\n",
      "Clustering   730: ASW= 0.8158, DB= 0.2537, CH= 47302.4639\n",
      "Training epoch 731, recon_loss:0.778329, zinb_loss:0.643000, cluster_loss:0.154654\n",
      "Clustering   731: ASW= 0.8145, DB= 0.2534, CH= 46777.8121\n",
      "Training epoch 732, recon_loss:0.777722, zinb_loss:0.643079, cluster_loss:0.154186\n",
      "Clustering   732: ASW= 0.8157, DB= 0.2539, CH= 47282.2563\n",
      "Training epoch 733, recon_loss:0.778195, zinb_loss:0.642983, cluster_loss:0.154594\n",
      "Clustering   733: ASW= 0.8148, DB= 0.2529, CH= 46894.3607\n",
      "Training epoch 734, recon_loss:0.777556, zinb_loss:0.643051, cluster_loss:0.154137\n",
      "Clustering   734: ASW= 0.8158, DB= 0.2528, CH= 47289.6340\n",
      "Training epoch 735, recon_loss:0.778127, zinb_loss:0.642976, cluster_loss:0.154633\n",
      "Clustering   735: ASW= 0.8147, DB= 0.2528, CH= 46873.6539\n",
      "Training epoch 736, recon_loss:0.777626, zinb_loss:0.643013, cluster_loss:0.154179\n",
      "Clustering   736: ASW= 0.8158, DB= 0.2534, CH= 47272.0766\n",
      "Training epoch 737, recon_loss:0.778224, zinb_loss:0.643000, cluster_loss:0.154608\n",
      "Clustering   737: ASW= 0.8149, DB= 0.2525, CH= 46986.4518\n",
      "Training epoch 738, recon_loss:0.777667, zinb_loss:0.642999, cluster_loss:0.154204\n",
      "Clustering   738: ASW= 0.8160, DB= 0.2525, CH= 47320.4043\n",
      "Training epoch 739, recon_loss:0.778128, zinb_loss:0.642991, cluster_loss:0.154619\n",
      "Clustering   739: ASW= 0.8150, DB= 0.2525, CH= 47063.7451\n",
      "Training epoch 740, recon_loss:0.777665, zinb_loss:0.642931, cluster_loss:0.154264\n",
      "Clustering   740: ASW= 0.8162, DB= 0.2524, CH= 47371.2833\n",
      "Training epoch 741, recon_loss:0.778091, zinb_loss:0.642901, cluster_loss:0.154643\n",
      "Clustering   741: ASW= 0.8152, DB= 0.2522, CH= 47139.6468\n",
      "Training epoch 742, recon_loss:0.777611, zinb_loss:0.642796, cluster_loss:0.154310\n",
      "Clustering   742: ASW= 0.8163, DB= 0.2526, CH= 47401.2335\n",
      "Training epoch 743, recon_loss:0.778017, zinb_loss:0.642759, cluster_loss:0.154595\n",
      "Clustering   743: ASW= 0.8156, DB= 0.2522, CH= 47266.1735\n",
      "Training epoch 744, recon_loss:0.777525, zinb_loss:0.642649, cluster_loss:0.154324\n",
      "Clustering   744: ASW= 0.8163, DB= 0.2529, CH= 47411.0784\n",
      "Training epoch 745, recon_loss:0.778003, zinb_loss:0.642626, cluster_loss:0.154541\n",
      "Clustering   745: ASW= 0.8160, DB= 0.2523, CH= 47403.1263\n",
      "Training epoch 746, recon_loss:0.777745, zinb_loss:0.642576, cluster_loss:0.154450\n",
      "Clustering   746: ASW= 0.8160, DB= 0.2527, CH= 47327.2942\n",
      "Training epoch 747, recon_loss:0.778618, zinb_loss:0.642709, cluster_loss:0.154583\n",
      "Clustering   747: ASW= 0.8165, DB= 0.2523, CH= 47529.0536\n",
      "Training epoch 748, recon_loss:0.778444, zinb_loss:0.642643, cluster_loss:0.154656\n",
      "Clustering   748: ASW= 0.8158, DB= 0.2532, CH= 47145.3374\n",
      "Training epoch 749, recon_loss:0.778834, zinb_loss:0.642681, cluster_loss:0.154616\n",
      "Clustering   749: ASW= 0.8166, DB= 0.2518, CH= 47480.6571\n",
      "Training epoch 750, recon_loss:0.778071, zinb_loss:0.642586, cluster_loss:0.154525\n",
      "Clustering   750: ASW= 0.8162, DB= 0.2519, CH= 47376.8111\n",
      "Training epoch 751, recon_loss:0.778302, zinb_loss:0.642574, cluster_loss:0.154495\n",
      "Clustering   751: ASW= 0.8173, DB= 0.2515, CH= 47628.3939\n",
      "Training epoch 752, recon_loss:0.777697, zinb_loss:0.642467, cluster_loss:0.154564\n",
      "Clustering   752: ASW= 0.8159, DB= 0.2515, CH= 47291.5846\n",
      "Training epoch 753, recon_loss:0.777785, zinb_loss:0.642463, cluster_loss:0.154371\n",
      "Clustering   753: ASW= 0.8174, DB= 0.2506, CH= 47658.2725\n",
      "Training epoch 754, recon_loss:0.777374, zinb_loss:0.642398, cluster_loss:0.154467\n",
      "Clustering   754: ASW= 0.8161, DB= 0.2514, CH= 47404.3883\n",
      "Training epoch 755, recon_loss:0.777439, zinb_loss:0.642371, cluster_loss:0.154306\n",
      "Clustering   755: ASW= 0.8175, DB= 0.2505, CH= 47725.8554\n",
      "Training epoch 756, recon_loss:0.777338, zinb_loss:0.642399, cluster_loss:0.154446\n",
      "Clustering   756: ASW= 0.8160, DB= 0.2512, CH= 47434.3642\n",
      "Training epoch 757, recon_loss:0.777346, zinb_loss:0.642335, cluster_loss:0.154306\n",
      "Clustering   757: ASW= 0.8176, DB= 0.2506, CH= 47756.8651\n",
      "Training epoch 758, recon_loss:0.777390, zinb_loss:0.642441, cluster_loss:0.154453\n",
      "Clustering   758: ASW= 0.8160, DB= 0.2512, CH= 47490.4779\n",
      "Training epoch 759, recon_loss:0.777287, zinb_loss:0.642327, cluster_loss:0.154353\n",
      "Clustering   759: ASW= 0.8176, DB= 0.2505, CH= 47780.7299\n",
      "Training epoch 760, recon_loss:0.777355, zinb_loss:0.642479, cluster_loss:0.154476\n",
      "Clustering   760: ASW= 0.8159, DB= 0.2511, CH= 47512.1207\n",
      "Training epoch 761, recon_loss:0.777063, zinb_loss:0.642295, cluster_loss:0.154399\n",
      "Clustering   761: ASW= 0.8177, DB= 0.2504, CH= 47804.9705\n",
      "Training epoch 762, recon_loss:0.777165, zinb_loss:0.642490, cluster_loss:0.154481\n",
      "Clustering   762: ASW= 0.8159, DB= 0.2509, CH= 47541.0533\n",
      "Training epoch 763, recon_loss:0.776744, zinb_loss:0.642252, cluster_loss:0.154441\n",
      "Clustering   763: ASW= 0.8177, DB= 0.2505, CH= 47860.1241\n",
      "Training epoch 764, recon_loss:0.776937, zinb_loss:0.642473, cluster_loss:0.154499\n",
      "Clustering   764: ASW= 0.8159, DB= 0.2506, CH= 47546.3944\n",
      "Training epoch 765, recon_loss:0.776554, zinb_loss:0.642238, cluster_loss:0.154515\n",
      "Clustering   765: ASW= 0.8177, DB= 0.2508, CH= 47919.2878\n",
      "Training epoch 766, recon_loss:0.776914, zinb_loss:0.642466, cluster_loss:0.154557\n",
      "Clustering   766: ASW= 0.8159, DB= 0.2506, CH= 47535.5444\n",
      "Training epoch 767, recon_loss:0.776619, zinb_loss:0.642280, cluster_loss:0.154659\n",
      "Clustering   767: ASW= 0.8177, DB= 0.2513, CH= 47962.3543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 768, recon_loss:0.777070, zinb_loss:0.642494, cluster_loss:0.154678\n",
      "Clustering   768: ASW= 0.8160, DB= 0.2504, CH= 47496.6849\n",
      "Training epoch 769, recon_loss:0.776925, zinb_loss:0.642351, cluster_loss:0.154852\n",
      "Clustering   769: ASW= 0.8175, DB= 0.2519, CH= 47951.3571\n",
      "Training epoch 770, recon_loss:0.777375, zinb_loss:0.642543, cluster_loss:0.154771\n",
      "Clustering   770: ASW= 0.8162, DB= 0.2503, CH= 47494.7784\n",
      "Training epoch 771, recon_loss:0.777223, zinb_loss:0.642378, cluster_loss:0.154939\n",
      "Clustering   771: ASW= 0.8175, DB= 0.2517, CH= 47902.5939\n",
      "Training epoch 772, recon_loss:0.777657, zinb_loss:0.642523, cluster_loss:0.154726\n",
      "Clustering   772: ASW= 0.8163, DB= 0.2498, CH= 47455.9876\n",
      "Training epoch 773, recon_loss:0.777357, zinb_loss:0.642314, cluster_loss:0.154855\n",
      "Clustering   773: ASW= 0.8174, DB= 0.2517, CH= 47929.0949\n",
      "Training epoch 774, recon_loss:0.777748, zinb_loss:0.642456, cluster_loss:0.154608\n",
      "Clustering   774: ASW= 0.8167, DB= 0.2500, CH= 47466.7829\n",
      "Training epoch 775, recon_loss:0.777261, zinb_loss:0.642362, cluster_loss:0.154642\n",
      "Clustering   775: ASW= 0.8175, DB= 0.2517, CH= 48040.5220\n",
      "Training epoch 776, recon_loss:0.777675, zinb_loss:0.642417, cluster_loss:0.154412\n",
      "Clustering   776: ASW= 0.8167, DB= 0.2502, CH= 47520.0338\n",
      "Training epoch 777, recon_loss:0.777097, zinb_loss:0.642360, cluster_loss:0.154397\n",
      "Clustering   777: ASW= 0.8177, DB= 0.2512, CH= 48195.8718\n",
      "Training epoch 778, recon_loss:0.777402, zinb_loss:0.642360, cluster_loss:0.154232\n",
      "Clustering   778: ASW= 0.8170, DB= 0.2500, CH= 47569.5095\n",
      "Training epoch 779, recon_loss:0.777089, zinb_loss:0.642446, cluster_loss:0.154225\n",
      "Clustering   779: ASW= 0.8177, DB= 0.2509, CH= 48318.4659\n",
      "Training epoch 780, recon_loss:0.777348, zinb_loss:0.642391, cluster_loss:0.154156\n",
      "Clustering   780: ASW= 0.8171, DB= 0.2500, CH= 47600.6326\n",
      "Training epoch 781, recon_loss:0.777106, zinb_loss:0.642578, cluster_loss:0.154125\n",
      "Clustering   781: ASW= 0.8178, DB= 0.2504, CH= 48402.8916\n",
      "Training epoch 782, recon_loss:0.777366, zinb_loss:0.642482, cluster_loss:0.154117\n",
      "Clustering   782: ASW= 0.8172, DB= 0.2501, CH= 47661.2916\n",
      "Training epoch 783, recon_loss:0.777195, zinb_loss:0.642721, cluster_loss:0.154126\n",
      "Clustering   783: ASW= 0.8177, DB= 0.2500, CH= 48385.4478\n",
      "Training epoch 784, recon_loss:0.777570, zinb_loss:0.642617, cluster_loss:0.154152\n",
      "Clustering   784: ASW= 0.8173, DB= 0.2500, CH= 47741.7082\n",
      "Training epoch 785, recon_loss:0.777457, zinb_loss:0.642872, cluster_loss:0.154243\n",
      "Clustering   785: ASW= 0.8176, DB= 0.2498, CH= 48351.9413\n",
      "Training epoch 786, recon_loss:0.777874, zinb_loss:0.642687, cluster_loss:0.154366\n",
      "Clustering   786: ASW= 0.8172, DB= 0.2504, CH= 47727.0965\n",
      "Training epoch 787, recon_loss:0.777787, zinb_loss:0.643018, cluster_loss:0.154381\n",
      "Clustering   787: ASW= 0.8178, DB= 0.2493, CH= 48381.5106\n",
      "Training epoch 788, recon_loss:0.778060, zinb_loss:0.642752, cluster_loss:0.154558\n",
      "Clustering   788: ASW= 0.8170, DB= 0.2506, CH= 47712.5417\n",
      "Training epoch 789, recon_loss:0.777937, zinb_loss:0.643020, cluster_loss:0.154499\n",
      "Clustering   789: ASW= 0.8178, DB= 0.2491, CH= 48367.9728\n",
      "Training epoch 790, recon_loss:0.777849, zinb_loss:0.642692, cluster_loss:0.154598\n",
      "Clustering   790: ASW= 0.8170, DB= 0.2508, CH= 47727.6427\n",
      "Training epoch 791, recon_loss:0.777770, zinb_loss:0.642872, cluster_loss:0.154431\n",
      "Clustering   791: ASW= 0.8181, DB= 0.2493, CH= 48373.9331\n",
      "Training epoch 792, recon_loss:0.777454, zinb_loss:0.642546, cluster_loss:0.154465\n",
      "Clustering   792: ASW= 0.8172, DB= 0.2506, CH= 47795.4850\n",
      "Training epoch 793, recon_loss:0.777261, zinb_loss:0.642619, cluster_loss:0.154237\n",
      "Clustering   793: ASW= 0.8181, DB= 0.2492, CH= 48399.6225\n",
      "Training epoch 794, recon_loss:0.777027, zinb_loss:0.642347, cluster_loss:0.154358\n",
      "Clustering   794: ASW= 0.8172, DB= 0.2508, CH= 47901.9421\n",
      "Training epoch 795, recon_loss:0.776922, zinb_loss:0.642503, cluster_loss:0.154068\n",
      "Clustering   795: ASW= 0.8186, DB= 0.2489, CH= 48454.2996\n",
      "Training epoch 796, recon_loss:0.776757, zinb_loss:0.642251, cluster_loss:0.154356\n",
      "Clustering   796: ASW= 0.8169, DB= 0.2507, CH= 47818.2988\n",
      "Training epoch 797, recon_loss:0.776722, zinb_loss:0.642344, cluster_loss:0.154045\n",
      "Clustering   797: ASW= 0.8187, DB= 0.2486, CH= 48441.2521\n",
      "Training epoch 798, recon_loss:0.776676, zinb_loss:0.642167, cluster_loss:0.154342\n",
      "Clustering   798: ASW= 0.8170, DB= 0.2504, CH= 47976.0835\n",
      "Training epoch 799, recon_loss:0.776579, zinb_loss:0.642202, cluster_loss:0.154067\n",
      "Clustering   799: ASW= 0.8190, DB= 0.2488, CH= 48440.0572\n",
      "Training epoch 800, recon_loss:0.776459, zinb_loss:0.642036, cluster_loss:0.154354\n",
      "Clustering   800: ASW= 0.8169, DB= 0.2502, CH= 48030.9443\n",
      "Training epoch 801, recon_loss:0.776354, zinb_loss:0.642006, cluster_loss:0.154069\n",
      "Clustering   801: ASW= 0.8190, DB= 0.2491, CH= 48375.9419\n",
      "Training epoch 802, recon_loss:0.776317, zinb_loss:0.641914, cluster_loss:0.154342\n",
      "Clustering   802: ASW= 0.8168, DB= 0.2502, CH= 48102.7502\n",
      "Training epoch 803, recon_loss:0.776293, zinb_loss:0.641914, cluster_loss:0.154051\n",
      "Clustering   803: ASW= 0.8191, DB= 0.2494, CH= 48320.9716\n",
      "Training epoch 804, recon_loss:0.776182, zinb_loss:0.641863, cluster_loss:0.154286\n",
      "Clustering   804: ASW= 0.8166, DB= 0.2503, CH= 48104.1027\n",
      "Training epoch 805, recon_loss:0.776241, zinb_loss:0.641802, cluster_loss:0.153981\n",
      "Clustering   805: ASW= 0.8187, DB= 0.2487, CH= 48238.8003\n",
      "Training epoch 806, recon_loss:0.776287, zinb_loss:0.641803, cluster_loss:0.154259\n",
      "Clustering   806: ASW= 0.8162, DB= 0.2501, CH= 48069.5493\n",
      "Training epoch 807, recon_loss:0.776543, zinb_loss:0.641743, cluster_loss:0.154024\n",
      "Clustering   807: ASW= 0.8186, DB= 0.2488, CH= 48205.3504\n",
      "Training epoch 808, recon_loss:0.776842, zinb_loss:0.641853, cluster_loss:0.154445\n",
      "Clustering   808: ASW= 0.8159, DB= 0.2499, CH= 47990.6884\n",
      "Training epoch 809, recon_loss:0.777597, zinb_loss:0.641726, cluster_loss:0.154176\n",
      "Clustering   809: ASW= 0.8180, DB= 0.2495, CH= 48075.4070\n",
      "Training epoch 810, recon_loss:0.778103, zinb_loss:0.641896, cluster_loss:0.154592\n",
      "Clustering   810: ASW= 0.8158, DB= 0.2492, CH= 47977.0058\n",
      "Training epoch 811, recon_loss:0.778300, zinb_loss:0.641769, cluster_loss:0.154309\n",
      "Clustering   811: ASW= 0.8181, DB= 0.2492, CH= 48138.0054\n",
      "Training epoch 812, recon_loss:0.778090, zinb_loss:0.641837, cluster_loss:0.154685\n",
      "Clustering   812: ASW= 0.8160, DB= 0.2492, CH= 47929.4855\n",
      "Training epoch 813, recon_loss:0.778098, zinb_loss:0.641629, cluster_loss:0.154289\n",
      "Clustering   813: ASW= 0.8177, DB= 0.2498, CH= 48024.8445\n",
      "Training epoch 814, recon_loss:0.777516, zinb_loss:0.641690, cluster_loss:0.154535\n",
      "Clustering   814: ASW= 0.8165, DB= 0.2485, CH= 48183.1802\n",
      "Training epoch 815, recon_loss:0.777463, zinb_loss:0.641537, cluster_loss:0.154098\n",
      "Clustering   815: ASW= 0.8182, DB= 0.2489, CH= 48281.9729\n",
      "Training epoch 816, recon_loss:0.776912, zinb_loss:0.641548, cluster_loss:0.154250\n",
      "Clustering   816: ASW= 0.8172, DB= 0.2483, CH= 48368.4019\n",
      "Training epoch 817, recon_loss:0.776816, zinb_loss:0.641339, cluster_loss:0.154047\n",
      "Clustering   817: ASW= 0.8183, DB= 0.2493, CH= 48405.7639\n",
      "Training epoch 818, recon_loss:0.776597, zinb_loss:0.641512, cluster_loss:0.154149\n",
      "Clustering   818: ASW= 0.8179, DB= 0.2478, CH= 48606.5199\n",
      "Training epoch 819, recon_loss:0.776546, zinb_loss:0.641218, cluster_loss:0.154173\n",
      "Clustering   819: ASW= 0.8182, DB= 0.2495, CH= 48396.1961\n",
      "Training epoch 820, recon_loss:0.776774, zinb_loss:0.641644, cluster_loss:0.154259\n",
      "Clustering   820: ASW= 0.8182, DB= 0.2475, CH= 48763.4320\n",
      "Training epoch 821, recon_loss:0.776839, zinb_loss:0.641285, cluster_loss:0.154583\n",
      "Clustering   821: ASW= 0.8178, DB= 0.2503, CH= 48176.7142\n",
      "Training epoch 822, recon_loss:0.777665, zinb_loss:0.642235, cluster_loss:0.154503\n",
      "Clustering   822: ASW= 0.8187, DB= 0.2472, CH= 48936.6512\n",
      "Training epoch 823, recon_loss:0.777820, zinb_loss:0.641901, cluster_loss:0.154954\n",
      "Clustering   823: ASW= 0.8176, DB= 0.2507, CH= 47989.3517\n",
      "Training epoch 824, recon_loss:0.778772, zinb_loss:0.642889, cluster_loss:0.154606\n",
      "Clustering   824: ASW= 0.8190, DB= 0.2472, CH= 49019.9698\n",
      "Training epoch 825, recon_loss:0.778876, zinb_loss:0.642552, cluster_loss:0.155118\n",
      "Clustering   825: ASW= 0.8175, DB= 0.2504, CH= 47861.7666\n",
      "Training epoch 826, recon_loss:0.778771, zinb_loss:0.643058, cluster_loss:0.154575\n",
      "Clustering   826: ASW= 0.8189, DB= 0.2470, CH= 48913.4511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 827, recon_loss:0.777959, zinb_loss:0.642427, cluster_loss:0.154883\n",
      "Clustering   827: ASW= 0.8177, DB= 0.2507, CH= 47915.0480\n",
      "Training epoch 828, recon_loss:0.777856, zinb_loss:0.642755, cluster_loss:0.154371\n",
      "Clustering   828: ASW= 0.8188, DB= 0.2477, CH= 48821.8021\n",
      "Training epoch 829, recon_loss:0.777135, zinb_loss:0.642103, cluster_loss:0.154636\n",
      "Clustering   829: ASW= 0.8178, DB= 0.2504, CH= 48037.2285\n",
      "Training epoch 830, recon_loss:0.777458, zinb_loss:0.642535, cluster_loss:0.154187\n",
      "Clustering   830: ASW= 0.8189, DB= 0.2474, CH= 48886.5609\n",
      "Training epoch 831, recon_loss:0.776849, zinb_loss:0.641941, cluster_loss:0.154521\n",
      "Clustering   831: ASW= 0.8181, DB= 0.2506, CH= 48239.6799\n",
      "Training epoch 832, recon_loss:0.777565, zinb_loss:0.642464, cluster_loss:0.154255\n",
      "Clustering   832: ASW= 0.8187, DB= 0.2479, CH= 48836.7624\n",
      "Training epoch 833, recon_loss:0.776898, zinb_loss:0.641913, cluster_loss:0.154519\n",
      "Clustering   833: ASW= 0.8183, DB= 0.2507, CH= 48351.9679\n",
      "Training epoch 834, recon_loss:0.777486, zinb_loss:0.642343, cluster_loss:0.154225\n",
      "Clustering   834: ASW= 0.8189, DB= 0.2474, CH= 48877.4241\n",
      "Training epoch 835, recon_loss:0.776627, zinb_loss:0.641858, cluster_loss:0.154377\n",
      "Clustering   835: ASW= 0.8186, DB= 0.2503, CH= 48513.1028\n",
      "Training epoch 836, recon_loss:0.777136, zinb_loss:0.642107, cluster_loss:0.154150\n",
      "Clustering   836: ASW= 0.8187, DB= 0.2477, CH= 48818.0219\n",
      "Training epoch 837, recon_loss:0.776188, zinb_loss:0.641703, cluster_loss:0.154169\n",
      "Clustering   837: ASW= 0.8188, DB= 0.2500, CH= 48629.4057\n",
      "Training epoch 838, recon_loss:0.776659, zinb_loss:0.641880, cluster_loss:0.153944\n",
      "Clustering   838: ASW= 0.8188, DB= 0.2479, CH= 48890.2256\n",
      "Training epoch 839, recon_loss:0.775945, zinb_loss:0.641649, cluster_loss:0.153953\n",
      "Clustering   839: ASW= 0.8192, DB= 0.2493, CH= 48838.2534\n",
      "Training epoch 840, recon_loss:0.776527, zinb_loss:0.641753, cluster_loss:0.153930\n",
      "Clustering   840: ASW= 0.8186, DB= 0.2479, CH= 48808.6493\n",
      "Training epoch 841, recon_loss:0.776093, zinb_loss:0.641703, cluster_loss:0.153929\n",
      "Clustering   841: ASW= 0.8193, DB= 0.2488, CH= 48952.4310\n",
      "Training epoch 842, recon_loss:0.776862, zinb_loss:0.641785, cluster_loss:0.154051\n",
      "Clustering   842: ASW= 0.8184, DB= 0.2480, CH= 48691.5546\n",
      "Training epoch 843, recon_loss:0.776577, zinb_loss:0.641929, cluster_loss:0.153992\n",
      "Clustering   843: ASW= 0.8194, DB= 0.2483, CH= 49078.8101\n",
      "Training epoch 844, recon_loss:0.777104, zinb_loss:0.641928, cluster_loss:0.154271\n",
      "Clustering   844: ASW= 0.8179, DB= 0.2485, CH= 48428.4199\n",
      "Training epoch 845, recon_loss:0.776686, zinb_loss:0.642224, cluster_loss:0.153975\n",
      "Clustering   845: ASW= 0.8194, DB= 0.2481, CH= 49182.5850\n",
      "Training epoch 846, recon_loss:0.776856, zinb_loss:0.642071, cluster_loss:0.154360\n",
      "Clustering   846: ASW= 0.8174, DB= 0.2488, CH= 48188.5857\n",
      "Training epoch 847, recon_loss:0.776391, zinb_loss:0.642389, cluster_loss:0.153843\n",
      "Clustering   847: ASW= 0.8193, DB= 0.2476, CH= 49199.4540\n",
      "Training epoch 848, recon_loss:0.776544, zinb_loss:0.642187, cluster_loss:0.154341\n",
      "Clustering   848: ASW= 0.8172, DB= 0.2497, CH= 48098.5152\n",
      "Training epoch 849, recon_loss:0.776126, zinb_loss:0.642438, cluster_loss:0.153665\n",
      "Clustering   849: ASW= 0.8194, DB= 0.2473, CH= 49257.7995\n",
      "Training epoch 850, recon_loss:0.776290, zinb_loss:0.642164, cluster_loss:0.154293\n",
      "Clustering   850: ASW= 0.8172, DB= 0.2500, CH= 48094.6433\n",
      "Training epoch 851, recon_loss:0.776050, zinb_loss:0.642405, cluster_loss:0.153577\n",
      "Clustering   851: ASW= 0.8195, DB= 0.2470, CH= 49271.4837\n",
      "Training epoch 852, recon_loss:0.776298, zinb_loss:0.642146, cluster_loss:0.154313\n",
      "Clustering   852: ASW= 0.8173, DB= 0.2500, CH= 48112.5844\n",
      "Training epoch 853, recon_loss:0.776291, zinb_loss:0.642317, cluster_loss:0.153604\n",
      "Clustering   853: ASW= 0.8195, DB= 0.2470, CH= 49225.9738\n",
      "Training epoch 854, recon_loss:0.776489, zinb_loss:0.642110, cluster_loss:0.154367\n",
      "Clustering   854: ASW= 0.8176, DB= 0.2497, CH= 48214.9162\n",
      "Training epoch 855, recon_loss:0.776680, zinb_loss:0.642224, cluster_loss:0.153711\n",
      "Clustering   855: ASW= 0.8194, DB= 0.2468, CH= 49177.6360\n",
      "Training epoch 856, recon_loss:0.776897, zinb_loss:0.642057, cluster_loss:0.154503\n",
      "Clustering   856: ASW= 0.8178, DB= 0.2490, CH= 48223.4335\n",
      "Training epoch 857, recon_loss:0.777168, zinb_loss:0.642093, cluster_loss:0.153857\n",
      "Clustering   857: ASW= 0.8193, DB= 0.2470, CH= 49111.9809\n",
      "Training epoch 858, recon_loss:0.777337, zinb_loss:0.641986, cluster_loss:0.154566\n",
      "Clustering   858: ASW= 0.8183, DB= 0.2485, CH= 48402.0899\n",
      "Training epoch 859, recon_loss:0.777555, zinb_loss:0.641897, cluster_loss:0.153988\n",
      "Clustering   859: ASW= 0.8191, DB= 0.2473, CH= 49053.1113\n",
      "Training epoch 860, recon_loss:0.777549, zinb_loss:0.641824, cluster_loss:0.154606\n",
      "Clustering   860: ASW= 0.8186, DB= 0.2482, CH= 48452.2812\n",
      "Training epoch 861, recon_loss:0.777591, zinb_loss:0.641687, cluster_loss:0.154002\n",
      "Clustering   861: ASW= 0.8190, DB= 0.2476, CH= 49075.9767\n",
      "Training epoch 862, recon_loss:0.777537, zinb_loss:0.641681, cluster_loss:0.154504\n",
      "Clustering   862: ASW= 0.8192, DB= 0.2474, CH= 48710.5364\n",
      "Training epoch 863, recon_loss:0.777382, zinb_loss:0.641459, cluster_loss:0.153977\n",
      "Clustering   863: ASW= 0.8190, DB= 0.2476, CH= 49083.5528\n",
      "Training epoch 864, recon_loss:0.777363, zinb_loss:0.641582, cluster_loss:0.154385\n",
      "Clustering   864: ASW= 0.8195, DB= 0.2470, CH= 48804.1760\n",
      "Training epoch 865, recon_loss:0.777330, zinb_loss:0.641377, cluster_loss:0.153967\n",
      "Clustering   865: ASW= 0.8189, DB= 0.2481, CH= 49119.5843\n",
      "Training epoch 866, recon_loss:0.777327, zinb_loss:0.641616, cluster_loss:0.154273\n",
      "Clustering   866: ASW= 0.8198, DB= 0.2466, CH= 48910.9381\n",
      "Training epoch 867, recon_loss:0.777462, zinb_loss:0.641447, cluster_loss:0.154025\n",
      "Clustering   867: ASW= 0.8189, DB= 0.2484, CH= 49139.2860\n",
      "Training epoch 868, recon_loss:0.777375, zinb_loss:0.641889, cluster_loss:0.154189\n",
      "Clustering   868: ASW= 0.8201, DB= 0.2462, CH= 49053.0416\n",
      "Training epoch 869, recon_loss:0.777481, zinb_loss:0.641662, cluster_loss:0.154128\n",
      "Clustering   869: ASW= 0.8186, DB= 0.2485, CH= 49001.0625\n",
      "Training epoch 870, recon_loss:0.777012, zinb_loss:0.642208, cluster_loss:0.154050\n",
      "Clustering   870: ASW= 0.8204, DB= 0.2454, CH= 49177.6975\n",
      "Training epoch 871, recon_loss:0.776890, zinb_loss:0.641845, cluster_loss:0.154109\n",
      "Clustering   871: ASW= 0.8187, DB= 0.2491, CH= 48911.9288\n",
      "Training epoch 872, recon_loss:0.776434, zinb_loss:0.642285, cluster_loss:0.153890\n",
      "Clustering   872: ASW= 0.8204, DB= 0.2458, CH= 49266.3066\n",
      "Training epoch 873, recon_loss:0.776381, zinb_loss:0.641928, cluster_loss:0.153984\n",
      "Clustering   873: ASW= 0.8187, DB= 0.2490, CH= 48958.6766\n",
      "Training epoch 874, recon_loss:0.776054, zinb_loss:0.642358, cluster_loss:0.153766\n",
      "Clustering   874: ASW= 0.8204, DB= 0.2453, CH= 49322.4070\n",
      "Training epoch 875, recon_loss:0.776039, zinb_loss:0.641927, cluster_loss:0.153966\n",
      "Clustering   875: ASW= 0.8188, DB= 0.2494, CH= 48940.5879\n",
      "Training epoch 876, recon_loss:0.775909, zinb_loss:0.642292, cluster_loss:0.153727\n",
      "Clustering   876: ASW= 0.8203, DB= 0.2457, CH= 49378.6881\n",
      "Training epoch 877, recon_loss:0.775911, zinb_loss:0.641876, cluster_loss:0.153920\n",
      "Clustering   877: ASW= 0.8189, DB= 0.2492, CH= 49021.6875\n",
      "Training epoch 878, recon_loss:0.775879, zinb_loss:0.642224, cluster_loss:0.153697\n",
      "Clustering   878: ASW= 0.8204, DB= 0.2455, CH= 49424.6005\n",
      "Training epoch 879, recon_loss:0.775882, zinb_loss:0.641790, cluster_loss:0.153885\n",
      "Clustering   879: ASW= 0.8192, DB= 0.2491, CH= 49060.6757\n",
      "Training epoch 880, recon_loss:0.775906, zinb_loss:0.642080, cluster_loss:0.153682\n",
      "Clustering   880: ASW= 0.8203, DB= 0.2462, CH= 49474.9811\n",
      "Training epoch 881, recon_loss:0.775888, zinb_loss:0.641684, cluster_loss:0.153857\n",
      "Clustering   881: ASW= 0.8194, DB= 0.2490, CH= 49091.7465\n",
      "Training epoch 882, recon_loss:0.776044, zinb_loss:0.641960, cluster_loss:0.153683\n",
      "Clustering   882: ASW= 0.8202, DB= 0.2460, CH= 49515.3348\n",
      "Training epoch 883, recon_loss:0.776074, zinb_loss:0.641578, cluster_loss:0.153841\n",
      "Clustering   883: ASW= 0.8195, DB= 0.2490, CH= 49137.5406\n",
      "Training epoch 884, recon_loss:0.776212, zinb_loss:0.641790, cluster_loss:0.153684\n",
      "Clustering   884: ASW= 0.8201, DB= 0.2460, CH= 49532.8735\n",
      "Training epoch 885, recon_loss:0.776194, zinb_loss:0.641456, cluster_loss:0.153851\n",
      "Clustering   885: ASW= 0.8194, DB= 0.2487, CH= 49141.0682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 886, recon_loss:0.776248, zinb_loss:0.641718, cluster_loss:0.153637\n",
      "Clustering   886: ASW= 0.8203, DB= 0.2461, CH= 49586.6471\n",
      "Training epoch 887, recon_loss:0.776062, zinb_loss:0.641394, cluster_loss:0.153757\n",
      "Clustering   887: ASW= 0.8197, DB= 0.2482, CH= 49189.4583\n",
      "Training epoch 888, recon_loss:0.776075, zinb_loss:0.641554, cluster_loss:0.153617\n",
      "Clustering   888: ASW= 0.8201, DB= 0.2461, CH= 49618.5872\n",
      "Training epoch 889, recon_loss:0.775857, zinb_loss:0.641287, cluster_loss:0.153741\n",
      "Clustering   889: ASW= 0.8197, DB= 0.2483, CH= 49236.0970\n",
      "Training epoch 890, recon_loss:0.775937, zinb_loss:0.641456, cluster_loss:0.153599\n",
      "Clustering   890: ASW= 0.8202, DB= 0.2461, CH= 49685.8725\n",
      "Training epoch 891, recon_loss:0.775771, zinb_loss:0.641211, cluster_loss:0.153749\n",
      "Clustering   891: ASW= 0.8198, DB= 0.2477, CH= 49256.8575\n",
      "Training epoch 892, recon_loss:0.776006, zinb_loss:0.641408, cluster_loss:0.153616\n",
      "Clustering   892: ASW= 0.8203, DB= 0.2460, CH= 49729.4514\n",
      "Training epoch 893, recon_loss:0.776000, zinb_loss:0.641221, cluster_loss:0.153811\n",
      "Clustering   893: ASW= 0.8198, DB= 0.2476, CH= 49261.4920\n",
      "Training epoch 894, recon_loss:0.776626, zinb_loss:0.641509, cluster_loss:0.153740\n",
      "Clustering   894: ASW= 0.8205, DB= 0.2456, CH= 49730.7893\n",
      "Training epoch 895, recon_loss:0.776918, zinb_loss:0.641520, cluster_loss:0.153961\n",
      "Clustering   895: ASW= 0.8198, DB= 0.2476, CH= 49312.8215\n",
      "Training epoch 896, recon_loss:0.777924, zinb_loss:0.641974, cluster_loss:0.154006\n",
      "Clustering   896: ASW= 0.8206, DB= 0.2454, CH= 49635.2764\n",
      "Training epoch 897, recon_loss:0.778024, zinb_loss:0.642160, cluster_loss:0.154118\n",
      "Clustering   897: ASW= 0.8199, DB= 0.2477, CH= 49419.9058\n",
      "Training epoch 898, recon_loss:0.778888, zinb_loss:0.642606, cluster_loss:0.154226\n",
      "Clustering   898: ASW= 0.8208, DB= 0.2453, CH= 49470.1499\n",
      "Training epoch 899, recon_loss:0.777643, zinb_loss:0.642440, cluster_loss:0.154074\n",
      "Clustering   899: ASW= 0.8201, DB= 0.2476, CH= 49560.5846\n",
      "Training epoch 900, recon_loss:0.778005, zinb_loss:0.642648, cluster_loss:0.154119\n",
      "Clustering   900: ASW= 0.8209, DB= 0.2450, CH= 49384.7983\n",
      "Training epoch 901, recon_loss:0.776661, zinb_loss:0.642404, cluster_loss:0.153874\n",
      "Clustering   901: ASW= 0.8200, DB= 0.2474, CH= 49649.3758\n",
      "Training epoch 902, recon_loss:0.777056, zinb_loss:0.642510, cluster_loss:0.154074\n",
      "Clustering   902: ASW= 0.8208, DB= 0.2450, CH= 49246.1665\n",
      "Training epoch 903, recon_loss:0.776133, zinb_loss:0.642395, cluster_loss:0.153687\n",
      "Clustering   903: ASW= 0.8202, DB= 0.2473, CH= 49827.8885\n",
      "Training epoch 904, recon_loss:0.776688, zinb_loss:0.642479, cluster_loss:0.154041\n",
      "Clustering   904: ASW= 0.8209, DB= 0.2449, CH= 49214.7281\n",
      "Training epoch 905, recon_loss:0.776016, zinb_loss:0.642453, cluster_loss:0.153575\n",
      "Clustering   905: ASW= 0.8202, DB= 0.2473, CH= 49962.7548\n",
      "Training epoch 906, recon_loss:0.776620, zinb_loss:0.642479, cluster_loss:0.154053\n",
      "Clustering   906: ASW= 0.8208, DB= 0.2448, CH= 49136.2638\n",
      "Training epoch 907, recon_loss:0.775994, zinb_loss:0.642473, cluster_loss:0.153460\n",
      "Clustering   907: ASW= 0.8203, DB= 0.2471, CH= 50121.1496\n",
      "Training epoch 908, recon_loss:0.776681, zinb_loss:0.642443, cluster_loss:0.154127\n",
      "Clustering   908: ASW= 0.8206, DB= 0.2447, CH= 48995.7732\n",
      "Training epoch 909, recon_loss:0.776141, zinb_loss:0.642524, cluster_loss:0.153410\n",
      "Clustering   909: ASW= 0.8204, DB= 0.2471, CH= 50248.5051\n",
      "Training epoch 910, recon_loss:0.776631, zinb_loss:0.642364, cluster_loss:0.154162\n",
      "Clustering   910: ASW= 0.8208, DB= 0.2447, CH= 49014.4653\n",
      "Training epoch 911, recon_loss:0.775948, zinb_loss:0.642436, cluster_loss:0.153302\n",
      "Clustering   911: ASW= 0.8204, DB= 0.2470, CH= 50340.6364\n",
      "Training epoch 912, recon_loss:0.776161, zinb_loss:0.642151, cluster_loss:0.154118\n",
      "Clustering   912: ASW= 0.8210, DB= 0.2447, CH= 49106.5318\n",
      "Training epoch 913, recon_loss:0.775764, zinb_loss:0.642362, cluster_loss:0.153293\n",
      "Clustering   913: ASW= 0.8203, DB= 0.2465, CH= 50341.7872\n",
      "Training epoch 914, recon_loss:0.776203, zinb_loss:0.641954, cluster_loss:0.154253\n",
      "Clustering   914: ASW= 0.8210, DB= 0.2453, CH= 49148.2019\n",
      "Training epoch 915, recon_loss:0.776044, zinb_loss:0.642134, cluster_loss:0.153481\n",
      "Clustering   915: ASW= 0.8202, DB= 0.2461, CH= 50242.1804\n",
      "Training epoch 916, recon_loss:0.776715, zinb_loss:0.641684, cluster_loss:0.154564\n",
      "Clustering   916: ASW= 0.8207, DB= 0.2460, CH= 49102.8026\n",
      "Training epoch 917, recon_loss:0.776289, zinb_loss:0.641765, cluster_loss:0.153764\n",
      "Clustering   917: ASW= 0.8199, DB= 0.2461, CH= 50081.1733\n",
      "Training epoch 918, recon_loss:0.776485, zinb_loss:0.641361, cluster_loss:0.154756\n",
      "Clustering   918: ASW= 0.8206, DB= 0.2460, CH= 49043.0344\n",
      "Training epoch 919, recon_loss:0.776031, zinb_loss:0.641425, cluster_loss:0.153844\n",
      "Clustering   919: ASW= 0.8200, DB= 0.2456, CH= 49940.2353\n",
      "Training epoch 920, recon_loss:0.776033, zinb_loss:0.641055, cluster_loss:0.154786\n",
      "Clustering   920: ASW= 0.8205, DB= 0.2466, CH= 49015.7086\n",
      "Training epoch 921, recon_loss:0.776022, zinb_loss:0.641189, cluster_loss:0.153862\n",
      "Clustering   921: ASW= 0.8202, DB= 0.2454, CH= 49871.7022\n",
      "Training epoch 922, recon_loss:0.775995, zinb_loss:0.640775, cluster_loss:0.154793\n",
      "Clustering   922: ASW= 0.8204, DB= 0.2470, CH= 49013.4202\n",
      "Training epoch 923, recon_loss:0.776050, zinb_loss:0.640999, cluster_loss:0.153827\n",
      "Clustering   923: ASW= 0.8203, DB= 0.2452, CH= 49775.1593\n",
      "Training epoch 924, recon_loss:0.775631, zinb_loss:0.640561, cluster_loss:0.154574\n",
      "Clustering   924: ASW= 0.8206, DB= 0.2469, CH= 49208.0691\n",
      "Training epoch 925, recon_loss:0.775892, zinb_loss:0.640883, cluster_loss:0.153674\n",
      "Clustering   925: ASW= 0.8206, DB= 0.2445, CH= 49792.9730\n",
      "Training epoch 926, recon_loss:0.775377, zinb_loss:0.640449, cluster_loss:0.154408\n",
      "Clustering   926: ASW= 0.8206, DB= 0.2471, CH= 49378.4618\n",
      "Training epoch 927, recon_loss:0.775965, zinb_loss:0.640841, cluster_loss:0.153659\n",
      "Clustering   927: ASW= 0.8206, DB= 0.2441, CH= 49754.3804\n",
      "Training epoch 928, recon_loss:0.775498, zinb_loss:0.640606, cluster_loss:0.154298\n",
      "Clustering   928: ASW= 0.8210, DB= 0.2469, CH= 49661.6168\n",
      "Training epoch 929, recon_loss:0.776103, zinb_loss:0.640991, cluster_loss:0.153740\n",
      "Clustering   929: ASW= 0.8206, DB= 0.2443, CH= 49637.9140\n",
      "Training epoch 930, recon_loss:0.775426, zinb_loss:0.640912, cluster_loss:0.154189\n",
      "Clustering   930: ASW= 0.8212, DB= 0.2463, CH= 49907.6346\n",
      "Training epoch 931, recon_loss:0.775953, zinb_loss:0.641246, cluster_loss:0.153845\n",
      "Clustering   931: ASW= 0.8204, DB= 0.2444, CH= 49528.2611\n",
      "Training epoch 932, recon_loss:0.775294, zinb_loss:0.641312, cluster_loss:0.154089\n",
      "Clustering   932: ASW= 0.8214, DB= 0.2455, CH= 50078.0948\n",
      "Training epoch 933, recon_loss:0.775774, zinb_loss:0.641584, cluster_loss:0.153893\n",
      "Clustering   933: ASW= 0.8204, DB= 0.2447, CH= 49575.5879\n",
      "Training epoch 934, recon_loss:0.775276, zinb_loss:0.641691, cluster_loss:0.154007\n",
      "Clustering   934: ASW= 0.8217, DB= 0.2447, CH= 50171.4320\n",
      "Training epoch 935, recon_loss:0.775643, zinb_loss:0.641841, cluster_loss:0.153967\n",
      "Clustering   935: ASW= 0.8203, DB= 0.2449, CH= 49606.7985\n",
      "Training epoch 936, recon_loss:0.775418, zinb_loss:0.641946, cluster_loss:0.153999\n",
      "Clustering   936: ASW= 0.8219, DB= 0.2448, CH= 50261.8751\n",
      "Training epoch 937, recon_loss:0.775770, zinb_loss:0.641974, cluster_loss:0.154115\n",
      "Clustering   937: ASW= 0.8203, DB= 0.2453, CH= 49602.3933\n",
      "Training epoch 938, recon_loss:0.775727, zinb_loss:0.642072, cluster_loss:0.154040\n",
      "Clustering   938: ASW= 0.8219, DB= 0.2440, CH= 50241.8509\n",
      "Training epoch 939, recon_loss:0.776015, zinb_loss:0.641972, cluster_loss:0.154267\n",
      "Clustering   939: ASW= 0.8202, DB= 0.2454, CH= 49552.2925\n",
      "Training epoch 940, recon_loss:0.776008, zinb_loss:0.642108, cluster_loss:0.154051\n",
      "Clustering   940: ASW= 0.8220, DB= 0.2439, CH= 50264.3502\n",
      "Training epoch 941, recon_loss:0.776126, zinb_loss:0.641864, cluster_loss:0.154313\n",
      "Clustering   941: ASW= 0.8202, DB= 0.2459, CH= 49507.5603\n",
      "Training epoch 942, recon_loss:0.776039, zinb_loss:0.642041, cluster_loss:0.153949\n",
      "Clustering   942: ASW= 0.8221, DB= 0.2437, CH= 50320.6255\n",
      "Training epoch 943, recon_loss:0.775981, zinb_loss:0.641658, cluster_loss:0.154253\n",
      "Clustering   943: ASW= 0.8203, DB= 0.2459, CH= 49477.8211\n",
      "Training epoch 944, recon_loss:0.775839, zinb_loss:0.641838, cluster_loss:0.153782\n",
      "Clustering   944: ASW= 0.8222, DB= 0.2435, CH= 50390.3066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 945, recon_loss:0.775697, zinb_loss:0.641373, cluster_loss:0.154116\n",
      "Clustering   945: ASW= 0.8204, DB= 0.2462, CH= 49517.5703\n",
      "Training epoch 946, recon_loss:0.775706, zinb_loss:0.641617, cluster_loss:0.153643\n",
      "Clustering   946: ASW= 0.8223, DB= 0.2434, CH= 50452.5558\n",
      "Training epoch 947, recon_loss:0.775548, zinb_loss:0.641124, cluster_loss:0.153979\n",
      "Clustering   947: ASW= 0.8206, DB= 0.2460, CH= 49581.5547\n",
      "Training epoch 948, recon_loss:0.775715, zinb_loss:0.641411, cluster_loss:0.153572\n",
      "Clustering   948: ASW= 0.8223, DB= 0.2436, CH= 50440.3620\n",
      "Training epoch 949, recon_loss:0.775554, zinb_loss:0.640954, cluster_loss:0.153866\n",
      "Clustering   949: ASW= 0.8208, DB= 0.2456, CH= 49667.0371\n",
      "Training epoch 950, recon_loss:0.775860, zinb_loss:0.641265, cluster_loss:0.153520\n",
      "Clustering   950: ASW= 0.8222, DB= 0.2437, CH= 50460.8942\n",
      "Training epoch 951, recon_loss:0.775514, zinb_loss:0.640827, cluster_loss:0.153793\n",
      "Clustering   951: ASW= 0.8209, DB= 0.2460, CH= 49678.3158\n",
      "Training epoch 952, recon_loss:0.776003, zinb_loss:0.641251, cluster_loss:0.153448\n",
      "Clustering   952: ASW= 0.8223, DB= 0.2439, CH= 50473.1184\n",
      "Training epoch 953, recon_loss:0.775709, zinb_loss:0.640835, cluster_loss:0.153730\n",
      "Clustering   953: ASW= 0.8212, DB= 0.2455, CH= 49782.0559\n",
      "Training epoch 954, recon_loss:0.776081, zinb_loss:0.641191, cluster_loss:0.153390\n",
      "Clustering   954: ASW= 0.8223, DB= 0.2437, CH= 50508.5557\n",
      "Training epoch 955, recon_loss:0.775705, zinb_loss:0.640885, cluster_loss:0.153591\n",
      "Clustering   955: ASW= 0.8216, DB= 0.2455, CH= 49942.8353\n",
      "Training epoch 956, recon_loss:0.775884, zinb_loss:0.641126, cluster_loss:0.153374\n",
      "Clustering   956: ASW= 0.8220, DB= 0.2438, CH= 50456.2604\n",
      "Training epoch 957, recon_loss:0.775367, zinb_loss:0.640870, cluster_loss:0.153429\n",
      "Clustering   957: ASW= 0.8218, DB= 0.2453, CH= 50021.5257\n",
      "Training epoch 958, recon_loss:0.775568, zinb_loss:0.641136, cluster_loss:0.153301\n",
      "Clustering   958: ASW= 0.8220, DB= 0.2435, CH= 50543.7448\n",
      "Training epoch 959, recon_loss:0.775313, zinb_loss:0.641017, cluster_loss:0.153332\n",
      "Clustering   959: ASW= 0.8220, DB= 0.2445, CH= 50103.0253\n",
      "Training epoch 960, recon_loss:0.775505, zinb_loss:0.641212, cluster_loss:0.153419\n",
      "Clustering   960: ASW= 0.8216, DB= 0.2441, CH= 50358.6860\n",
      "Training epoch 961, recon_loss:0.775427, zinb_loss:0.641155, cluster_loss:0.153333\n",
      "Clustering   961: ASW= 0.8218, DB= 0.2440, CH= 50036.3343\n",
      "Training epoch 962, recon_loss:0.775604, zinb_loss:0.641232, cluster_loss:0.153506\n",
      "Clustering   962: ASW= 0.8214, DB= 0.2443, CH= 50348.6532\n",
      "Training epoch 963, recon_loss:0.776001, zinb_loss:0.641393, cluster_loss:0.153394\n",
      "Clustering   963: ASW= 0.8218, DB= 0.2436, CH= 50133.4659\n",
      "Training epoch 964, recon_loss:0.775528, zinb_loss:0.641249, cluster_loss:0.153777\n",
      "Clustering   964: ASW= 0.8203, DB= 0.2454, CH= 49707.4270\n",
      "Training epoch 965, recon_loss:0.775644, zinb_loss:0.641277, cluster_loss:0.153624\n",
      "Clustering   965: ASW= 0.8206, DB= 0.2450, CH= 49505.6651\n",
      "Training epoch 966, recon_loss:0.775920, zinb_loss:0.641359, cluster_loss:0.154351\n",
      "Clustering   966: ASW= 0.8197, DB= 0.2457, CH= 49611.9980\n",
      "Training epoch 967, recon_loss:0.775865, zinb_loss:0.641455, cluster_loss:0.153637\n",
      "Clustering   967: ASW= 0.8209, DB= 0.2441, CH= 49806.9834\n",
      "Training epoch 968, recon_loss:0.775242, zinb_loss:0.641323, cluster_loss:0.154025\n",
      "Clustering   968: ASW= 0.8200, DB= 0.2452, CH= 49682.6118\n",
      "Training epoch 969, recon_loss:0.775132, zinb_loss:0.641191, cluster_loss:0.153353\n",
      "Clustering   969: ASW= 0.8210, DB= 0.2440, CH= 49892.1741\n",
      "Training epoch 970, recon_loss:0.774995, zinb_loss:0.641213, cluster_loss:0.153712\n",
      "Clustering   970: ASW= 0.8202, DB= 0.2448, CH= 49904.3556\n",
      "Training epoch 971, recon_loss:0.774953, zinb_loss:0.640983, cluster_loss:0.153206\n",
      "Clustering   971: ASW= 0.8211, DB= 0.2442, CH= 49945.0347\n",
      "Training epoch 972, recon_loss:0.775123, zinb_loss:0.641131, cluster_loss:0.153545\n",
      "Clustering   972: ASW= 0.8204, DB= 0.2439, CH= 50083.6425\n",
      "Training epoch 973, recon_loss:0.775388, zinb_loss:0.640825, cluster_loss:0.153212\n",
      "Clustering   973: ASW= 0.8211, DB= 0.2446, CH= 50003.1611\n",
      "Training epoch 974, recon_loss:0.775489, zinb_loss:0.641091, cluster_loss:0.153475\n",
      "Clustering   974: ASW= 0.8207, DB= 0.2439, CH= 50221.4849\n",
      "Training epoch 975, recon_loss:0.775899, zinb_loss:0.640686, cluster_loss:0.153341\n",
      "Clustering   975: ASW= 0.8210, DB= 0.2448, CH= 49975.2323\n",
      "Training epoch 976, recon_loss:0.775533, zinb_loss:0.641061, cluster_loss:0.153438\n",
      "Clustering   976: ASW= 0.8210, DB= 0.2435, CH= 50348.5970\n",
      "Training epoch 977, recon_loss:0.775812, zinb_loss:0.640572, cluster_loss:0.153414\n",
      "Clustering   977: ASW= 0.8210, DB= 0.2450, CH= 50029.6433\n",
      "Training epoch 978, recon_loss:0.775255, zinb_loss:0.641029, cluster_loss:0.153381\n",
      "Clustering   978: ASW= 0.8215, DB= 0.2435, CH= 50457.8365\n",
      "Training epoch 979, recon_loss:0.775421, zinb_loss:0.640511, cluster_loss:0.153397\n",
      "Clustering   979: ASW= 0.8214, DB= 0.2450, CH= 50177.2513\n",
      "Training epoch 980, recon_loss:0.774964, zinb_loss:0.641081, cluster_loss:0.153307\n",
      "Clustering   980: ASW= 0.8220, DB= 0.2429, CH= 50649.4802\n",
      "Training epoch 981, recon_loss:0.775087, zinb_loss:0.640552, cluster_loss:0.153419\n",
      "Clustering   981: ASW= 0.8215, DB= 0.2454, CH= 50247.2643\n",
      "Training epoch 982, recon_loss:0.774816, zinb_loss:0.641254, cluster_loss:0.153259\n",
      "Clustering   982: ASW= 0.8225, DB= 0.2425, CH= 50773.6733\n",
      "Training epoch 983, recon_loss:0.774959, zinb_loss:0.640777, cluster_loss:0.153436\n",
      "Clustering   983: ASW= 0.8216, DB= 0.2455, CH= 50279.7720\n",
      "Training epoch 984, recon_loss:0.774858, zinb_loss:0.641565, cluster_loss:0.153215\n",
      "Clustering   984: ASW= 0.8228, DB= 0.2420, CH= 50878.0192\n",
      "Training epoch 985, recon_loss:0.774992, zinb_loss:0.641189, cluster_loss:0.153491\n",
      "Clustering   985: ASW= 0.8216, DB= 0.2453, CH= 50272.7620\n",
      "Training epoch 986, recon_loss:0.775061, zinb_loss:0.641959, cluster_loss:0.153230\n",
      "Clustering   986: ASW= 0.8231, DB= 0.2419, CH= 50939.6393\n",
      "Training epoch 987, recon_loss:0.774991, zinb_loss:0.641593, cluster_loss:0.153502\n",
      "Clustering   987: ASW= 0.8216, DB= 0.2457, CH= 50311.2786\n",
      "Training epoch 988, recon_loss:0.774966, zinb_loss:0.642084, cluster_loss:0.153216\n",
      "Clustering   988: ASW= 0.8232, DB= 0.2416, CH= 50958.2240\n",
      "Training epoch 989, recon_loss:0.774881, zinb_loss:0.641655, cluster_loss:0.153486\n",
      "Clustering   989: ASW= 0.8214, DB= 0.2454, CH= 50347.6130\n",
      "Training epoch 990, recon_loss:0.774816, zinb_loss:0.641966, cluster_loss:0.153200\n",
      "Clustering   990: ASW= 0.8233, DB= 0.2416, CH= 50920.7321\n",
      "Training epoch 991, recon_loss:0.774580, zinb_loss:0.641527, cluster_loss:0.153281\n",
      "Clustering   991: ASW= 0.8217, DB= 0.2451, CH= 50522.3623\n",
      "Training epoch 992, recon_loss:0.774458, zinb_loss:0.641629, cluster_loss:0.153183\n",
      "Clustering   992: ASW= 0.8231, DB= 0.2417, CH= 50901.0535\n",
      "Training epoch 993, recon_loss:0.774289, zinb_loss:0.641304, cluster_loss:0.153184\n",
      "Clustering   993: ASW= 0.8218, DB= 0.2446, CH= 50638.3711\n",
      "Training epoch 994, recon_loss:0.774186, zinb_loss:0.641344, cluster_loss:0.153230\n",
      "Clustering   994: ASW= 0.8231, DB= 0.2416, CH= 50873.8986\n",
      "Training epoch 995, recon_loss:0.774071, zinb_loss:0.641112, cluster_loss:0.153066\n",
      "Clustering   995: ASW= 0.8220, DB= 0.2438, CH= 50775.4149\n",
      "Training epoch 996, recon_loss:0.773959, zinb_loss:0.641106, cluster_loss:0.153265\n",
      "Clustering   996: ASW= 0.8232, DB= 0.2424, CH= 50900.3564\n",
      "Training epoch 997, recon_loss:0.774089, zinb_loss:0.641002, cluster_loss:0.153042\n",
      "Clustering   997: ASW= 0.8221, DB= 0.2431, CH= 50854.4477\n",
      "Training epoch 998, recon_loss:0.774073, zinb_loss:0.640900, cluster_loss:0.153453\n",
      "Clustering   998: ASW= 0.8231, DB= 0.2430, CH= 50836.5761\n",
      "Training epoch 999, recon_loss:0.774590, zinb_loss:0.640967, cluster_loss:0.153166\n",
      "Clustering   999: ASW= 0.8222, DB= 0.2429, CH= 50888.1350\n",
      "Training epoch 1000, recon_loss:0.774464, zinb_loss:0.640694, cluster_loss:0.153752\n",
      "Clustering   1000: ASW= 0.8229, DB= 0.2437, CH= 50672.6236\n",
      "Training epoch 1001, recon_loss:0.775363, zinb_loss:0.640935, cluster_loss:0.153360\n",
      "Clustering   1001: ASW= 0.8221, DB= 0.2433, CH= 50857.1890\n",
      "Training epoch 1002, recon_loss:0.774808, zinb_loss:0.640473, cluster_loss:0.153986\n",
      "Clustering   1002: ASW= 0.8227, DB= 0.2444, CH= 50474.1399\n",
      "Training epoch 1003, recon_loss:0.775801, zinb_loss:0.640854, cluster_loss:0.153354\n",
      "Clustering   1003: ASW= 0.8223, DB= 0.2428, CH= 50842.6361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1004, recon_loss:0.774872, zinb_loss:0.640223, cluster_loss:0.154007\n",
      "Clustering   1004: ASW= 0.8225, DB= 0.2448, CH= 50373.5489\n",
      "Training epoch 1005, recon_loss:0.775838, zinb_loss:0.640723, cluster_loss:0.153199\n",
      "Clustering   1005: ASW= 0.8225, DB= 0.2425, CH= 50910.2397\n",
      "Training epoch 1006, recon_loss:0.774710, zinb_loss:0.640018, cluster_loss:0.153908\n",
      "Clustering   1006: ASW= 0.8223, DB= 0.2450, CH= 50386.1029\n",
      "Training epoch 1007, recon_loss:0.775670, zinb_loss:0.640588, cluster_loss:0.153072\n",
      "Clustering   1007: ASW= 0.8228, DB= 0.2423, CH= 50950.2659\n",
      "Training epoch 1008, recon_loss:0.774410, zinb_loss:0.639877, cluster_loss:0.153721\n",
      "Clustering   1008: ASW= 0.8224, DB= 0.2454, CH= 50523.0174\n",
      "Training epoch 1009, recon_loss:0.775360, zinb_loss:0.640477, cluster_loss:0.152959\n",
      "Clustering   1009: ASW= 0.8232, DB= 0.2420, CH= 51011.8464\n",
      "Training epoch 1010, recon_loss:0.774118, zinb_loss:0.639831, cluster_loss:0.153528\n",
      "Clustering   1010: ASW= 0.8224, DB= 0.2451, CH= 50669.9318\n",
      "Training epoch 1011, recon_loss:0.775077, zinb_loss:0.640449, cluster_loss:0.152924\n",
      "Clustering   1011: ASW= 0.8235, DB= 0.2414, CH= 51081.0035\n",
      "Training epoch 1012, recon_loss:0.774010, zinb_loss:0.639937, cluster_loss:0.153440\n",
      "Clustering   1012: ASW= 0.8224, DB= 0.2452, CH= 50823.8857\n",
      "Training epoch 1013, recon_loss:0.775031, zinb_loss:0.640563, cluster_loss:0.153040\n",
      "Clustering   1013: ASW= 0.8238, DB= 0.2414, CH= 51139.0150\n",
      "Training epoch 1014, recon_loss:0.774251, zinb_loss:0.640280, cluster_loss:0.153439\n",
      "Clustering   1014: ASW= 0.8225, DB= 0.2444, CH= 50925.8907\n",
      "Training epoch 1015, recon_loss:0.775307, zinb_loss:0.640821, cluster_loss:0.153284\n",
      "Clustering   1015: ASW= 0.8238, DB= 0.2414, CH= 51105.4937\n",
      "Training epoch 1016, recon_loss:0.774991, zinb_loss:0.640868, cluster_loss:0.153498\n",
      "Clustering   1016: ASW= 0.8227, DB= 0.2437, CH= 51133.6367\n",
      "Training epoch 1017, recon_loss:0.775943, zinb_loss:0.641317, cluster_loss:0.153719\n",
      "Clustering   1017: ASW= 0.8238, DB= 0.2419, CH= 51000.1370\n",
      "Training epoch 1018, recon_loss:0.775815, zinb_loss:0.641657, cluster_loss:0.153689\n",
      "Clustering   1018: ASW= 0.8227, DB= 0.2434, CH= 51217.3950\n",
      "Training epoch 1019, recon_loss:0.776512, zinb_loss:0.641917, cluster_loss:0.154245\n",
      "Clustering   1019: ASW= 0.8235, DB= 0.2427, CH= 50728.5421\n",
      "Training epoch 1020, recon_loss:0.776355, zinb_loss:0.642495, cluster_loss:0.153775\n",
      "Clustering   1020: ASW= 0.8229, DB= 0.2428, CH= 51365.9575\n",
      "Training epoch 1021, recon_loss:0.776758, zinb_loss:0.642362, cluster_loss:0.154420\n",
      "Clustering   1021: ASW= 0.8233, DB= 0.2429, CH= 50529.7505\n",
      "Training epoch 1022, recon_loss:0.776063, zinb_loss:0.642713, cluster_loss:0.153677\n",
      "Clustering   1022: ASW= 0.8229, DB= 0.2427, CH= 51345.4629\n",
      "Training epoch 1023, recon_loss:0.776250, zinb_loss:0.642265, cluster_loss:0.154222\n",
      "Clustering   1023: ASW= 0.8232, DB= 0.2432, CH= 50448.4959\n",
      "Training epoch 1024, recon_loss:0.775208, zinb_loss:0.642434, cluster_loss:0.153395\n",
      "Clustering   1024: ASW= 0.8230, DB= 0.2426, CH= 51360.8792\n",
      "Training epoch 1025, recon_loss:0.775333, zinb_loss:0.641856, cluster_loss:0.153865\n",
      "Clustering   1025: ASW= 0.8232, DB= 0.2433, CH= 50535.0558\n",
      "Training epoch 1026, recon_loss:0.774392, zinb_loss:0.642016, cluster_loss:0.153109\n",
      "Clustering   1026: ASW= 0.8231, DB= 0.2425, CH= 51379.5732\n",
      "Training epoch 1027, recon_loss:0.774608, zinb_loss:0.641495, cluster_loss:0.153566\n",
      "Clustering   1027: ASW= 0.8232, DB= 0.2432, CH= 50645.3112\n",
      "Training epoch 1028, recon_loss:0.773931, zinb_loss:0.641739, cluster_loss:0.152941\n",
      "Clustering   1028: ASW= 0.8231, DB= 0.2424, CH= 51406.4381\n",
      "Training epoch 1029, recon_loss:0.774152, zinb_loss:0.641274, cluster_loss:0.153398\n",
      "Clustering   1029: ASW= 0.8231, DB= 0.2435, CH= 50729.9048\n",
      "Training epoch 1030, recon_loss:0.773753, zinb_loss:0.641649, cluster_loss:0.152848\n",
      "Clustering   1030: ASW= 0.8234, DB= 0.2420, CH= 51450.8016\n",
      "Training epoch 1031, recon_loss:0.774161, zinb_loss:0.641222, cluster_loss:0.153375\n",
      "Clustering   1031: ASW= 0.8226, DB= 0.2429, CH= 50657.3865\n",
      "Training epoch 1032, recon_loss:0.773908, zinb_loss:0.641620, cluster_loss:0.152852\n",
      "Clustering   1032: ASW= 0.8233, DB= 0.2422, CH= 51445.9065\n",
      "Training epoch 1033, recon_loss:0.774363, zinb_loss:0.641182, cluster_loss:0.153409\n",
      "Clustering   1033: ASW= 0.8226, DB= 0.2441, CH= 50726.6793\n",
      "Training epoch 1034, recon_loss:0.774273, zinb_loss:0.641693, cluster_loss:0.152939\n",
      "Clustering   1034: ASW= 0.8237, DB= 0.2420, CH= 51445.7000\n",
      "Training epoch 1035, recon_loss:0.774909, zinb_loss:0.641179, cluster_loss:0.153614\n",
      "Clustering   1035: ASW= 0.8221, DB= 0.2441, CH= 50640.6332\n",
      "Training epoch 1036, recon_loss:0.774827, zinb_loss:0.641639, cluster_loss:0.153193\n",
      "Clustering   1036: ASW= 0.8237, DB= 0.2410, CH= 51328.6195\n",
      "Training epoch 1037, recon_loss:0.775599, zinb_loss:0.641076, cluster_loss:0.153754\n",
      "Clustering   1037: ASW= 0.8220, DB= 0.2447, CH= 50645.0992\n",
      "Training epoch 1038, recon_loss:0.775457, zinb_loss:0.641503, cluster_loss:0.153392\n",
      "Clustering   1038: ASW= 0.8238, DB= 0.2412, CH= 51317.1885\n",
      "Training epoch 1039, recon_loss:0.776150, zinb_loss:0.640891, cluster_loss:0.153924\n",
      "Clustering   1039: ASW= 0.8215, DB= 0.2444, CH= 50439.6038\n",
      "Training epoch 1040, recon_loss:0.775409, zinb_loss:0.641043, cluster_loss:0.153610\n",
      "Clustering   1040: ASW= 0.8236, DB= 0.2414, CH= 51142.4569\n",
      "Training epoch 1041, recon_loss:0.776085, zinb_loss:0.640597, cluster_loss:0.153765\n",
      "Clustering   1041: ASW= 0.8218, DB= 0.2442, CH= 50534.2116\n",
      "Training epoch 1042, recon_loss:0.775227, zinb_loss:0.640506, cluster_loss:0.153550\n",
      "Clustering   1042: ASW= 0.8234, DB= 0.2420, CH= 50946.9501\n",
      "Training epoch 1043, recon_loss:0.775270, zinb_loss:0.640090, cluster_loss:0.153450\n",
      "Clustering   1043: ASW= 0.8220, DB= 0.2449, CH= 50615.6787\n",
      "Training epoch 1044, recon_loss:0.775073, zinb_loss:0.640136, cluster_loss:0.153444\n",
      "Clustering   1044: ASW= 0.8236, DB= 0.2422, CH= 51139.0971\n",
      "Training epoch 1045, recon_loss:0.775178, zinb_loss:0.640066, cluster_loss:0.153327\n",
      "Clustering   1045: ASW= 0.8226, DB= 0.2437, CH= 50918.9657\n",
      "Training epoch 1046, recon_loss:0.775118, zinb_loss:0.639966, cluster_loss:0.153562\n",
      "Clustering   1046: ASW= 0.8234, DB= 0.2426, CH= 50914.6717\n",
      "Training epoch 1047, recon_loss:0.775040, zinb_loss:0.640179, cluster_loss:0.153307\n",
      "Clustering   1047: ASW= 0.8231, DB= 0.2437, CH= 50909.4443\n",
      "Training epoch 1048, recon_loss:0.774894, zinb_loss:0.640003, cluster_loss:0.153475\n",
      "Clustering   1048: ASW= 0.8235, DB= 0.2427, CH= 50978.8432\n",
      "Training epoch 1049, recon_loss:0.775082, zinb_loss:0.640514, cluster_loss:0.153313\n",
      "Clustering   1049: ASW= 0.8236, DB= 0.2428, CH= 50998.7662\n",
      "Training epoch 1050, recon_loss:0.774542, zinb_loss:0.640175, cluster_loss:0.153407\n",
      "Clustering   1050: ASW= 0.8226, DB= 0.2441, CH= 50527.4209\n",
      "Training epoch 1051, recon_loss:0.774207, zinb_loss:0.640311, cluster_loss:0.153117\n",
      "Clustering   1051: ASW= 0.8230, DB= 0.2427, CH= 50615.8578\n",
      "Training epoch 1052, recon_loss:0.774763, zinb_loss:0.640424, cluster_loss:0.153562\n",
      "Clustering   1052: ASW= 0.8227, DB= 0.2430, CH= 50726.0921\n",
      "Training epoch 1053, recon_loss:0.774962, zinb_loss:0.640882, cluster_loss:0.153375\n",
      "Clustering   1053: ASW= 0.8237, DB= 0.2428, CH= 50846.7273\n",
      "Training epoch 1054, recon_loss:0.774569, zinb_loss:0.640683, cluster_loss:0.153771\n",
      "Clustering   1054: ASW= 0.8223, DB= 0.2427, CH= 50592.4757\n",
      "Training epoch 1055, recon_loss:0.774593, zinb_loss:0.640904, cluster_loss:0.153288\n",
      "Clustering   1055: ASW= 0.8244, DB= 0.2420, CH= 51085.2438\n",
      "Training epoch 1056, recon_loss:0.774126, zinb_loss:0.640694, cluster_loss:0.153757\n",
      "Clustering   1056: ASW= 0.8219, DB= 0.2428, CH= 50617.1630\n",
      "Training epoch 1057, recon_loss:0.774238, zinb_loss:0.640874, cluster_loss:0.153069\n",
      "Clustering   1057: ASW= 0.8246, DB= 0.2416, CH= 51191.7610\n",
      "Training epoch 1058, recon_loss:0.773749, zinb_loss:0.640563, cluster_loss:0.153385\n",
      "Clustering   1058: ASW= 0.8220, DB= 0.2427, CH= 50809.5882\n",
      "Training epoch 1059, recon_loss:0.773707, zinb_loss:0.640579, cluster_loss:0.152936\n",
      "Clustering   1059: ASW= 0.8247, DB= 0.2408, CH= 51349.3776\n",
      "Training epoch 1060, recon_loss:0.773702, zinb_loss:0.640581, cluster_loss:0.153265\n",
      "Clustering   1060: ASW= 0.8221, DB= 0.2424, CH= 51002.1613\n",
      "Training epoch 1061, recon_loss:0.773891, zinb_loss:0.640587, cluster_loss:0.152985\n",
      "Clustering   1061: ASW= 0.8246, DB= 0.2414, CH= 51511.6095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1062, recon_loss:0.774019, zinb_loss:0.640683, cluster_loss:0.153377\n",
      "Clustering   1062: ASW= 0.8220, DB= 0.2417, CH= 50940.9325\n",
      "Training epoch 1063, recon_loss:0.774221, zinb_loss:0.640612, cluster_loss:0.153107\n",
      "Clustering   1063: ASW= 0.8240, DB= 0.2429, CH= 51431.4129\n",
      "Training epoch 1064, recon_loss:0.774509, zinb_loss:0.640865, cluster_loss:0.153341\n",
      "Clustering   1064: ASW= 0.8223, DB= 0.2406, CH= 51057.2812\n",
      "Training epoch 1065, recon_loss:0.774559, zinb_loss:0.640687, cluster_loss:0.153104\n",
      "Clustering   1065: ASW= 0.8239, DB= 0.2429, CH= 51534.6099\n",
      "Training epoch 1066, recon_loss:0.774634, zinb_loss:0.640854, cluster_loss:0.153398\n",
      "Clustering   1066: ASW= 0.8225, DB= 0.2410, CH= 50923.8774\n",
      "Training epoch 1067, recon_loss:0.774593, zinb_loss:0.640601, cluster_loss:0.153095\n",
      "Clustering   1067: ASW= 0.8235, DB= 0.2435, CH= 51388.3023\n",
      "Training epoch 1068, recon_loss:0.774729, zinb_loss:0.640764, cluster_loss:0.153330\n",
      "Clustering   1068: ASW= 0.8230, DB= 0.2399, CH= 51054.2692\n",
      "Training epoch 1069, recon_loss:0.774364, zinb_loss:0.640619, cluster_loss:0.152938\n",
      "Clustering   1069: ASW= 0.8239, DB= 0.2430, CH= 51659.7362\n",
      "Training epoch 1070, recon_loss:0.774401, zinb_loss:0.640605, cluster_loss:0.153222\n",
      "Clustering   1070: ASW= 0.8232, DB= 0.2402, CH= 51050.6337\n",
      "Training epoch 1071, recon_loss:0.774138, zinb_loss:0.640660, cluster_loss:0.152782\n",
      "Clustering   1071: ASW= 0.8243, DB= 0.2429, CH= 51846.1960\n",
      "Training epoch 1072, recon_loss:0.774420, zinb_loss:0.640596, cluster_loss:0.153217\n",
      "Clustering   1072: ASW= 0.8234, DB= 0.2400, CH= 51082.2987\n",
      "Training epoch 1073, recon_loss:0.774229, zinb_loss:0.640792, cluster_loss:0.152731\n",
      "Clustering   1073: ASW= 0.8245, DB= 0.2424, CH= 51937.3765\n",
      "Training epoch 1074, recon_loss:0.774712, zinb_loss:0.640671, cluster_loss:0.153346\n",
      "Clustering   1074: ASW= 0.8235, DB= 0.2402, CH= 51088.7055\n",
      "Training epoch 1075, recon_loss:0.774490, zinb_loss:0.640942, cluster_loss:0.152768\n",
      "Clustering   1075: ASW= 0.8243, DB= 0.2417, CH= 51883.9524\n",
      "Training epoch 1076, recon_loss:0.774980, zinb_loss:0.640764, cluster_loss:0.153382\n",
      "Clustering   1076: ASW= 0.8239, DB= 0.2400, CH= 51142.7324\n",
      "Training epoch 1077, recon_loss:0.774598, zinb_loss:0.640989, cluster_loss:0.152789\n",
      "Clustering   1077: ASW= 0.8243, DB= 0.2418, CH= 51869.1058\n",
      "Training epoch 1078, recon_loss:0.774833, zinb_loss:0.640711, cluster_loss:0.153492\n",
      "Clustering   1078: ASW= 0.8238, DB= 0.2404, CH= 51108.9723\n",
      "Training epoch 1079, recon_loss:0.774583, zinb_loss:0.640901, cluster_loss:0.152872\n",
      "Clustering   1079: ASW= 0.8239, DB= 0.2421, CH= 51719.8050\n",
      "Training epoch 1080, recon_loss:0.774981, zinb_loss:0.640761, cluster_loss:0.153492\n",
      "Clustering   1080: ASW= 0.8243, DB= 0.2398, CH= 51259.6815\n",
      "Training epoch 1081, recon_loss:0.774704, zinb_loss:0.640871, cluster_loss:0.152948\n",
      "Clustering   1081: ASW= 0.8236, DB= 0.2422, CH= 51605.4614\n",
      "Training epoch 1082, recon_loss:0.774981, zinb_loss:0.640852, cluster_loss:0.153420\n",
      "Clustering   1082: ASW= 0.8250, DB= 0.2394, CH= 51509.9498\n",
      "Training epoch 1083, recon_loss:0.774794, zinb_loss:0.640819, cluster_loss:0.153038\n",
      "Clustering   1083: ASW= 0.8233, DB= 0.2424, CH= 51507.7788\n",
      "Training epoch 1084, recon_loss:0.775162, zinb_loss:0.640915, cluster_loss:0.153370\n",
      "Clustering   1084: ASW= 0.8253, DB= 0.2391, CH= 51659.6416\n",
      "Training epoch 1085, recon_loss:0.775069, zinb_loss:0.640713, cluster_loss:0.153158\n",
      "Clustering   1085: ASW= 0.8231, DB= 0.2425, CH= 51503.0449\n",
      "Training epoch 1086, recon_loss:0.775363, zinb_loss:0.640967, cluster_loss:0.153366\n",
      "Clustering   1086: ASW= 0.8256, DB= 0.2386, CH= 51760.6702\n",
      "Training epoch 1087, recon_loss:0.775365, zinb_loss:0.640588, cluster_loss:0.153284\n",
      "Clustering   1087: ASW= 0.8228, DB= 0.2431, CH= 51419.6931\n",
      "Training epoch 1088, recon_loss:0.775667, zinb_loss:0.640988, cluster_loss:0.153335\n",
      "Clustering   1088: ASW= 0.8257, DB= 0.2390, CH= 51803.2571\n",
      "Training epoch 1089, recon_loss:0.775316, zinb_loss:0.640372, cluster_loss:0.153329\n",
      "Clustering   1089: ASW= 0.8227, DB= 0.2437, CH= 51351.8570\n",
      "Training epoch 1090, recon_loss:0.775607, zinb_loss:0.640937, cluster_loss:0.153221\n",
      "Clustering   1090: ASW= 0.8259, DB= 0.2383, CH= 51901.4069\n",
      "Training epoch 1091, recon_loss:0.774640, zinb_loss:0.640090, cluster_loss:0.153346\n",
      "Clustering   1091: ASW= 0.8224, DB= 0.2442, CH= 51165.4304\n",
      "Training epoch 1092, recon_loss:0.775006, zinb_loss:0.640767, cluster_loss:0.153085\n",
      "Clustering   1092: ASW= 0.8257, DB= 0.2388, CH= 51866.0391\n",
      "Training epoch 1093, recon_loss:0.774432, zinb_loss:0.639868, cluster_loss:0.153382\n",
      "Clustering   1093: ASW= 0.8223, DB= 0.2446, CH= 51167.3338\n",
      "Training epoch 1094, recon_loss:0.774701, zinb_loss:0.640543, cluster_loss:0.152940\n",
      "Clustering   1094: ASW= 0.8260, DB= 0.2385, CH= 52017.7733\n",
      "Training epoch 1095, recon_loss:0.773701, zinb_loss:0.639661, cluster_loss:0.153250\n",
      "Clustering   1095: ASW= 0.8223, DB= 0.2442, CH= 51244.8684\n",
      "Training epoch 1096, recon_loss:0.774248, zinb_loss:0.640361, cluster_loss:0.152779\n",
      "Clustering   1096: ASW= 0.8262, DB= 0.2385, CH= 52104.3727\n",
      "Training epoch 1097, recon_loss:0.773344, zinb_loss:0.639556, cluster_loss:0.153161\n",
      "Clustering   1097: ASW= 0.8227, DB= 0.2436, CH= 51361.3292\n",
      "Training epoch 1098, recon_loss:0.774044, zinb_loss:0.640206, cluster_loss:0.152743\n",
      "Clustering   1098: ASW= 0.8262, DB= 0.2386, CH= 52137.9762\n",
      "Training epoch 1099, recon_loss:0.773376, zinb_loss:0.639533, cluster_loss:0.153263\n",
      "Clustering   1099: ASW= 0.8227, DB= 0.2437, CH= 51428.4399\n",
      "Training epoch 1100, recon_loss:0.774526, zinb_loss:0.640167, cluster_loss:0.152879\n",
      "Clustering   1100: ASW= 0.8261, DB= 0.2388, CH= 52086.6075\n",
      "Training epoch 1101, recon_loss:0.773812, zinb_loss:0.639569, cluster_loss:0.153430\n",
      "Clustering   1101: ASW= 0.8227, DB= 0.2439, CH= 51406.2934\n",
      "Training epoch 1102, recon_loss:0.775028, zinb_loss:0.640084, cluster_loss:0.152991\n",
      "Clustering   1102: ASW= 0.8259, DB= 0.2381, CH= 51996.2657\n",
      "Training epoch 1103, recon_loss:0.774034, zinb_loss:0.639585, cluster_loss:0.153513\n",
      "Clustering   1103: ASW= 0.8227, DB= 0.2436, CH= 51429.1156\n",
      "Training epoch 1104, recon_loss:0.775141, zinb_loss:0.640104, cluster_loss:0.152937\n",
      "Clustering   1104: ASW= 0.8260, DB= 0.2381, CH= 52029.6968\n",
      "Training epoch 1105, recon_loss:0.774218, zinb_loss:0.639716, cluster_loss:0.153393\n",
      "Clustering   1105: ASW= 0.8231, DB= 0.2427, CH= 51449.5722\n",
      "Training epoch 1106, recon_loss:0.775279, zinb_loss:0.640196, cluster_loss:0.152881\n",
      "Clustering   1106: ASW= 0.8257, DB= 0.2385, CH= 52018.8848\n",
      "Training epoch 1107, recon_loss:0.774670, zinb_loss:0.639990, cluster_loss:0.153325\n",
      "Clustering   1107: ASW= 0.8236, DB= 0.2425, CH= 51643.5129\n",
      "Training epoch 1108, recon_loss:0.775963, zinb_loss:0.640750, cluster_loss:0.152927\n",
      "Clustering   1108: ASW= 0.8258, DB= 0.2386, CH= 52141.6058\n",
      "Training epoch 1109, recon_loss:0.775591, zinb_loss:0.640705, cluster_loss:0.153502\n",
      "Clustering   1109: ASW= 0.8236, DB= 0.2421, CH= 51470.9866\n",
      "Training epoch 1110, recon_loss:0.776680, zinb_loss:0.641447, cluster_loss:0.153176\n",
      "Clustering   1110: ASW= 0.8255, DB= 0.2391, CH= 52054.1761\n",
      "Training epoch 1111, recon_loss:0.776127, zinb_loss:0.641302, cluster_loss:0.153557\n",
      "Clustering   1111: ASW= 0.8240, DB= 0.2415, CH= 51653.9585\n",
      "Training epoch 1112, recon_loss:0.776176, zinb_loss:0.641567, cluster_loss:0.153269\n",
      "Clustering   1112: ASW= 0.8253, DB= 0.2394, CH= 52059.3186\n",
      "Training epoch 1113, recon_loss:0.775525, zinb_loss:0.641375, cluster_loss:0.153512\n",
      "Clustering   1113: ASW= 0.8240, DB= 0.2417, CH= 51611.5281\n",
      "Training epoch 1114, recon_loss:0.775458, zinb_loss:0.641543, cluster_loss:0.153236\n",
      "Clustering   1114: ASW= 0.8256, DB= 0.2394, CH= 52091.8983\n",
      "Training epoch 1115, recon_loss:0.775129, zinb_loss:0.641331, cluster_loss:0.153353\n",
      "Clustering   1115: ASW= 0.8242, DB= 0.2406, CH= 51698.4322\n",
      "Training epoch 1116, recon_loss:0.774855, zinb_loss:0.641166, cluster_loss:0.153240\n",
      "Clustering   1116: ASW= 0.8254, DB= 0.2398, CH= 52120.3927\n",
      "Training epoch 1117, recon_loss:0.775041, zinb_loss:0.641037, cluster_loss:0.153251\n",
      "Clustering   1117: ASW= 0.8242, DB= 0.2403, CH= 51823.8278\n",
      "Training epoch 1118, recon_loss:0.774369, zinb_loss:0.640735, cluster_loss:0.153305\n",
      "Clustering   1118: ASW= 0.8253, DB= 0.2402, CH= 52125.7374\n",
      "Training epoch 1119, recon_loss:0.774744, zinb_loss:0.640754, cluster_loss:0.153174\n",
      "Clustering   1119: ASW= 0.8242, DB= 0.2394, CH= 51918.3676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1120, recon_loss:0.773992, zinb_loss:0.640414, cluster_loss:0.153438\n",
      "Clustering   1120: ASW= 0.8254, DB= 0.2406, CH= 52084.5068\n",
      "Training epoch 1121, recon_loss:0.774311, zinb_loss:0.640638, cluster_loss:0.153123\n",
      "Clustering   1121: ASW= 0.8244, DB= 0.2392, CH= 51884.4798\n",
      "Training epoch 1122, recon_loss:0.773832, zinb_loss:0.640374, cluster_loss:0.153533\n",
      "Clustering   1122: ASW= 0.8250, DB= 0.2414, CH= 52085.3913\n",
      "Training epoch 1123, recon_loss:0.774137, zinb_loss:0.640543, cluster_loss:0.153118\n",
      "Clustering   1123: ASW= 0.8242, DB= 0.2389, CH= 51808.9576\n",
      "Training epoch 1124, recon_loss:0.773502, zinb_loss:0.640161, cluster_loss:0.153528\n",
      "Clustering   1124: ASW= 0.8249, DB= 0.2416, CH= 52095.1682\n",
      "Training epoch 1125, recon_loss:0.773787, zinb_loss:0.640369, cluster_loss:0.153102\n",
      "Clustering   1125: ASW= 0.8240, DB= 0.2388, CH= 51683.2277\n",
      "Training epoch 1126, recon_loss:0.773275, zinb_loss:0.639947, cluster_loss:0.153599\n",
      "Clustering   1126: ASW= 0.8247, DB= 0.2421, CH= 52004.1862\n",
      "Training epoch 1127, recon_loss:0.773484, zinb_loss:0.640274, cluster_loss:0.153079\n",
      "Clustering   1127: ASW= 0.8242, DB= 0.2387, CH= 51574.9759\n",
      "Training epoch 1128, recon_loss:0.773219, zinb_loss:0.639899, cluster_loss:0.153571\n",
      "Clustering   1128: ASW= 0.8243, DB= 0.2425, CH= 52028.4084\n",
      "Training epoch 1129, recon_loss:0.773344, zinb_loss:0.640147, cluster_loss:0.153046\n",
      "Clustering   1129: ASW= 0.8242, DB= 0.2386, CH= 51502.4272\n",
      "Training epoch 1130, recon_loss:0.773360, zinb_loss:0.639836, cluster_loss:0.153498\n",
      "Clustering   1130: ASW= 0.8244, DB= 0.2426, CH= 52084.3247\n",
      "Training epoch 1131, recon_loss:0.773414, zinb_loss:0.640045, cluster_loss:0.153024\n",
      "Clustering   1131: ASW= 0.8243, DB= 0.2387, CH= 51423.7873\n",
      "Training epoch 1132, recon_loss:0.773593, zinb_loss:0.639898, cluster_loss:0.153372\n",
      "Clustering   1132: ASW= 0.8245, DB= 0.2423, CH= 52180.8246\n",
      "Training epoch 1133, recon_loss:0.773553, zinb_loss:0.640044, cluster_loss:0.153000\n",
      "Clustering   1133: ASW= 0.8244, DB= 0.2390, CH= 51377.4795\n",
      "Training epoch 1134, recon_loss:0.773899, zinb_loss:0.640093, cluster_loss:0.153252\n",
      "Clustering   1134: ASW= 0.8246, DB= 0.2419, CH= 52253.0278\n",
      "Training epoch 1135, recon_loss:0.773864, zinb_loss:0.640183, cluster_loss:0.152996\n",
      "Clustering   1135: ASW= 0.8245, DB= 0.2399, CH= 51392.7580\n",
      "Training epoch 1136, recon_loss:0.774363, zinb_loss:0.640392, cluster_loss:0.153210\n",
      "Clustering   1136: ASW= 0.8245, DB= 0.2411, CH= 52216.9799\n",
      "Training epoch 1137, recon_loss:0.774300, zinb_loss:0.640426, cluster_loss:0.153007\n",
      "Clustering   1137: ASW= 0.8246, DB= 0.2404, CH= 51458.0421\n",
      "Training epoch 1138, recon_loss:0.774738, zinb_loss:0.640647, cluster_loss:0.153197\n",
      "Clustering   1138: ASW= 0.8244, DB= 0.2407, CH= 52109.0542\n",
      "Training epoch 1139, recon_loss:0.774380, zinb_loss:0.640560, cluster_loss:0.152934\n",
      "Clustering   1139: ASW= 0.8247, DB= 0.2405, CH= 51568.8514\n",
      "Training epoch 1140, recon_loss:0.774525, zinb_loss:0.640571, cluster_loss:0.153064\n",
      "Clustering   1140: ASW= 0.8242, DB= 0.2401, CH= 52082.4063\n",
      "Training epoch 1141, recon_loss:0.774184, zinb_loss:0.640517, cluster_loss:0.152812\n",
      "Clustering   1141: ASW= 0.8249, DB= 0.2405, CH= 51760.1324\n",
      "Training epoch 1142, recon_loss:0.774208, zinb_loss:0.640438, cluster_loss:0.152979\n",
      "Clustering   1142: ASW= 0.8241, DB= 0.2401, CH= 51950.2610\n",
      "Training epoch 1143, recon_loss:0.773986, zinb_loss:0.640333, cluster_loss:0.152676\n",
      "Clustering   1143: ASW= 0.8250, DB= 0.2396, CH= 51850.7547\n",
      "Training epoch 1144, recon_loss:0.774106, zinb_loss:0.640246, cluster_loss:0.152929\n",
      "Clustering   1144: ASW= 0.8241, DB= 0.2397, CH= 52004.6299\n",
      "Training epoch 1145, recon_loss:0.773952, zinb_loss:0.640201, cluster_loss:0.152657\n",
      "Clustering   1145: ASW= 0.8252, DB= 0.2389, CH= 52035.3136\n",
      "Training epoch 1146, recon_loss:0.774066, zinb_loss:0.640100, cluster_loss:0.152965\n",
      "Clustering   1146: ASW= 0.8240, DB= 0.2400, CH= 51909.4906\n",
      "Training epoch 1147, recon_loss:0.774043, zinb_loss:0.640134, cluster_loss:0.152655\n",
      "Clustering   1147: ASW= 0.8255, DB= 0.2389, CH= 52294.2629\n",
      "Training epoch 1148, recon_loss:0.774168, zinb_loss:0.640008, cluster_loss:0.153148\n",
      "Clustering   1148: ASW= 0.8239, DB= 0.2396, CH= 51718.4139\n",
      "Training epoch 1149, recon_loss:0.773959, zinb_loss:0.640101, cluster_loss:0.152732\n",
      "Clustering   1149: ASW= 0.8256, DB= 0.2392, CH= 52487.7063\n",
      "Training epoch 1150, recon_loss:0.774141, zinb_loss:0.639915, cluster_loss:0.153341\n",
      "Clustering   1150: ASW= 0.8238, DB= 0.2397, CH= 51595.1088\n",
      "Training epoch 1151, recon_loss:0.773750, zinb_loss:0.640059, cluster_loss:0.152834\n",
      "Clustering   1151: ASW= 0.8257, DB= 0.2399, CH= 52606.6785\n",
      "Training epoch 1152, recon_loss:0.773956, zinb_loss:0.639880, cluster_loss:0.153417\n",
      "Clustering   1152: ASW= 0.8241, DB= 0.2394, CH= 51578.1339\n",
      "Training epoch 1153, recon_loss:0.773477, zinb_loss:0.640064, cluster_loss:0.152847\n",
      "Clustering   1153: ASW= 0.8259, DB= 0.2403, CH= 52772.0409\n",
      "Training epoch 1154, recon_loss:0.773696, zinb_loss:0.639894, cluster_loss:0.153479\n",
      "Clustering   1154: ASW= 0.8243, DB= 0.2390, CH= 51520.3428\n",
      "Training epoch 1155, recon_loss:0.773269, zinb_loss:0.640124, cluster_loss:0.152838\n",
      "Clustering   1155: ASW= 0.8260, DB= 0.2404, CH= 52870.6729\n",
      "Training epoch 1156, recon_loss:0.773541, zinb_loss:0.639966, cluster_loss:0.153552\n",
      "Clustering   1156: ASW= 0.8244, DB= 0.2389, CH= 51449.2789\n",
      "Training epoch 1157, recon_loss:0.773215, zinb_loss:0.640260, cluster_loss:0.152786\n",
      "Clustering   1157: ASW= 0.8261, DB= 0.2405, CH= 53005.3532\n",
      "Training epoch 1158, recon_loss:0.773522, zinb_loss:0.640055, cluster_loss:0.153602\n",
      "Clustering   1158: ASW= 0.8245, DB= 0.2390, CH= 51399.6881\n",
      "Training epoch 1159, recon_loss:0.773254, zinb_loss:0.640402, cluster_loss:0.152654\n",
      "Clustering   1159: ASW= 0.8263, DB= 0.2401, CH= 53121.8767\n",
      "Training epoch 1160, recon_loss:0.773560, zinb_loss:0.640101, cluster_loss:0.153592\n",
      "Clustering   1160: ASW= 0.8247, DB= 0.2388, CH= 51385.4219\n",
      "Training epoch 1161, recon_loss:0.773378, zinb_loss:0.640503, cluster_loss:0.152484\n",
      "Clustering   1161: ASW= 0.8264, DB= 0.2401, CH= 53180.5796\n",
      "Training epoch 1162, recon_loss:0.773661, zinb_loss:0.640116, cluster_loss:0.153474\n",
      "Clustering   1162: ASW= 0.8250, DB= 0.2387, CH= 51484.4627\n",
      "Training epoch 1163, recon_loss:0.773558, zinb_loss:0.640579, cluster_loss:0.152345\n",
      "Clustering   1163: ASW= 0.8263, DB= 0.2398, CH= 53178.2999\n",
      "Training epoch 1164, recon_loss:0.774011, zinb_loss:0.640125, cluster_loss:0.153435\n",
      "Clustering   1164: ASW= 0.8251, DB= 0.2390, CH= 51575.4033\n",
      "Training epoch 1165, recon_loss:0.773865, zinb_loss:0.640670, cluster_loss:0.152360\n",
      "Clustering   1165: ASW= 0.8261, DB= 0.2396, CH= 53002.7730\n",
      "Training epoch 1166, recon_loss:0.774536, zinb_loss:0.640079, cluster_loss:0.153543\n",
      "Clustering   1166: ASW= 0.8251, DB= 0.2389, CH= 51588.9712\n",
      "Training epoch 1167, recon_loss:0.774139, zinb_loss:0.640731, cluster_loss:0.152510\n",
      "Clustering   1167: ASW= 0.8259, DB= 0.2391, CH= 52780.4065\n",
      "Training epoch 1168, recon_loss:0.774274, zinb_loss:0.639958, cluster_loss:0.153537\n",
      "Clustering   1168: ASW= 0.8251, DB= 0.2396, CH= 51602.0208\n",
      "Training epoch 1169, recon_loss:0.773977, zinb_loss:0.640550, cluster_loss:0.152580\n",
      "Clustering   1169: ASW= 0.8257, DB= 0.2396, CH= 52601.1868\n",
      "Training epoch 1170, recon_loss:0.773890, zinb_loss:0.639770, cluster_loss:0.153393\n",
      "Clustering   1170: ASW= 0.8251, DB= 0.2396, CH= 51761.1263\n",
      "Training epoch 1171, recon_loss:0.773642, zinb_loss:0.640298, cluster_loss:0.152523\n",
      "Clustering   1171: ASW= 0.8256, DB= 0.2389, CH= 52577.2820\n",
      "Training epoch 1172, recon_loss:0.773540, zinb_loss:0.639665, cluster_loss:0.153175\n",
      "Clustering   1172: ASW= 0.8254, DB= 0.2397, CH= 51933.4751\n",
      "Training epoch 1173, recon_loss:0.773283, zinb_loss:0.640143, cluster_loss:0.152420\n",
      "Clustering   1173: ASW= 0.8257, DB= 0.2384, CH= 52632.3763\n",
      "Training epoch 1174, recon_loss:0.773275, zinb_loss:0.639574, cluster_loss:0.153075\n",
      "Clustering   1174: ASW= 0.8253, DB= 0.2399, CH= 51936.4637\n",
      "Training epoch 1175, recon_loss:0.773104, zinb_loss:0.640052, cluster_loss:0.152398\n",
      "Clustering   1175: ASW= 0.8256, DB= 0.2382, CH= 52619.7743\n",
      "Training epoch 1176, recon_loss:0.773389, zinb_loss:0.639558, cluster_loss:0.152984\n",
      "Clustering   1176: ASW= 0.8253, DB= 0.2403, CH= 52035.9552\n",
      "Training epoch 1177, recon_loss:0.773411, zinb_loss:0.640104, cluster_loss:0.152410\n",
      "Clustering   1177: ASW= 0.8256, DB= 0.2379, CH= 52658.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1178, recon_loss:0.773879, zinb_loss:0.639723, cluster_loss:0.153041\n",
      "Clustering   1178: ASW= 0.8250, DB= 0.2410, CH= 51940.5408\n",
      "Training epoch 1179, recon_loss:0.773628, zinb_loss:0.640301, cluster_loss:0.152513\n",
      "Clustering   1179: ASW= 0.8255, DB= 0.2384, CH= 52565.1082\n",
      "Training epoch 1180, recon_loss:0.773520, zinb_loss:0.639959, cluster_loss:0.152949\n",
      "Clustering   1180: ASW= 0.8254, DB= 0.2406, CH= 52130.1792\n",
      "Training epoch 1181, recon_loss:0.773253, zinb_loss:0.640443, cluster_loss:0.152514\n",
      "Clustering   1181: ASW= 0.8257, DB= 0.2374, CH= 52714.9296\n",
      "Training epoch 1182, recon_loss:0.773152, zinb_loss:0.640021, cluster_loss:0.152951\n",
      "Clustering   1182: ASW= 0.8251, DB= 0.2408, CH= 52030.9084\n",
      "Training epoch 1183, recon_loss:0.772849, zinb_loss:0.640276, cluster_loss:0.152601\n",
      "Clustering   1183: ASW= 0.8253, DB= 0.2377, CH= 52599.8909\n",
      "Training epoch 1184, recon_loss:0.772800, zinb_loss:0.640041, cluster_loss:0.152884\n",
      "Clustering   1184: ASW= 0.8254, DB= 0.2405, CH= 52247.6021\n",
      "Training epoch 1185, recon_loss:0.772670, zinb_loss:0.640430, cluster_loss:0.152589\n",
      "Clustering   1185: ASW= 0.8257, DB= 0.2379, CH= 52674.3102\n",
      "Training epoch 1186, recon_loss:0.772657, zinb_loss:0.640146, cluster_loss:0.152762\n",
      "Clustering   1186: ASW= 0.8258, DB= 0.2407, CH= 52360.1836\n",
      "Training epoch 1187, recon_loss:0.772596, zinb_loss:0.640497, cluster_loss:0.152742\n",
      "Clustering   1187: ASW= 0.8256, DB= 0.2375, CH= 52806.4751\n",
      "Training epoch 1188, recon_loss:0.772822, zinb_loss:0.640424, cluster_loss:0.152858\n",
      "Clustering   1188: ASW= 0.8260, DB= 0.2405, CH= 52340.2728\n",
      "Training epoch 1189, recon_loss:0.773038, zinb_loss:0.640818, cluster_loss:0.153066\n",
      "Clustering   1189: ASW= 0.8253, DB= 0.2378, CH= 52832.2911\n",
      "Training epoch 1190, recon_loss:0.773270, zinb_loss:0.640767, cluster_loss:0.153074\n",
      "Clustering   1190: ASW= 0.8260, DB= 0.2395, CH= 52142.9966\n",
      "Training epoch 1191, recon_loss:0.773345, zinb_loss:0.641056, cluster_loss:0.153225\n",
      "Clustering   1191: ASW= 0.8252, DB= 0.2380, CH= 52942.3564\n",
      "Training epoch 1192, recon_loss:0.773540, zinb_loss:0.640889, cluster_loss:0.153168\n",
      "Clustering   1192: ASW= 0.8259, DB= 0.2391, CH= 51871.2018\n",
      "Training epoch 1193, recon_loss:0.773318, zinb_loss:0.641006, cluster_loss:0.153224\n",
      "Clustering   1193: ASW= 0.8251, DB= 0.2385, CH= 52875.4583\n",
      "Training epoch 1194, recon_loss:0.773589, zinb_loss:0.640727, cluster_loss:0.153052\n",
      "Clustering   1194: ASW= 0.8258, DB= 0.2389, CH= 51822.6577\n",
      "Training epoch 1195, recon_loss:0.772884, zinb_loss:0.640557, cluster_loss:0.152927\n",
      "Clustering   1195: ASW= 0.8252, DB= 0.2387, CH= 52918.3441\n",
      "Training epoch 1196, recon_loss:0.773222, zinb_loss:0.640231, cluster_loss:0.152723\n",
      "Clustering   1196: ASW= 0.8258, DB= 0.2387, CH= 51940.3220\n",
      "Training epoch 1197, recon_loss:0.772581, zinb_loss:0.640128, cluster_loss:0.152647\n",
      "Clustering   1197: ASW= 0.8255, DB= 0.2387, CH= 53020.6417\n",
      "Training epoch 1198, recon_loss:0.772796, zinb_loss:0.639813, cluster_loss:0.152464\n",
      "Clustering   1198: ASW= 0.8260, DB= 0.2387, CH= 52148.0125\n",
      "Training epoch 1199, recon_loss:0.772579, zinb_loss:0.639864, cluster_loss:0.152505\n",
      "Clustering   1199: ASW= 0.8257, DB= 0.2385, CH= 53060.7496\n",
      "Training epoch 1200, recon_loss:0.772887, zinb_loss:0.639630, cluster_loss:0.152383\n",
      "Clustering   1200: ASW= 0.8261, DB= 0.2386, CH= 52342.0570\n",
      "Training epoch 1201, recon_loss:0.772740, zinb_loss:0.639710, cluster_loss:0.152498\n",
      "Clustering   1201: ASW= 0.8258, DB= 0.2384, CH= 53054.3929\n",
      "Training epoch 1202, recon_loss:0.773177, zinb_loss:0.639517, cluster_loss:0.152350\n",
      "Clustering   1202: ASW= 0.8264, DB= 0.2383, CH= 52525.6678\n",
      "Training epoch 1203, recon_loss:0.773056, zinb_loss:0.639618, cluster_loss:0.152557\n",
      "Clustering   1203: ASW= 0.8257, DB= 0.2387, CH= 52997.4996\n",
      "Training epoch 1204, recon_loss:0.773416, zinb_loss:0.639457, cluster_loss:0.152367\n",
      "Clustering   1204: ASW= 0.8266, DB= 0.2382, CH= 52673.7909\n",
      "Training epoch 1205, recon_loss:0.773211, zinb_loss:0.639555, cluster_loss:0.152562\n",
      "Clustering   1205: ASW= 0.8259, DB= 0.2383, CH= 53015.3574\n",
      "Training epoch 1206, recon_loss:0.773785, zinb_loss:0.639489, cluster_loss:0.152404\n",
      "Clustering   1206: ASW= 0.8268, DB= 0.2379, CH= 52876.1055\n",
      "Training epoch 1207, recon_loss:0.773613, zinb_loss:0.639596, cluster_loss:0.152666\n",
      "Clustering   1207: ASW= 0.8259, DB= 0.2384, CH= 52886.9848\n",
      "Training epoch 1208, recon_loss:0.774213, zinb_loss:0.639492, cluster_loss:0.152541\n",
      "Clustering   1208: ASW= 0.8268, DB= 0.2380, CH= 52925.3192\n",
      "Training epoch 1209, recon_loss:0.773710, zinb_loss:0.639605, cluster_loss:0.152738\n",
      "Clustering   1209: ASW= 0.8263, DB= 0.2375, CH= 52946.4231\n",
      "Training epoch 1210, recon_loss:0.774354, zinb_loss:0.639542, cluster_loss:0.152632\n",
      "Clustering   1210: ASW= 0.8271, DB= 0.2377, CH= 53189.4076\n",
      "Training epoch 1211, recon_loss:0.773974, zinb_loss:0.639543, cluster_loss:0.152907\n",
      "Clustering   1211: ASW= 0.8257, DB= 0.2383, CH= 52588.4641\n",
      "Training epoch 1212, recon_loss:0.774335, zinb_loss:0.639426, cluster_loss:0.152831\n",
      "Clustering   1212: ASW= 0.8268, DB= 0.2385, CH= 53142.3723\n",
      "Training epoch 1213, recon_loss:0.773912, zinb_loss:0.639539, cluster_loss:0.153169\n",
      "Clustering   1213: ASW= 0.8261, DB= 0.2377, CH= 52659.0462\n",
      "Training epoch 1214, recon_loss:0.773850, zinb_loss:0.639419, cluster_loss:0.152867\n",
      "Clustering   1214: ASW= 0.8271, DB= 0.2379, CH= 53315.1462\n",
      "Training epoch 1215, recon_loss:0.773142, zinb_loss:0.639278, cluster_loss:0.153091\n",
      "Clustering   1215: ASW= 0.8257, DB= 0.2380, CH= 52534.3408\n",
      "Training epoch 1216, recon_loss:0.773420, zinb_loss:0.639421, cluster_loss:0.152793\n",
      "Clustering   1216: ASW= 0.8273, DB= 0.2376, CH= 53459.8552\n",
      "Training epoch 1217, recon_loss:0.773014, zinb_loss:0.639235, cluster_loss:0.153052\n",
      "Clustering   1217: ASW= 0.8252, DB= 0.2386, CH= 52374.6546\n",
      "Training epoch 1218, recon_loss:0.773343, zinb_loss:0.639507, cluster_loss:0.152872\n",
      "Clustering   1218: ASW= 0.8266, DB= 0.2381, CH= 53149.4360\n",
      "Training epoch 1219, recon_loss:0.773039, zinb_loss:0.639406, cluster_loss:0.153256\n",
      "Clustering   1219: ASW= 0.8252, DB= 0.2390, CH= 52374.6408\n",
      "Training epoch 1220, recon_loss:0.773708, zinb_loss:0.639991, cluster_loss:0.152914\n",
      "Clustering   1220: ASW= 0.8268, DB= 0.2375, CH= 53144.2294\n",
      "Training epoch 1221, recon_loss:0.773598, zinb_loss:0.639668, cluster_loss:0.153368\n",
      "Clustering   1221: ASW= 0.8246, DB= 0.2400, CH= 52233.1000\n",
      "Training epoch 1222, recon_loss:0.774698, zinb_loss:0.640606, cluster_loss:0.153103\n",
      "Clustering   1222: ASW= 0.8270, DB= 0.2374, CH= 53012.3100\n",
      "Training epoch 1223, recon_loss:0.774425, zinb_loss:0.639995, cluster_loss:0.153472\n",
      "Clustering   1223: ASW= 0.8243, DB= 0.2407, CH= 52064.4050\n",
      "Training epoch 1224, recon_loss:0.774816, zinb_loss:0.640711, cluster_loss:0.153293\n",
      "Clustering   1224: ASW= 0.8261, DB= 0.2378, CH= 52477.4685\n",
      "Training epoch 1225, recon_loss:0.774592, zinb_loss:0.639950, cluster_loss:0.153763\n",
      "Clustering   1225: ASW= 0.8242, DB= 0.2409, CH= 51952.6897\n",
      "Training epoch 1226, recon_loss:0.774813, zinb_loss:0.640743, cluster_loss:0.153189\n",
      "Clustering   1226: ASW= 0.8271, DB= 0.2374, CH= 52623.0317\n",
      "Training epoch 1227, recon_loss:0.773425, zinb_loss:0.639822, cluster_loss:0.153026\n",
      "Clustering   1227: ASW= 0.8253, DB= 0.2399, CH= 52455.9489\n",
      "Training epoch 1228, recon_loss:0.773228, zinb_loss:0.640115, cluster_loss:0.152722\n",
      "Clustering   1228: ASW= 0.8274, DB= 0.2365, CH= 52887.0687\n",
      "Training epoch 1229, recon_loss:0.772639, zinb_loss:0.639721, cluster_loss:0.152655\n",
      "Clustering   1229: ASW= 0.8258, DB= 0.2395, CH= 52896.5378\n",
      "Training epoch 1230, recon_loss:0.772949, zinb_loss:0.639846, cluster_loss:0.152633\n",
      "Clustering   1230: ASW= 0.8274, DB= 0.2373, CH= 52772.0216\n",
      "Training epoch 1231, recon_loss:0.772962, zinb_loss:0.639751, cluster_loss:0.152544\n",
      "Clustering   1231: ASW= 0.8263, DB= 0.2389, CH= 53189.2710\n",
      "Training epoch 1232, recon_loss:0.773304, zinb_loss:0.639794, cluster_loss:0.152812\n",
      "Clustering   1232: ASW= 0.8274, DB= 0.2367, CH= 52749.8823\n",
      "Training epoch 1233, recon_loss:0.773529, zinb_loss:0.639899, cluster_loss:0.152616\n",
      "Clustering   1233: ASW= 0.8262, DB= 0.2391, CH= 53288.3904\n",
      "Training epoch 1234, recon_loss:0.773822, zinb_loss:0.639714, cluster_loss:0.153080\n",
      "Clustering   1234: ASW= 0.8273, DB= 0.2370, CH= 52651.1027\n",
      "Training epoch 1235, recon_loss:0.773946, zinb_loss:0.640035, cluster_loss:0.152630\n",
      "Clustering   1235: ASW= 0.8263, DB= 0.2388, CH= 53336.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1236, recon_loss:0.773905, zinb_loss:0.639551, cluster_loss:0.153211\n",
      "Clustering   1236: ASW= 0.8267, DB= 0.2373, CH= 52404.9179\n",
      "Training epoch 1237, recon_loss:0.773953, zinb_loss:0.639922, cluster_loss:0.152549\n",
      "Clustering   1237: ASW= 0.8265, DB= 0.2382, CH= 53397.7540\n",
      "Training epoch 1238, recon_loss:0.773323, zinb_loss:0.639355, cluster_loss:0.152972\n",
      "Clustering   1238: ASW= 0.8270, DB= 0.2371, CH= 52532.7435\n",
      "Training epoch 1239, recon_loss:0.774101, zinb_loss:0.639987, cluster_loss:0.152722\n",
      "Clustering   1239: ASW= 0.8255, DB= 0.2386, CH= 53064.5111\n",
      "Training epoch 1240, recon_loss:0.774628, zinb_loss:0.640008, cluster_loss:0.153295\n",
      "Clustering   1240: ASW= 0.8260, DB= 0.2395, CH= 51811.1712\n",
      "Training epoch 1241, recon_loss:0.773324, zinb_loss:0.639445, cluster_loss:0.152822\n",
      "Clustering   1241: ASW= 0.8242, DB= 0.2426, CH= 52168.5521\n",
      "Training epoch 1242, recon_loss:0.772676, zinb_loss:0.638777, cluster_loss:0.153262\n",
      "Clustering   1242: ASW= 0.8255, DB= 0.2388, CH= 51957.5249\n",
      "Training epoch 1243, recon_loss:0.772486, zinb_loss:0.639272, cluster_loss:0.152500\n",
      "Clustering   1243: ASW= 0.8257, DB= 0.2394, CH= 52653.6044\n",
      "Training epoch 1244, recon_loss:0.773146, zinb_loss:0.638788, cluster_loss:0.153501\n",
      "Clustering   1244: ASW= 0.8249, DB= 0.2392, CH= 52034.6760\n",
      "Training epoch 1245, recon_loss:0.773426, zinb_loss:0.639563, cluster_loss:0.152544\n",
      "Clustering   1245: ASW= 0.8260, DB= 0.2381, CH= 52526.4345\n",
      "Training epoch 1246, recon_loss:0.773649, zinb_loss:0.639096, cluster_loss:0.153369\n",
      "Clustering   1246: ASW= 0.8255, DB= 0.2398, CH= 52351.5216\n",
      "Training epoch 1247, recon_loss:0.773961, zinb_loss:0.639814, cluster_loss:0.152561\n",
      "Clustering   1247: ASW= 0.8262, DB= 0.2375, CH= 52407.5617\n",
      "Training epoch 1248, recon_loss:0.774013, zinb_loss:0.639358, cluster_loss:0.153368\n",
      "Clustering   1248: ASW= 0.8260, DB= 0.2397, CH= 52689.2571\n",
      "Training epoch 1249, recon_loss:0.773997, zinb_loss:0.639811, cluster_loss:0.152558\n",
      "Clustering   1249: ASW= 0.8264, DB= 0.2375, CH= 52461.2553\n",
      "Training epoch 1250, recon_loss:0.773667, zinb_loss:0.639438, cluster_loss:0.153083\n",
      "Clustering   1250: ASW= 0.8266, DB= 0.2388, CH= 53076.7576\n",
      "Training epoch 1251, recon_loss:0.773416, zinb_loss:0.639648, cluster_loss:0.152423\n",
      "Clustering   1251: ASW= 0.8265, DB= 0.2374, CH= 52582.8023\n",
      "Training epoch 1252, recon_loss:0.772978, zinb_loss:0.639434, cluster_loss:0.152670\n",
      "Clustering   1252: ASW= 0.8272, DB= 0.2376, CH= 53417.0460\n",
      "Training epoch 1253, recon_loss:0.772728, zinb_loss:0.639524, cluster_loss:0.152293\n",
      "Clustering   1253: ASW= 0.8266, DB= 0.2367, CH= 52727.7179\n",
      "Training epoch 1254, recon_loss:0.772517, zinb_loss:0.639570, cluster_loss:0.152370\n",
      "Clustering   1254: ASW= 0.8277, DB= 0.2370, CH= 53702.9134\n",
      "Training epoch 1255, recon_loss:0.772389, zinb_loss:0.639589, cluster_loss:0.152299\n",
      "Clustering   1255: ASW= 0.8264, DB= 0.2372, CH= 52805.5919\n",
      "Training epoch 1256, recon_loss:0.772415, zinb_loss:0.639931, cluster_loss:0.152212\n",
      "Clustering   1256: ASW= 0.8281, DB= 0.2368, CH= 53937.5637\n",
      "Training epoch 1257, recon_loss:0.772301, zinb_loss:0.639809, cluster_loss:0.152425\n",
      "Clustering   1257: ASW= 0.8261, DB= 0.2379, CH= 52791.8541\n",
      "Training epoch 1258, recon_loss:0.772394, zinb_loss:0.640348, cluster_loss:0.152157\n",
      "Clustering   1258: ASW= 0.8286, DB= 0.2360, CH= 54090.0269\n",
      "Training epoch 1259, recon_loss:0.772317, zinb_loss:0.640022, cluster_loss:0.152675\n",
      "Clustering   1259: ASW= 0.8257, DB= 0.2382, CH= 52719.9984\n",
      "Training epoch 1260, recon_loss:0.772589, zinb_loss:0.640748, cluster_loss:0.152245\n",
      "Clustering   1260: ASW= 0.8289, DB= 0.2354, CH= 54110.5618\n",
      "Training epoch 1261, recon_loss:0.772501, zinb_loss:0.640235, cluster_loss:0.152989\n",
      "Clustering   1261: ASW= 0.8253, DB= 0.2388, CH= 52503.8430\n",
      "Training epoch 1262, recon_loss:0.772872, zinb_loss:0.641080, cluster_loss:0.152337\n",
      "Clustering   1262: ASW= 0.8289, DB= 0.2358, CH= 54088.8497\n",
      "Training epoch 1263, recon_loss:0.772502, zinb_loss:0.640396, cluster_loss:0.152952\n",
      "Clustering   1263: ASW= 0.8255, DB= 0.2386, CH= 52562.5212\n",
      "Training epoch 1264, recon_loss:0.773030, zinb_loss:0.641141, cluster_loss:0.152329\n",
      "Clustering   1264: ASW= 0.8287, DB= 0.2359, CH= 54056.4323\n",
      "Training epoch 1265, recon_loss:0.772856, zinb_loss:0.640495, cluster_loss:0.152845\n",
      "Clustering   1265: ASW= 0.8258, DB= 0.2384, CH= 52725.7372\n",
      "Training epoch 1266, recon_loss:0.773502, zinb_loss:0.641165, cluster_loss:0.152383\n",
      "Clustering   1266: ASW= 0.8284, DB= 0.2359, CH= 53995.8184\n",
      "Training epoch 1267, recon_loss:0.773494, zinb_loss:0.640582, cluster_loss:0.152812\n",
      "Clustering   1267: ASW= 0.8262, DB= 0.2380, CH= 52915.8851\n",
      "Training epoch 1268, recon_loss:0.773869, zinb_loss:0.641102, cluster_loss:0.152513\n",
      "Clustering   1268: ASW= 0.8280, DB= 0.2355, CH= 53882.6223\n",
      "Training epoch 1269, recon_loss:0.773748, zinb_loss:0.640538, cluster_loss:0.152821\n",
      "Clustering   1269: ASW= 0.8263, DB= 0.2384, CH= 53082.1690\n",
      "Training epoch 1270, recon_loss:0.773672, zinb_loss:0.640943, cluster_loss:0.152649\n",
      "Clustering   1270: ASW= 0.8277, DB= 0.2355, CH= 53744.7216\n",
      "Training epoch 1271, recon_loss:0.773310, zinb_loss:0.640458, cluster_loss:0.152809\n",
      "Clustering   1271: ASW= 0.8265, DB= 0.2389, CH= 53222.3117\n",
      "Training epoch 1272, recon_loss:0.773422, zinb_loss:0.640722, cluster_loss:0.152884\n",
      "Clustering   1272: ASW= 0.8273, DB= 0.2350, CH= 53528.4201\n",
      "Training epoch 1273, recon_loss:0.773282, zinb_loss:0.640418, cluster_loss:0.152903\n",
      "Clustering   1273: ASW= 0.8267, DB= 0.2388, CH= 53301.7142\n",
      "Training epoch 1274, recon_loss:0.773491, zinb_loss:0.640629, cluster_loss:0.153232\n",
      "Clustering   1274: ASW= 0.8269, DB= 0.2352, CH= 53184.5450\n",
      "Training epoch 1275, recon_loss:0.773307, zinb_loss:0.640555, cluster_loss:0.152861\n",
      "Clustering   1275: ASW= 0.8270, DB= 0.2387, CH= 53387.3448\n",
      "Training epoch 1276, recon_loss:0.773480, zinb_loss:0.640504, cluster_loss:0.153292\n",
      "Clustering   1276: ASW= 0.8270, DB= 0.2355, CH= 53058.0262\n",
      "Training epoch 1277, recon_loss:0.773318, zinb_loss:0.640609, cluster_loss:0.152715\n",
      "Clustering   1277: ASW= 0.8273, DB= 0.2378, CH= 53474.8746\n",
      "Training epoch 1278, recon_loss:0.772959, zinb_loss:0.640230, cluster_loss:0.153171\n",
      "Clustering   1278: ASW= 0.8267, DB= 0.2362, CH= 52878.1257\n",
      "Training epoch 1279, recon_loss:0.772881, zinb_loss:0.640450, cluster_loss:0.152897\n",
      "Clustering   1279: ASW= 0.8264, DB= 0.2396, CH= 52774.7447\n",
      "Training epoch 1280, recon_loss:0.773896, zinb_loss:0.640306, cluster_loss:0.154097\n",
      "Clustering   1280: ASW= 0.8257, DB= 0.2374, CH= 52421.0435\n",
      "Training epoch 1281, recon_loss:0.773564, zinb_loss:0.640496, cluster_loss:0.153063\n",
      "Clustering   1281: ASW= 0.8269, DB= 0.2391, CH= 52717.4841\n",
      "Training epoch 1282, recon_loss:0.772661, zinb_loss:0.639780, cluster_loss:0.153154\n",
      "Clustering   1282: ASW= 0.8268, DB= 0.2368, CH= 52809.8158\n",
      "Training epoch 1283, recon_loss:0.772273, zinb_loss:0.639690, cluster_loss:0.152395\n",
      "Clustering   1283: ASW= 0.8272, DB= 0.2377, CH= 53055.1454\n",
      "Training epoch 1284, recon_loss:0.772092, zinb_loss:0.639456, cluster_loss:0.152493\n",
      "Clustering   1284: ASW= 0.8273, DB= 0.2363, CH= 53371.9518\n",
      "Training epoch 1285, recon_loss:0.771819, zinb_loss:0.639249, cluster_loss:0.152105\n",
      "Clustering   1285: ASW= 0.8274, DB= 0.2370, CH= 53229.8145\n",
      "Training epoch 1286, recon_loss:0.772043, zinb_loss:0.639325, cluster_loss:0.152227\n",
      "Clustering   1286: ASW= 0.8276, DB= 0.2359, CH= 53696.5370\n",
      "Training epoch 1287, recon_loss:0.771912, zinb_loss:0.639081, cluster_loss:0.152052\n",
      "Clustering   1287: ASW= 0.8274, DB= 0.2373, CH= 53339.9386\n",
      "Training epoch 1288, recon_loss:0.772246, zinb_loss:0.639355, cluster_loss:0.152115\n",
      "Clustering   1288: ASW= 0.8278, DB= 0.2356, CH= 53926.4121\n",
      "Training epoch 1289, recon_loss:0.772178, zinb_loss:0.639042, cluster_loss:0.152085\n",
      "Clustering   1289: ASW= 0.8273, DB= 0.2373, CH= 53364.1451\n",
      "Training epoch 1290, recon_loss:0.772543, zinb_loss:0.639401, cluster_loss:0.152101\n",
      "Clustering   1290: ASW= 0.8277, DB= 0.2351, CH= 54006.8839\n",
      "Training epoch 1291, recon_loss:0.772459, zinb_loss:0.639086, cluster_loss:0.152177\n",
      "Clustering   1291: ASW= 0.8272, DB= 0.2371, CH= 53350.7648\n",
      "Training epoch 1292, recon_loss:0.772801, zinb_loss:0.639496, cluster_loss:0.152151\n",
      "Clustering   1292: ASW= 0.8275, DB= 0.2353, CH= 54019.5404\n",
      "Training epoch 1293, recon_loss:0.772689, zinb_loss:0.639241, cluster_loss:0.152321\n",
      "Clustering   1293: ASW= 0.8271, DB= 0.2370, CH= 53315.4273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1294, recon_loss:0.772979, zinb_loss:0.639685, cluster_loss:0.152234\n",
      "Clustering   1294: ASW= 0.8273, DB= 0.2355, CH= 53972.5431\n",
      "Training epoch 1295, recon_loss:0.772866, zinb_loss:0.639509, cluster_loss:0.152482\n",
      "Clustering   1295: ASW= 0.8270, DB= 0.2369, CH= 53299.0114\n",
      "Training epoch 1296, recon_loss:0.773143, zinb_loss:0.639984, cluster_loss:0.152344\n",
      "Clustering   1296: ASW= 0.8271, DB= 0.2361, CH= 53848.2824\n",
      "Training epoch 1297, recon_loss:0.772974, zinb_loss:0.639883, cluster_loss:0.152628\n",
      "Clustering   1297: ASW= 0.8271, DB= 0.2370, CH= 53315.5827\n",
      "Training epoch 1298, recon_loss:0.773333, zinb_loss:0.640344, cluster_loss:0.152419\n",
      "Clustering   1298: ASW= 0.8270, DB= 0.2363, CH= 53787.1338\n",
      "Training epoch 1299, recon_loss:0.773021, zinb_loss:0.640223, cluster_loss:0.152656\n",
      "Clustering   1299: ASW= 0.8273, DB= 0.2367, CH= 53401.7473\n",
      "Training epoch 1300, recon_loss:0.773605, zinb_loss:0.640603, cluster_loss:0.152450\n",
      "Clustering   1300: ASW= 0.8270, DB= 0.2362, CH= 53769.2998\n",
      "Training epoch 1301, recon_loss:0.772661, zinb_loss:0.640328, cluster_loss:0.152517\n",
      "Clustering   1301: ASW= 0.8277, DB= 0.2364, CH= 53576.5167\n",
      "Training epoch 1302, recon_loss:0.773202, zinb_loss:0.640488, cluster_loss:0.152412\n",
      "Clustering   1302: ASW= 0.8271, DB= 0.2359, CH= 53795.1628\n",
      "Training epoch 1303, recon_loss:0.772223, zinb_loss:0.640331, cluster_loss:0.152307\n",
      "Clustering   1303: ASW= 0.8283, DB= 0.2361, CH= 53832.6853\n",
      "Training epoch 1304, recon_loss:0.772725, zinb_loss:0.640236, cluster_loss:0.152400\n",
      "Clustering   1304: ASW= 0.8270, DB= 0.2369, CH= 53773.9855\n",
      "Training epoch 1305, recon_loss:0.772202, zinb_loss:0.640420, cluster_loss:0.152171\n",
      "Clustering   1305: ASW= 0.8287, DB= 0.2356, CH= 53990.6917\n",
      "Training epoch 1306, recon_loss:0.772838, zinb_loss:0.640102, cluster_loss:0.152653\n",
      "Clustering   1306: ASW= 0.8262, DB= 0.2371, CH= 53502.7436\n",
      "Training epoch 1307, recon_loss:0.772959, zinb_loss:0.640773, cluster_loss:0.152296\n",
      "Clustering   1307: ASW= 0.8290, DB= 0.2349, CH= 53907.6272\n",
      "Training epoch 1308, recon_loss:0.773748, zinb_loss:0.640093, cluster_loss:0.153133\n",
      "Clustering   1308: ASW= 0.8253, DB= 0.2386, CH= 53067.1000\n",
      "Training epoch 1309, recon_loss:0.773581, zinb_loss:0.640816, cluster_loss:0.152492\n",
      "Clustering   1309: ASW= 0.8282, DB= 0.2349, CH= 53563.2506\n",
      "Training epoch 1310, recon_loss:0.773248, zinb_loss:0.639795, cluster_loss:0.153240\n",
      "Clustering   1310: ASW= 0.8248, DB= 0.2392, CH= 52722.9944\n",
      "Training epoch 1311, recon_loss:0.773191, zinb_loss:0.640545, cluster_loss:0.152304\n",
      "Clustering   1311: ASW= 0.8284, DB= 0.2350, CH= 53737.9445\n",
      "Training epoch 1312, recon_loss:0.772790, zinb_loss:0.639521, cluster_loss:0.153062\n",
      "Clustering   1312: ASW= 0.8251, DB= 0.2390, CH= 52739.0572\n",
      "Training epoch 1313, recon_loss:0.772648, zinb_loss:0.640186, cluster_loss:0.152071\n",
      "Clustering   1313: ASW= 0.8288, DB= 0.2347, CH= 54016.0657\n",
      "Training epoch 1314, recon_loss:0.772170, zinb_loss:0.639200, cluster_loss:0.152844\n",
      "Clustering   1314: ASW= 0.8256, DB= 0.2383, CH= 52916.8695\n",
      "Training epoch 1315, recon_loss:0.772117, zinb_loss:0.639795, cluster_loss:0.151918\n",
      "Clustering   1315: ASW= 0.8289, DB= 0.2349, CH= 54147.0361\n",
      "Training epoch 1316, recon_loss:0.771709, zinb_loss:0.638938, cluster_loss:0.152639\n",
      "Clustering   1316: ASW= 0.8260, DB= 0.2374, CH= 53151.3073\n",
      "Training epoch 1317, recon_loss:0.771604, zinb_loss:0.639517, cluster_loss:0.151812\n",
      "Clustering   1317: ASW= 0.8292, DB= 0.2347, CH= 54357.3798\n",
      "Training epoch 1318, recon_loss:0.771231, zinb_loss:0.638732, cluster_loss:0.152560\n",
      "Clustering   1318: ASW= 0.8261, DB= 0.2372, CH= 53226.9599\n",
      "Training epoch 1319, recon_loss:0.771289, zinb_loss:0.639319, cluster_loss:0.151772\n",
      "Clustering   1319: ASW= 0.8293, DB= 0.2348, CH= 54439.6854\n",
      "Training epoch 1320, recon_loss:0.771004, zinb_loss:0.638622, cluster_loss:0.152536\n",
      "Clustering   1320: ASW= 0.8263, DB= 0.2364, CH= 53291.3935\n",
      "Training epoch 1321, recon_loss:0.771117, zinb_loss:0.639178, cluster_loss:0.151792\n",
      "Clustering   1321: ASW= 0.8295, DB= 0.2349, CH= 54602.5150\n",
      "Training epoch 1322, recon_loss:0.770955, zinb_loss:0.638579, cluster_loss:0.152606\n",
      "Clustering   1322: ASW= 0.8262, DB= 0.2363, CH= 53163.9338\n",
      "Training epoch 1323, recon_loss:0.771075, zinb_loss:0.639052, cluster_loss:0.151846\n",
      "Clustering   1323: ASW= 0.8293, DB= 0.2360, CH= 54509.1654\n",
      "Training epoch 1324, recon_loss:0.771129, zinb_loss:0.638660, cluster_loss:0.152708\n",
      "Clustering   1324: ASW= 0.8266, DB= 0.2357, CH= 53344.0206\n",
      "Training epoch 1325, recon_loss:0.771259, zinb_loss:0.639023, cluster_loss:0.151957\n",
      "Clustering   1325: ASW= 0.8293, DB= 0.2363, CH= 54741.7416\n",
      "Training epoch 1326, recon_loss:0.771313, zinb_loss:0.638748, cluster_loss:0.152673\n",
      "Clustering   1326: ASW= 0.8267, DB= 0.2356, CH= 53140.5080\n",
      "Training epoch 1327, recon_loss:0.771661, zinb_loss:0.638906, cluster_loss:0.152217\n",
      "Clustering   1327: ASW= 0.8283, DB= 0.2377, CH= 54396.1820\n",
      "Training epoch 1328, recon_loss:0.772621, zinb_loss:0.639121, cluster_loss:0.153482\n",
      "Clustering   1328: ASW= 0.8269, DB= 0.2346, CH= 53108.1177\n",
      "Training epoch 1329, recon_loss:0.772317, zinb_loss:0.639056, cluster_loss:0.152412\n",
      "Clustering   1329: ASW= 0.8282, DB= 0.2383, CH= 54493.1348\n",
      "Training epoch 1330, recon_loss:0.772259, zinb_loss:0.639088, cluster_loss:0.153074\n",
      "Clustering   1330: ASW= 0.8274, DB= 0.2341, CH= 53189.7430\n",
      "Training epoch 1331, recon_loss:0.771799, zinb_loss:0.638923, cluster_loss:0.152269\n",
      "Clustering   1331: ASW= 0.8282, DB= 0.2381, CH= 54563.5728\n",
      "Training epoch 1332, recon_loss:0.772087, zinb_loss:0.639045, cluster_loss:0.152708\n",
      "Clustering   1332: ASW= 0.8280, DB= 0.2343, CH= 53228.3962\n",
      "Training epoch 1333, recon_loss:0.772107, zinb_loss:0.638757, cluster_loss:0.152475\n",
      "Clustering   1333: ASW= 0.8274, DB= 0.2390, CH= 54335.8374\n",
      "Training epoch 1334, recon_loss:0.772799, zinb_loss:0.639118, cluster_loss:0.152771\n",
      "Clustering   1334: ASW= 0.8284, DB= 0.2344, CH= 53235.2281\n",
      "Training epoch 1335, recon_loss:0.772561, zinb_loss:0.638542, cluster_loss:0.152761\n",
      "Clustering   1335: ASW= 0.8269, DB= 0.2395, CH= 54019.1946\n",
      "Training epoch 1336, recon_loss:0.772946, zinb_loss:0.638963, cluster_loss:0.152696\n",
      "Clustering   1336: ASW= 0.8286, DB= 0.2349, CH= 53253.3780\n",
      "Training epoch 1337, recon_loss:0.772281, zinb_loss:0.638362, cluster_loss:0.152574\n",
      "Clustering   1337: ASW= 0.8271, DB= 0.2390, CH= 54024.4060\n",
      "Training epoch 1338, recon_loss:0.772488, zinb_loss:0.638792, cluster_loss:0.152385\n",
      "Clustering   1338: ASW= 0.8289, DB= 0.2341, CH= 53407.9346\n",
      "Training epoch 1339, recon_loss:0.772009, zinb_loss:0.638395, cluster_loss:0.152352\n",
      "Clustering   1339: ASW= 0.8273, DB= 0.2384, CH= 54087.7310\n",
      "Training epoch 1340, recon_loss:0.772294, zinb_loss:0.638841, cluster_loss:0.152222\n",
      "Clustering   1340: ASW= 0.8290, DB= 0.2339, CH= 53503.1926\n",
      "Training epoch 1341, recon_loss:0.772069, zinb_loss:0.638627, cluster_loss:0.152261\n",
      "Clustering   1341: ASW= 0.8273, DB= 0.2385, CH= 54165.7747\n",
      "Training epoch 1342, recon_loss:0.772464, zinb_loss:0.639116, cluster_loss:0.152235\n",
      "Clustering   1342: ASW= 0.8291, DB= 0.2341, CH= 53496.7086\n",
      "Training epoch 1343, recon_loss:0.772215, zinb_loss:0.639002, cluster_loss:0.152238\n",
      "Clustering   1343: ASW= 0.8275, DB= 0.2384, CH= 54259.7082\n",
      "Training epoch 1344, recon_loss:0.772485, zinb_loss:0.639393, cluster_loss:0.152354\n",
      "Clustering   1344: ASW= 0.8289, DB= 0.2341, CH= 53534.9030\n",
      "Training epoch 1345, recon_loss:0.772231, zinb_loss:0.639366, cluster_loss:0.152308\n",
      "Clustering   1345: ASW= 0.8276, DB= 0.2383, CH= 54283.5878\n",
      "Training epoch 1346, recon_loss:0.772503, zinb_loss:0.639520, cluster_loss:0.152580\n",
      "Clustering   1346: ASW= 0.8286, DB= 0.2341, CH= 53443.5884\n",
      "Training epoch 1347, recon_loss:0.772475, zinb_loss:0.639645, cluster_loss:0.152489\n",
      "Clustering   1347: ASW= 0.8275, DB= 0.2376, CH= 54256.0822\n",
      "Training epoch 1348, recon_loss:0.772724, zinb_loss:0.639736, cluster_loss:0.152739\n",
      "Clustering   1348: ASW= 0.8287, DB= 0.2340, CH= 53414.8077\n",
      "Training epoch 1349, recon_loss:0.772372, zinb_loss:0.639763, cluster_loss:0.152453\n",
      "Clustering   1349: ASW= 0.8277, DB= 0.2371, CH= 54233.5372\n",
      "Training epoch 1350, recon_loss:0.772237, zinb_loss:0.639666, cluster_loss:0.152681\n",
      "Clustering   1350: ASW= 0.8288, DB= 0.2333, CH= 53678.8647\n",
      "Training epoch 1351, recon_loss:0.772123, zinb_loss:0.639805, cluster_loss:0.152406\n",
      "Clustering   1351: ASW= 0.8278, DB= 0.2371, CH= 54191.3282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1352, recon_loss:0.772381, zinb_loss:0.639551, cluster_loss:0.152765\n",
      "Clustering   1352: ASW= 0.8287, DB= 0.2342, CH= 53831.7886\n",
      "Training epoch 1353, recon_loss:0.772489, zinb_loss:0.639781, cluster_loss:0.152451\n",
      "Clustering   1353: ASW= 0.8278, DB= 0.2364, CH= 54028.8505\n",
      "Training epoch 1354, recon_loss:0.772211, zinb_loss:0.639389, cluster_loss:0.152781\n",
      "Clustering   1354: ASW= 0.8286, DB= 0.2350, CH= 53883.4986\n",
      "Training epoch 1355, recon_loss:0.772476, zinb_loss:0.639586, cluster_loss:0.152365\n",
      "Clustering   1355: ASW= 0.8280, DB= 0.2358, CH= 54031.2378\n",
      "Training epoch 1356, recon_loss:0.771945, zinb_loss:0.639173, cluster_loss:0.152677\n",
      "Clustering   1356: ASW= 0.8288, DB= 0.2349, CH= 53970.0474\n",
      "Training epoch 1357, recon_loss:0.772143, zinb_loss:0.639446, cluster_loss:0.152306\n",
      "Clustering   1357: ASW= 0.8275, DB= 0.2358, CH= 53898.5450\n",
      "Training epoch 1358, recon_loss:0.771996, zinb_loss:0.639219, cluster_loss:0.152726\n",
      "Clustering   1358: ASW= 0.8289, DB= 0.2354, CH= 53885.0351\n",
      "Training epoch 1359, recon_loss:0.772407, zinb_loss:0.639474, cluster_loss:0.152217\n",
      "Clustering   1359: ASW= 0.8279, DB= 0.2354, CH= 53920.3606\n",
      "Training epoch 1360, recon_loss:0.772296, zinb_loss:0.639357, cluster_loss:0.152701\n",
      "Clustering   1360: ASW= 0.8290, DB= 0.2353, CH= 54077.9333\n",
      "Training epoch 1361, recon_loss:0.772598, zinb_loss:0.639454, cluster_loss:0.152200\n",
      "Clustering   1361: ASW= 0.8279, DB= 0.2354, CH= 53861.2428\n",
      "Training epoch 1362, recon_loss:0.772201, zinb_loss:0.639188, cluster_loss:0.152631\n",
      "Clustering   1362: ASW= 0.8290, DB= 0.2356, CH= 54163.8676\n",
      "Training epoch 1363, recon_loss:0.772340, zinb_loss:0.639220, cluster_loss:0.152070\n",
      "Clustering   1363: ASW= 0.8281, DB= 0.2354, CH= 53934.7163\n",
      "Training epoch 1364, recon_loss:0.771886, zinb_loss:0.638910, cluster_loss:0.152486\n",
      "Clustering   1364: ASW= 0.8290, DB= 0.2355, CH= 54186.3529\n",
      "Training epoch 1365, recon_loss:0.771984, zinb_loss:0.638978, cluster_loss:0.151931\n",
      "Clustering   1365: ASW= 0.8283, DB= 0.2355, CH= 54102.8711\n",
      "Training epoch 1366, recon_loss:0.771751, zinb_loss:0.638725, cluster_loss:0.152391\n",
      "Clustering   1366: ASW= 0.8290, DB= 0.2347, CH= 54151.0044\n",
      "Training epoch 1367, recon_loss:0.771773, zinb_loss:0.638776, cluster_loss:0.151826\n",
      "Clustering   1367: ASW= 0.8284, DB= 0.2356, CH= 54209.7089\n",
      "Training epoch 1368, recon_loss:0.771647, zinb_loss:0.638549, cluster_loss:0.152264\n",
      "Clustering   1368: ASW= 0.8290, DB= 0.2349, CH= 54267.3439\n",
      "Training epoch 1369, recon_loss:0.771651, zinb_loss:0.638662, cluster_loss:0.151805\n",
      "Clustering   1369: ASW= 0.8285, DB= 0.2352, CH= 54291.2511\n",
      "Training epoch 1370, recon_loss:0.771663, zinb_loss:0.638513, cluster_loss:0.152191\n",
      "Clustering   1370: ASW= 0.8291, DB= 0.2348, CH= 54306.3594\n",
      "Training epoch 1371, recon_loss:0.771562, zinb_loss:0.638600, cluster_loss:0.151768\n",
      "Clustering   1371: ASW= 0.8286, DB= 0.2346, CH= 54370.0542\n",
      "Training epoch 1372, recon_loss:0.771643, zinb_loss:0.638477, cluster_loss:0.152142\n",
      "Clustering   1372: ASW= 0.8291, DB= 0.2353, CH= 54415.1828\n",
      "Training epoch 1373, recon_loss:0.771579, zinb_loss:0.638609, cluster_loss:0.151799\n",
      "Clustering   1373: ASW= 0.8286, DB= 0.2351, CH= 54454.6469\n",
      "Training epoch 1374, recon_loss:0.771753, zinb_loss:0.638597, cluster_loss:0.152159\n",
      "Clustering   1374: ASW= 0.8291, DB= 0.2349, CH= 54347.4044\n",
      "Training epoch 1375, recon_loss:0.771631, zinb_loss:0.638704, cluster_loss:0.151851\n",
      "Clustering   1375: ASW= 0.8285, DB= 0.2357, CH= 54514.7884\n",
      "Training epoch 1376, recon_loss:0.772086, zinb_loss:0.638876, cluster_loss:0.152300\n",
      "Clustering   1376: ASW= 0.8291, DB= 0.2352, CH= 54379.2300\n",
      "Training epoch 1377, recon_loss:0.771992, zinb_loss:0.639027, cluster_loss:0.151972\n",
      "Clustering   1377: ASW= 0.8285, DB= 0.2359, CH= 54619.0368\n",
      "Training epoch 1378, recon_loss:0.772404, zinb_loss:0.639248, cluster_loss:0.152374\n",
      "Clustering   1378: ASW= 0.8292, DB= 0.2345, CH= 54299.0085\n",
      "Training epoch 1379, recon_loss:0.772835, zinb_loss:0.639553, cluster_loss:0.152250\n",
      "Clustering   1379: ASW= 0.8283, DB= 0.2366, CH= 54669.4939\n",
      "Training epoch 1380, recon_loss:0.773144, zinb_loss:0.639691, cluster_loss:0.152723\n",
      "Clustering   1380: ASW= 0.8282, DB= 0.2348, CH= 53425.9921\n",
      "Training epoch 1381, recon_loss:0.773066, zinb_loss:0.639559, cluster_loss:0.152447\n",
      "Clustering   1381: ASW= 0.8273, DB= 0.2379, CH= 54511.7982\n",
      "Training epoch 1382, recon_loss:0.774201, zinb_loss:0.640286, cluster_loss:0.152549\n",
      "Clustering   1382: ASW= 0.8296, DB= 0.2331, CH= 53949.9548\n",
      "Training epoch 1383, recon_loss:0.773321, zinb_loss:0.639825, cluster_loss:0.152426\n",
      "Clustering   1383: ASW= 0.8272, DB= 0.2386, CH= 54586.2291\n",
      "Training epoch 1384, recon_loss:0.773336, zinb_loss:0.640328, cluster_loss:0.152405\n",
      "Clustering   1384: ASW= 0.8296, DB= 0.2330, CH= 53904.2068\n",
      "Training epoch 1385, recon_loss:0.772507, zinb_loss:0.639978, cluster_loss:0.152176\n",
      "Clustering   1385: ASW= 0.8278, DB= 0.2372, CH= 54733.3690\n",
      "Training epoch 1386, recon_loss:0.772887, zinb_loss:0.640298, cluster_loss:0.152376\n",
      "Clustering   1386: ASW= 0.8296, DB= 0.2330, CH= 54114.7877\n",
      "Training epoch 1387, recon_loss:0.772312, zinb_loss:0.640075, cluster_loss:0.152264\n",
      "Clustering   1387: ASW= 0.8281, DB= 0.2365, CH= 54779.2037\n",
      "Training epoch 1388, recon_loss:0.772854, zinb_loss:0.640248, cluster_loss:0.152553\n",
      "Clustering   1388: ASW= 0.8298, DB= 0.2335, CH= 54215.1935\n",
      "Training epoch 1389, recon_loss:0.772104, zinb_loss:0.640141, cluster_loss:0.152399\n",
      "Clustering   1389: ASW= 0.8283, DB= 0.2357, CH= 54736.5597\n",
      "Training epoch 1390, recon_loss:0.772619, zinb_loss:0.640005, cluster_loss:0.152631\n",
      "Clustering   1390: ASW= 0.8298, DB= 0.2348, CH= 54384.8279\n",
      "Training epoch 1391, recon_loss:0.772085, zinb_loss:0.640213, cluster_loss:0.152488\n",
      "Clustering   1391: ASW= 0.8284, DB= 0.2346, CH= 54721.8953\n",
      "Training epoch 1392, recon_loss:0.772427, zinb_loss:0.639817, cluster_loss:0.152825\n",
      "Clustering   1392: ASW= 0.8295, DB= 0.2356, CH= 54297.4635\n",
      "Training epoch 1393, recon_loss:0.772256, zinb_loss:0.640200, cluster_loss:0.152467\n",
      "Clustering   1393: ASW= 0.8285, DB= 0.2345, CH= 54644.4699\n",
      "Training epoch 1394, recon_loss:0.772506, zinb_loss:0.639754, cluster_loss:0.152980\n",
      "Clustering   1394: ASW= 0.8290, DB= 0.2357, CH= 54064.9132\n",
      "Training epoch 1395, recon_loss:0.772247, zinb_loss:0.640269, cluster_loss:0.152309\n",
      "Clustering   1395: ASW= 0.8287, DB= 0.2344, CH= 54614.6933\n",
      "Training epoch 1396, recon_loss:0.772559, zinb_loss:0.639751, cluster_loss:0.153014\n",
      "Clustering   1396: ASW= 0.8286, DB= 0.2359, CH= 53910.2489\n",
      "Training epoch 1397, recon_loss:0.772225, zinb_loss:0.640218, cluster_loss:0.152192\n",
      "Clustering   1397: ASW= 0.8288, DB= 0.2344, CH= 54555.9312\n",
      "Training epoch 1398, recon_loss:0.772444, zinb_loss:0.639629, cluster_loss:0.153082\n",
      "Clustering   1398: ASW= 0.8280, DB= 0.2361, CH= 53665.6823\n",
      "Training epoch 1399, recon_loss:0.771805, zinb_loss:0.640190, cluster_loss:0.152012\n",
      "Clustering   1399: ASW= 0.8288, DB= 0.2338, CH= 54472.5884\n",
      "Training epoch 1400, recon_loss:0.772179, zinb_loss:0.639541, cluster_loss:0.152912\n",
      "Clustering   1400: ASW= 0.8277, DB= 0.2359, CH= 53660.7053\n",
      "Training epoch 1401, recon_loss:0.771531, zinb_loss:0.639928, cluster_loss:0.152004\n",
      "Clustering   1401: ASW= 0.8286, DB= 0.2344, CH= 54253.1947\n",
      "Training epoch 1402, recon_loss:0.771881, zinb_loss:0.639451, cluster_loss:0.152713\n",
      "Clustering   1402: ASW= 0.8278, DB= 0.2353, CH= 53763.1419\n",
      "Training epoch 1403, recon_loss:0.771235, zinb_loss:0.639777, cluster_loss:0.151784\n",
      "Clustering   1403: ASW= 0.8289, DB= 0.2344, CH= 54454.4181\n",
      "Training epoch 1404, recon_loss:0.771695, zinb_loss:0.639306, cluster_loss:0.152505\n",
      "Clustering   1404: ASW= 0.8278, DB= 0.2350, CH= 53912.0201\n",
      "Training epoch 1405, recon_loss:0.770906, zinb_loss:0.639534, cluster_loss:0.151721\n",
      "Clustering   1405: ASW= 0.8290, DB= 0.2341, CH= 54464.7775\n",
      "Training epoch 1406, recon_loss:0.771493, zinb_loss:0.639203, cluster_loss:0.152312\n",
      "Clustering   1406: ASW= 0.8281, DB= 0.2345, CH= 54193.3249\n",
      "Training epoch 1407, recon_loss:0.770836, zinb_loss:0.639415, cluster_loss:0.151635\n",
      "Clustering   1407: ASW= 0.8293, DB= 0.2346, CH= 54666.8438\n",
      "Training epoch 1408, recon_loss:0.771380, zinb_loss:0.639143, cluster_loss:0.152154\n",
      "Clustering   1408: ASW= 0.8283, DB= 0.2340, CH= 54451.6608\n",
      "Training epoch 1409, recon_loss:0.770816, zinb_loss:0.639231, cluster_loss:0.151682\n",
      "Clustering   1409: ASW= 0.8292, DB= 0.2346, CH= 54646.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1410, recon_loss:0.771443, zinb_loss:0.639096, cluster_loss:0.152075\n",
      "Clustering   1410: ASW= 0.8285, DB= 0.2334, CH= 54604.9031\n",
      "Training epoch 1411, recon_loss:0.770890, zinb_loss:0.639158, cluster_loss:0.151716\n",
      "Clustering   1411: ASW= 0.8294, DB= 0.2346, CH= 54706.4227\n",
      "Training epoch 1412, recon_loss:0.771499, zinb_loss:0.639153, cluster_loss:0.152013\n",
      "Clustering   1412: ASW= 0.8285, DB= 0.2333, CH= 54792.2556\n",
      "Training epoch 1413, recon_loss:0.771085, zinb_loss:0.639050, cluster_loss:0.151885\n",
      "Clustering   1413: ASW= 0.8291, DB= 0.2356, CH= 54561.7477\n",
      "Training epoch 1414, recon_loss:0.771887, zinb_loss:0.639223, cluster_loss:0.152052\n",
      "Clustering   1414: ASW= 0.8286, DB= 0.2328, CH= 54874.3752\n",
      "Training epoch 1415, recon_loss:0.771371, zinb_loss:0.639026, cluster_loss:0.152114\n",
      "Clustering   1415: ASW= 0.8288, DB= 0.2360, CH= 54453.4938\n",
      "Training epoch 1416, recon_loss:0.772349, zinb_loss:0.639457, cluster_loss:0.152063\n",
      "Clustering   1416: ASW= 0.8288, DB= 0.2322, CH= 54898.2272\n",
      "Training epoch 1417, recon_loss:0.771999, zinb_loss:0.638990, cluster_loss:0.152471\n",
      "Clustering   1417: ASW= 0.8282, DB= 0.2377, CH= 54144.4925\n",
      "Training epoch 1418, recon_loss:0.773227, zinb_loss:0.639582, cluster_loss:0.152191\n",
      "Clustering   1418: ASW= 0.8290, DB= 0.2319, CH= 54865.1442\n",
      "Training epoch 1419, recon_loss:0.772469, zinb_loss:0.638764, cluster_loss:0.152846\n",
      "Clustering   1419: ASW= 0.8278, DB= 0.2389, CH= 53818.0596\n",
      "Training epoch 1420, recon_loss:0.773526, zinb_loss:0.639486, cluster_loss:0.152219\n",
      "Clustering   1420: ASW= 0.8296, DB= 0.2318, CH= 54813.2543\n",
      "Training epoch 1421, recon_loss:0.772539, zinb_loss:0.638518, cluster_loss:0.152988\n",
      "Clustering   1421: ASW= 0.8276, DB= 0.2392, CH= 53676.4965\n",
      "Training epoch 1422, recon_loss:0.773251, zinb_loss:0.639302, cluster_loss:0.152174\n",
      "Clustering   1422: ASW= 0.8299, DB= 0.2319, CH= 54764.5636\n",
      "Training epoch 1423, recon_loss:0.772323, zinb_loss:0.638421, cluster_loss:0.152848\n",
      "Clustering   1423: ASW= 0.8279, DB= 0.2386, CH= 53819.1749\n",
      "Training epoch 1424, recon_loss:0.772838, zinb_loss:0.639342, cluster_loss:0.152017\n",
      "Clustering   1424: ASW= 0.8303, DB= 0.2314, CH= 54877.3062\n",
      "Training epoch 1425, recon_loss:0.771979, zinb_loss:0.638596, cluster_loss:0.152603\n",
      "Clustering   1425: ASW= 0.8282, DB= 0.2375, CH= 54016.9811\n",
      "Training epoch 1426, recon_loss:0.772317, zinb_loss:0.639475, cluster_loss:0.151859\n",
      "Clustering   1426: ASW= 0.8306, DB= 0.2318, CH= 55070.8122\n",
      "Training epoch 1427, recon_loss:0.771571, zinb_loss:0.638842, cluster_loss:0.152383\n",
      "Clustering   1427: ASW= 0.8285, DB= 0.2370, CH= 54245.9790\n",
      "Training epoch 1428, recon_loss:0.772037, zinb_loss:0.639590, cluster_loss:0.151766\n",
      "Clustering   1428: ASW= 0.8308, DB= 0.2316, CH= 55225.0998\n",
      "Training epoch 1429, recon_loss:0.771432, zinb_loss:0.638991, cluster_loss:0.152226\n",
      "Clustering   1429: ASW= 0.8286, DB= 0.2363, CH= 54339.3585\n",
      "Training epoch 1430, recon_loss:0.772133, zinb_loss:0.639512, cluster_loss:0.151766\n",
      "Clustering   1430: ASW= 0.8306, DB= 0.2313, CH= 55301.0874\n",
      "Training epoch 1431, recon_loss:0.771736, zinb_loss:0.639135, cluster_loss:0.152194\n",
      "Clustering   1431: ASW= 0.8288, DB= 0.2362, CH= 54585.3976\n",
      "Training epoch 1432, recon_loss:0.772542, zinb_loss:0.639696, cluster_loss:0.151780\n",
      "Clustering   1432: ASW= 0.8308, DB= 0.2314, CH= 55356.2969\n",
      "Training epoch 1433, recon_loss:0.772024, zinb_loss:0.639320, cluster_loss:0.152198\n",
      "Clustering   1433: ASW= 0.8288, DB= 0.2358, CH= 54625.5232\n",
      "Training epoch 1434, recon_loss:0.772652, zinb_loss:0.639797, cluster_loss:0.151862\n",
      "Clustering   1434: ASW= 0.8307, DB= 0.2315, CH= 55378.2017\n",
      "Training epoch 1435, recon_loss:0.772109, zinb_loss:0.639419, cluster_loss:0.152228\n",
      "Clustering   1435: ASW= 0.8288, DB= 0.2360, CH= 54634.2800\n",
      "Training epoch 1436, recon_loss:0.772615, zinb_loss:0.639803, cluster_loss:0.151946\n",
      "Clustering   1436: ASW= 0.8307, DB= 0.2315, CH= 55364.8789\n",
      "Training epoch 1437, recon_loss:0.772186, zinb_loss:0.639450, cluster_loss:0.152217\n",
      "Clustering   1437: ASW= 0.8289, DB= 0.2357, CH= 54667.3469\n",
      "Training epoch 1438, recon_loss:0.772624, zinb_loss:0.639638, cluster_loss:0.152112\n",
      "Clustering   1438: ASW= 0.8304, DB= 0.2315, CH= 55245.2957\n",
      "Training epoch 1439, recon_loss:0.772491, zinb_loss:0.639424, cluster_loss:0.152257\n",
      "Clustering   1439: ASW= 0.8292, DB= 0.2353, CH= 54706.4187\n",
      "Training epoch 1440, recon_loss:0.772881, zinb_loss:0.639403, cluster_loss:0.152371\n",
      "Clustering   1440: ASW= 0.8300, DB= 0.2323, CH= 55020.8940\n",
      "Training epoch 1441, recon_loss:0.772932, zinb_loss:0.639434, cluster_loss:0.152328\n",
      "Clustering   1441: ASW= 0.8293, DB= 0.2346, CH= 54581.0326\n",
      "Training epoch 1442, recon_loss:0.772795, zinb_loss:0.639016, cluster_loss:0.152637\n",
      "Clustering   1442: ASW= 0.8295, DB= 0.2332, CH= 54678.1546\n",
      "Training epoch 1443, recon_loss:0.772662, zinb_loss:0.639197, cluster_loss:0.152522\n",
      "Clustering   1443: ASW= 0.8293, DB= 0.2352, CH= 54346.2627\n",
      "Training epoch 1444, recon_loss:0.771920, zinb_loss:0.638727, cluster_loss:0.152515\n",
      "Clustering   1444: ASW= 0.8288, DB= 0.2342, CH= 54147.9193\n",
      "Training epoch 1445, recon_loss:0.771788, zinb_loss:0.638959, cluster_loss:0.152561\n",
      "Clustering   1445: ASW= 0.8276, DB= 0.2378, CH= 53844.2491\n",
      "Training epoch 1446, recon_loss:0.772936, zinb_loss:0.638988, cluster_loss:0.153884\n",
      "Clustering   1446: ASW= 0.8280, DB= 0.2348, CH= 53354.0016\n",
      "Training epoch 1447, recon_loss:0.772248, zinb_loss:0.639176, cluster_loss:0.152610\n",
      "Clustering   1447: ASW= 0.8290, DB= 0.2357, CH= 54002.2283\n",
      "Training epoch 1448, recon_loss:0.773897, zinb_loss:0.639109, cluster_loss:0.154721\n",
      "Clustering   1448: ASW= 0.8266, DB= 0.2352, CH= 52883.8802\n",
      "Training epoch 1449, recon_loss:0.777321, zinb_loss:0.639760, cluster_loss:0.154381\n",
      "Clustering   1449: ASW= 0.8224, DB= 0.2433, CH= 51861.2071\n",
      "Training epoch 1450, recon_loss:0.775597, zinb_loss:0.639327, cluster_loss:0.153430\n",
      "Clustering   1450: ASW= 0.8259, DB= 0.2381, CH= 52838.2194\n",
      "Training epoch 1451, recon_loss:0.772094, zinb_loss:0.638829, cluster_loss:0.152364\n",
      "Clustering   1451: ASW= 0.8261, DB= 0.2389, CH= 52607.6735\n",
      "Training epoch 1452, recon_loss:0.770926, zinb_loss:0.638589, cluster_loss:0.152091\n",
      "Clustering   1452: ASW= 0.8275, DB= 0.2361, CH= 53903.0808\n",
      "Training epoch 1453, recon_loss:0.770454, zinb_loss:0.638291, cluster_loss:0.151875\n",
      "Clustering   1453: ASW= 0.8277, DB= 0.2361, CH= 53563.0658\n",
      "Training epoch 1454, recon_loss:0.770039, zinb_loss:0.638421, cluster_loss:0.151669\n",
      "Clustering   1454: ASW= 0.8286, DB= 0.2347, CH= 54531.1569\n",
      "Training epoch 1455, recon_loss:0.769956, zinb_loss:0.638136, cluster_loss:0.151758\n",
      "Clustering   1455: ASW= 0.8283, DB= 0.2354, CH= 53942.9018\n",
      "Training epoch 1456, recon_loss:0.769886, zinb_loss:0.638518, cluster_loss:0.151486\n",
      "Clustering   1456: ASW= 0.8293, DB= 0.2340, CH= 54921.4944\n",
      "Training epoch 1457, recon_loss:0.769926, zinb_loss:0.638240, cluster_loss:0.151816\n",
      "Clustering   1457: ASW= 0.8286, DB= 0.2353, CH= 54131.3247\n",
      "Training epoch 1458, recon_loss:0.770206, zinb_loss:0.638897, cluster_loss:0.151431\n",
      "Clustering   1458: ASW= 0.8298, DB= 0.2338, CH= 55220.3378\n",
      "Training epoch 1459, recon_loss:0.770463, zinb_loss:0.638607, cluster_loss:0.152103\n",
      "Clustering   1459: ASW= 0.8285, DB= 0.2357, CH= 54110.6906\n",
      "Training epoch 1460, recon_loss:0.771299, zinb_loss:0.639628, cluster_loss:0.151572\n",
      "Clustering   1460: ASW= 0.8301, DB= 0.2332, CH= 55251.4032\n",
      "Training epoch 1461, recon_loss:0.771987, zinb_loss:0.639261, cluster_loss:0.152608\n",
      "Clustering   1461: ASW= 0.8281, DB= 0.2366, CH= 53877.5904\n",
      "Training epoch 1462, recon_loss:0.773203, zinb_loss:0.640359, cluster_loss:0.151897\n",
      "Clustering   1462: ASW= 0.8300, DB= 0.2336, CH= 55016.5840\n",
      "Training epoch 1463, recon_loss:0.773890, zinb_loss:0.639701, cluster_loss:0.152999\n",
      "Clustering   1463: ASW= 0.8280, DB= 0.2368, CH= 53717.1440\n",
      "Training epoch 1464, recon_loss:0.773658, zinb_loss:0.640287, cluster_loss:0.151989\n",
      "Clustering   1464: ASW= 0.8301, DB= 0.2332, CH= 54827.6680\n",
      "Training epoch 1465, recon_loss:0.773532, zinb_loss:0.639236, cluster_loss:0.152768\n",
      "Clustering   1465: ASW= 0.8283, DB= 0.2366, CH= 54007.1876\n",
      "Training epoch 1466, recon_loss:0.772617, zinb_loss:0.639637, cluster_loss:0.151759\n",
      "Clustering   1466: ASW= 0.8304, DB= 0.2325, CH= 54978.0967\n",
      "Training epoch 1467, recon_loss:0.771911, zinb_loss:0.638607, cluster_loss:0.152299\n",
      "Clustering   1467: ASW= 0.8287, DB= 0.2364, CH= 54348.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1468, recon_loss:0.771728, zinb_loss:0.639071, cluster_loss:0.151544\n",
      "Clustering   1468: ASW= 0.8306, DB= 0.2321, CH= 55166.5309\n",
      "Training epoch 1469, recon_loss:0.771511, zinb_loss:0.638383, cluster_loss:0.151988\n",
      "Clustering   1469: ASW= 0.8292, DB= 0.2354, CH= 54741.2045\n",
      "Training epoch 1470, recon_loss:0.771677, zinb_loss:0.638753, cluster_loss:0.151469\n",
      "Clustering   1470: ASW= 0.8307, DB= 0.2319, CH= 55269.0282\n",
      "Training epoch 1471, recon_loss:0.771672, zinb_loss:0.638344, cluster_loss:0.151806\n",
      "Clustering   1471: ASW= 0.8296, DB= 0.2348, CH= 55068.2689\n",
      "Training epoch 1472, recon_loss:0.772098, zinb_loss:0.638704, cluster_loss:0.151551\n",
      "Clustering   1472: ASW= 0.8307, DB= 0.2314, CH= 55320.1890\n",
      "Training epoch 1473, recon_loss:0.772111, zinb_loss:0.638602, cluster_loss:0.151777\n",
      "Clustering   1473: ASW= 0.8301, DB= 0.2344, CH= 55326.3990\n",
      "Training epoch 1474, recon_loss:0.772626, zinb_loss:0.638941, cluster_loss:0.151835\n",
      "Clustering   1474: ASW= 0.8304, DB= 0.2317, CH= 55192.0030\n",
      "Training epoch 1475, recon_loss:0.772546, zinb_loss:0.639146, cluster_loss:0.151820\n",
      "Clustering   1475: ASW= 0.8305, DB= 0.2341, CH= 55600.1142\n",
      "Training epoch 1476, recon_loss:0.773071, zinb_loss:0.639313, cluster_loss:0.152297\n",
      "Clustering   1476: ASW= 0.8300, DB= 0.2321, CH= 54886.5920\n",
      "Training epoch 1477, recon_loss:0.772701, zinb_loss:0.639666, cluster_loss:0.151941\n",
      "Clustering   1477: ASW= 0.8308, DB= 0.2334, CH= 55710.4009\n",
      "Training epoch 1478, recon_loss:0.772924, zinb_loss:0.639418, cluster_loss:0.152765\n",
      "Clustering   1478: ASW= 0.8295, DB= 0.2324, CH= 54477.0440\n",
      "Training epoch 1479, recon_loss:0.772358, zinb_loss:0.639804, cluster_loss:0.152054\n",
      "Clustering   1479: ASW= 0.8308, DB= 0.2336, CH= 55654.1050\n",
      "Training epoch 1480, recon_loss:0.772299, zinb_loss:0.639218, cluster_loss:0.153102\n",
      "Clustering   1480: ASW= 0.8291, DB= 0.2332, CH= 54068.6999\n",
      "Training epoch 1481, recon_loss:0.771817, zinb_loss:0.639626, cluster_loss:0.152104\n",
      "Clustering   1481: ASW= 0.8307, DB= 0.2334, CH= 55480.8532\n",
      "Training epoch 1482, recon_loss:0.771602, zinb_loss:0.638895, cluster_loss:0.153181\n",
      "Clustering   1482: ASW= 0.8288, DB= 0.2335, CH= 53833.4347\n",
      "Training epoch 1483, recon_loss:0.771263, zinb_loss:0.639295, cluster_loss:0.152030\n",
      "Clustering   1483: ASW= 0.8307, DB= 0.2333, CH= 55396.4697\n",
      "Training epoch 1484, recon_loss:0.770999, zinb_loss:0.638703, cluster_loss:0.152880\n",
      "Clustering   1484: ASW= 0.8291, DB= 0.2336, CH= 53980.6921\n",
      "Training epoch 1485, recon_loss:0.770917, zinb_loss:0.639077, cluster_loss:0.151835\n",
      "Clustering   1485: ASW= 0.8306, DB= 0.2335, CH= 55424.1083\n",
      "Training epoch 1486, recon_loss:0.770758, zinb_loss:0.638671, cluster_loss:0.152770\n",
      "Clustering   1486: ASW= 0.8292, DB= 0.2329, CH= 54166.6173\n",
      "Training epoch 1487, recon_loss:0.770732, zinb_loss:0.638919, cluster_loss:0.151696\n",
      "Clustering   1487: ASW= 0.8307, DB= 0.2332, CH= 55538.8257\n",
      "Training epoch 1488, recon_loss:0.770634, zinb_loss:0.638627, cluster_loss:0.152366\n",
      "Clustering   1488: ASW= 0.8297, DB= 0.2329, CH= 54531.2592\n",
      "Training epoch 1489, recon_loss:0.770802, zinb_loss:0.638824, cluster_loss:0.151586\n",
      "Clustering   1489: ASW= 0.8306, DB= 0.2337, CH= 55663.8326\n",
      "Training epoch 1490, recon_loss:0.771036, zinb_loss:0.638755, cluster_loss:0.152185\n",
      "Clustering   1490: ASW= 0.8302, DB= 0.2323, CH= 54790.8168\n",
      "Training epoch 1491, recon_loss:0.771299, zinb_loss:0.638857, cluster_loss:0.151592\n",
      "Clustering   1491: ASW= 0.8304, DB= 0.2342, CH= 55736.1367\n",
      "Training epoch 1492, recon_loss:0.771659, zinb_loss:0.638938, cluster_loss:0.152160\n",
      "Clustering   1492: ASW= 0.8305, DB= 0.2318, CH= 54915.6403\n",
      "Training epoch 1493, recon_loss:0.771696, zinb_loss:0.638954, cluster_loss:0.151685\n",
      "Clustering   1493: ASW= 0.8300, DB= 0.2350, CH= 55766.9713\n",
      "Training epoch 1494, recon_loss:0.771913, zinb_loss:0.639188, cluster_loss:0.152233\n",
      "Clustering   1494: ASW= 0.8307, DB= 0.2320, CH= 54907.7332\n",
      "Training epoch 1495, recon_loss:0.772037, zinb_loss:0.639103, cluster_loss:0.151798\n",
      "Clustering   1495: ASW= 0.8297, DB= 0.2348, CH= 55745.1994\n",
      "Training epoch 1496, recon_loss:0.772467, zinb_loss:0.639348, cluster_loss:0.152308\n",
      "Clustering   1496: ASW= 0.8309, DB= 0.2318, CH= 54890.7813\n",
      "Training epoch 1497, recon_loss:0.772404, zinb_loss:0.639149, cluster_loss:0.151925\n",
      "Clustering   1497: ASW= 0.8294, DB= 0.2357, CH= 55740.5966\n",
      "Training epoch 1498, recon_loss:0.772332, zinb_loss:0.639419, cluster_loss:0.152376\n",
      "Clustering   1498: ASW= 0.8309, DB= 0.2319, CH= 54660.5209\n",
      "Training epoch 1499, recon_loss:0.772242, zinb_loss:0.639168, cluster_loss:0.151998\n",
      "Clustering   1499: ASW= 0.8292, DB= 0.2356, CH= 55597.6126\n",
      "Training epoch 1500, recon_loss:0.772234, zinb_loss:0.639390, cluster_loss:0.152354\n",
      "Clustering   1500: ASW= 0.8309, DB= 0.2316, CH= 54614.2404\n",
      "Training epoch 1501, recon_loss:0.771942, zinb_loss:0.638997, cluster_loss:0.151949\n",
      "Clustering   1501: ASW= 0.8292, DB= 0.2357, CH= 55507.2467\n",
      "Training epoch 1502, recon_loss:0.771585, zinb_loss:0.639148, cluster_loss:0.152174\n",
      "Clustering   1502: ASW= 0.8310, DB= 0.2315, CH= 54688.5617\n",
      "Training epoch 1503, recon_loss:0.771387, zinb_loss:0.638870, cluster_loss:0.151776\n",
      "Clustering   1503: ASW= 0.8293, DB= 0.2356, CH= 55558.1030\n",
      "Training epoch 1504, recon_loss:0.770958, zinb_loss:0.638929, cluster_loss:0.151985\n",
      "Clustering   1504: ASW= 0.8312, DB= 0.2314, CH= 54845.1961\n",
      "Training epoch 1505, recon_loss:0.771068, zinb_loss:0.638847, cluster_loss:0.151629\n",
      "Clustering   1505: ASW= 0.8296, DB= 0.2353, CH= 55650.9120\n",
      "Training epoch 1506, recon_loss:0.770752, zinb_loss:0.638751, cluster_loss:0.151973\n",
      "Clustering   1506: ASW= 0.8310, DB= 0.2313, CH= 54954.2454\n",
      "Training epoch 1507, recon_loss:0.771196, zinb_loss:0.638987, cluster_loss:0.151681\n",
      "Clustering   1507: ASW= 0.8293, DB= 0.2350, CH= 55501.4372\n",
      "Training epoch 1508, recon_loss:0.770929, zinb_loss:0.638906, cluster_loss:0.152014\n",
      "Clustering   1508: ASW= 0.8302, DB= 0.2329, CH= 54448.9850\n",
      "Training epoch 1509, recon_loss:0.771017, zinb_loss:0.638960, cluster_loss:0.151671\n",
      "Clustering   1509: ASW= 0.8295, DB= 0.2348, CH= 55558.0561\n",
      "Training epoch 1510, recon_loss:0.770633, zinb_loss:0.639007, cluster_loss:0.151926\n",
      "Clustering   1510: ASW= 0.8311, DB= 0.2318, CH= 54925.2463\n",
      "Training epoch 1511, recon_loss:0.771042, zinb_loss:0.639220, cluster_loss:0.151618\n",
      "Clustering   1511: ASW= 0.8296, DB= 0.2341, CH= 55531.7540\n",
      "Training epoch 1512, recon_loss:0.771349, zinb_loss:0.639359, cluster_loss:0.151979\n",
      "Clustering   1512: ASW= 0.8311, DB= 0.2316, CH= 55208.2002\n",
      "Training epoch 1513, recon_loss:0.771845, zinb_loss:0.639604, cluster_loss:0.151732\n",
      "Clustering   1513: ASW= 0.8296, DB= 0.2333, CH= 55276.1432\n",
      "Training epoch 1514, recon_loss:0.772170, zinb_loss:0.639526, cluster_loss:0.152207\n",
      "Clustering   1514: ASW= 0.8308, DB= 0.2324, CH= 55316.7699\n",
      "Training epoch 1515, recon_loss:0.772437, zinb_loss:0.639711, cluster_loss:0.151834\n",
      "Clustering   1515: ASW= 0.8299, DB= 0.2326, CH= 55083.7788\n",
      "Training epoch 1516, recon_loss:0.772015, zinb_loss:0.639113, cluster_loss:0.152435\n",
      "Clustering   1516: ASW= 0.8302, DB= 0.2335, CH= 55234.3056\n",
      "Training epoch 1517, recon_loss:0.771876, zinb_loss:0.639194, cluster_loss:0.151884\n",
      "Clustering   1517: ASW= 0.8301, DB= 0.2325, CH= 54809.6581\n",
      "Training epoch 1518, recon_loss:0.770984, zinb_loss:0.638289, cluster_loss:0.152413\n",
      "Clustering   1518: ASW= 0.8297, DB= 0.2339, CH= 55064.2249\n",
      "Training epoch 1519, recon_loss:0.770999, zinb_loss:0.638546, cluster_loss:0.151786\n",
      "Clustering   1519: ASW= 0.8306, DB= 0.2319, CH= 54903.3539\n",
      "Training epoch 1520, recon_loss:0.770280, zinb_loss:0.637760, cluster_loss:0.152288\n",
      "Clustering   1520: ASW= 0.8296, DB= 0.2339, CH= 55031.9780\n",
      "Training epoch 1521, recon_loss:0.770497, zinb_loss:0.638158, cluster_loss:0.151679\n",
      "Clustering   1521: ASW= 0.8311, DB= 0.2317, CH= 55114.4178\n",
      "Training epoch 1522, recon_loss:0.770128, zinb_loss:0.637595, cluster_loss:0.152093\n",
      "Clustering   1522: ASW= 0.8296, DB= 0.2331, CH= 55167.4511\n",
      "Training epoch 1523, recon_loss:0.770623, zinb_loss:0.638001, cluster_loss:0.151664\n",
      "Clustering   1523: ASW= 0.8314, DB= 0.2319, CH= 55304.7215\n",
      "Training epoch 1524, recon_loss:0.770709, zinb_loss:0.637679, cluster_loss:0.152053\n",
      "Clustering   1524: ASW= 0.8297, DB= 0.2327, CH= 55308.4663\n",
      "Training epoch 1525, recon_loss:0.771233, zinb_loss:0.638000, cluster_loss:0.151807\n",
      "Clustering   1525: ASW= 0.8315, DB= 0.2321, CH= 55393.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1526, recon_loss:0.771215, zinb_loss:0.637864, cluster_loss:0.152045\n",
      "Clustering   1526: ASW= 0.8298, DB= 0.2321, CH= 55411.1288\n",
      "Training epoch 1527, recon_loss:0.771298, zinb_loss:0.637991, cluster_loss:0.151889\n",
      "Clustering   1527: ASW= 0.8314, DB= 0.2329, CH= 55413.0791\n",
      "Training epoch 1528, recon_loss:0.770950, zinb_loss:0.638025, cluster_loss:0.151909\n",
      "Clustering   1528: ASW= 0.8301, DB= 0.2320, CH= 55562.8749\n",
      "Training epoch 1529, recon_loss:0.770981, zinb_loss:0.637977, cluster_loss:0.151831\n",
      "Clustering   1529: ASW= 0.8312, DB= 0.2336, CH= 55465.9097\n",
      "Training epoch 1530, recon_loss:0.770621, zinb_loss:0.638176, cluster_loss:0.151744\n",
      "Clustering   1530: ASW= 0.8304, DB= 0.2318, CH= 55688.3857\n",
      "Training epoch 1531, recon_loss:0.770783, zinb_loss:0.638020, cluster_loss:0.151758\n",
      "Clustering   1531: ASW= 0.8310, DB= 0.2338, CH= 55535.5689\n",
      "Training epoch 1532, recon_loss:0.770536, zinb_loss:0.638393, cluster_loss:0.151630\n",
      "Clustering   1532: ASW= 0.8306, DB= 0.2316, CH= 55773.6534\n",
      "Training epoch 1533, recon_loss:0.770799, zinb_loss:0.638155, cluster_loss:0.151754\n",
      "Clustering   1533: ASW= 0.8307, DB= 0.2342, CH= 55589.5773\n",
      "Training epoch 1534, recon_loss:0.770587, zinb_loss:0.638687, cluster_loss:0.151576\n",
      "Clustering   1534: ASW= 0.8309, DB= 0.2315, CH= 55829.8543\n",
      "Training epoch 1535, recon_loss:0.770948, zinb_loss:0.638373, cluster_loss:0.151876\n",
      "Clustering   1535: ASW= 0.8303, DB= 0.2349, CH= 55582.4980\n",
      "Training epoch 1536, recon_loss:0.770849, zinb_loss:0.639092, cluster_loss:0.151585\n",
      "Clustering   1536: ASW= 0.8312, DB= 0.2309, CH= 55791.2720\n",
      "Training epoch 1537, recon_loss:0.771306, zinb_loss:0.638676, cluster_loss:0.152169\n",
      "Clustering   1537: ASW= 0.8298, DB= 0.2356, CH= 55403.5113\n",
      "Training epoch 1538, recon_loss:0.771371, zinb_loss:0.639545, cluster_loss:0.151725\n",
      "Clustering   1538: ASW= 0.8313, DB= 0.2309, CH= 55528.1209\n",
      "Training epoch 1539, recon_loss:0.771778, zinb_loss:0.638989, cluster_loss:0.152549\n",
      "Clustering   1539: ASW= 0.8292, DB= 0.2366, CH= 55036.9383\n",
      "Training epoch 1540, recon_loss:0.771756, zinb_loss:0.639789, cluster_loss:0.151911\n",
      "Clustering   1540: ASW= 0.8311, DB= 0.2316, CH= 55153.9741\n",
      "Training epoch 1541, recon_loss:0.772131, zinb_loss:0.639164, cluster_loss:0.152642\n",
      "Clustering   1541: ASW= 0.8292, DB= 0.2355, CH= 54937.6862\n",
      "Training epoch 1542, recon_loss:0.771431, zinb_loss:0.639625, cluster_loss:0.151834\n",
      "Clustering   1542: ASW= 0.8311, DB= 0.2312, CH= 55145.5394\n",
      "Training epoch 1543, recon_loss:0.771500, zinb_loss:0.639088, cluster_loss:0.152285\n",
      "Clustering   1543: ASW= 0.8298, DB= 0.2343, CH= 55241.7947\n",
      "Training epoch 1544, recon_loss:0.770674, zinb_loss:0.639249, cluster_loss:0.151570\n",
      "Clustering   1544: ASW= 0.8313, DB= 0.2310, CH= 55408.8152\n",
      "Training epoch 1545, recon_loss:0.770898, zinb_loss:0.638904, cluster_loss:0.151900\n",
      "Clustering   1545: ASW= 0.8303, DB= 0.2334, CH= 55626.4227\n",
      "Training epoch 1546, recon_loss:0.770306, zinb_loss:0.638985, cluster_loss:0.151428\n",
      "Clustering   1546: ASW= 0.8311, DB= 0.2310, CH= 55601.3726\n",
      "Training epoch 1547, recon_loss:0.770606, zinb_loss:0.638939, cluster_loss:0.151700\n",
      "Clustering   1547: ASW= 0.8305, DB= 0.2327, CH= 55851.0166\n",
      "Training epoch 1548, recon_loss:0.770487, zinb_loss:0.638911, cluster_loss:0.151466\n",
      "Clustering   1548: ASW= 0.8307, DB= 0.2319, CH= 55570.6681\n",
      "Training epoch 1549, recon_loss:0.771068, zinb_loss:0.639106, cluster_loss:0.151669\n",
      "Clustering   1549: ASW= 0.8306, DB= 0.2327, CH= 56040.2589\n",
      "Training epoch 1550, recon_loss:0.771138, zinb_loss:0.638966, cluster_loss:0.151734\n",
      "Clustering   1550: ASW= 0.8299, DB= 0.2325, CH= 55352.5163\n",
      "Training epoch 1551, recon_loss:0.771838, zinb_loss:0.639509, cluster_loss:0.151808\n",
      "Clustering   1551: ASW= 0.8305, DB= 0.2318, CH= 56002.4530\n",
      "Training epoch 1552, recon_loss:0.772156, zinb_loss:0.639136, cluster_loss:0.152185\n",
      "Clustering   1552: ASW= 0.8288, DB= 0.2339, CH= 54941.6100\n",
      "Training epoch 1553, recon_loss:0.772611, zinb_loss:0.639795, cluster_loss:0.152047\n",
      "Clustering   1553: ASW= 0.8305, DB= 0.2306, CH= 55729.7692\n",
      "Training epoch 1554, recon_loss:0.772318, zinb_loss:0.639087, cluster_loss:0.152327\n",
      "Clustering   1554: ASW= 0.8287, DB= 0.2341, CH= 54800.6317\n",
      "Training epoch 1555, recon_loss:0.772329, zinb_loss:0.639584, cluster_loss:0.151976\n",
      "Clustering   1555: ASW= 0.8308, DB= 0.2304, CH= 55793.2278\n",
      "Training epoch 1556, recon_loss:0.771456, zinb_loss:0.638802, cluster_loss:0.152052\n",
      "Clustering   1556: ASW= 0.8296, DB= 0.2331, CH= 55100.3608\n",
      "Training epoch 1557, recon_loss:0.771473, zinb_loss:0.639037, cluster_loss:0.151742\n",
      "Clustering   1557: ASW= 0.8311, DB= 0.2305, CH= 56000.9700\n",
      "Training epoch 1558, recon_loss:0.770689, zinb_loss:0.638491, cluster_loss:0.151799\n",
      "Clustering   1558: ASW= 0.8303, DB= 0.2326, CH= 55408.2188\n",
      "Training epoch 1559, recon_loss:0.770940, zinb_loss:0.638621, cluster_loss:0.151601\n",
      "Clustering   1559: ASW= 0.8313, DB= 0.2307, CH= 56192.4526\n",
      "Training epoch 1560, recon_loss:0.770313, zinb_loss:0.638310, cluster_loss:0.151638\n",
      "Clustering   1560: ASW= 0.8309, DB= 0.2324, CH= 55675.6703\n",
      "Training epoch 1561, recon_loss:0.770726, zinb_loss:0.638355, cluster_loss:0.151600\n",
      "Clustering   1561: ASW= 0.8313, DB= 0.2308, CH= 56252.0157\n",
      "Training epoch 1562, recon_loss:0.770281, zinb_loss:0.638245, cluster_loss:0.151591\n",
      "Clustering   1562: ASW= 0.8313, DB= 0.2320, CH= 55857.3025\n",
      "Training epoch 1563, recon_loss:0.770865, zinb_loss:0.638207, cluster_loss:0.151738\n",
      "Clustering   1563: ASW= 0.8311, DB= 0.2310, CH= 56204.6150\n",
      "Training epoch 1564, recon_loss:0.770499, zinb_loss:0.638292, cluster_loss:0.151659\n",
      "Clustering   1564: ASW= 0.8316, DB= 0.2315, CH= 55950.7342\n",
      "Training epoch 1565, recon_loss:0.771067, zinb_loss:0.638133, cluster_loss:0.151944\n",
      "Clustering   1565: ASW= 0.8309, DB= 0.2310, CH= 56090.9688\n",
      "Training epoch 1566, recon_loss:0.770704, zinb_loss:0.638344, cluster_loss:0.151737\n",
      "Clustering   1566: ASW= 0.8317, DB= 0.2314, CH= 55990.6069\n",
      "Training epoch 1567, recon_loss:0.771079, zinb_loss:0.638106, cluster_loss:0.152058\n",
      "Clustering   1567: ASW= 0.8310, DB= 0.2309, CH= 56021.8463\n",
      "Training epoch 1568, recon_loss:0.770724, zinb_loss:0.638327, cluster_loss:0.151755\n",
      "Clustering   1568: ASW= 0.8317, DB= 0.2316, CH= 56030.4685\n",
      "Training epoch 1569, recon_loss:0.771030, zinb_loss:0.638079, cluster_loss:0.152140\n",
      "Clustering   1569: ASW= 0.8311, DB= 0.2306, CH= 55931.4126\n",
      "Training epoch 1570, recon_loss:0.770662, zinb_loss:0.638284, cluster_loss:0.151739\n",
      "Clustering   1570: ASW= 0.8315, DB= 0.2315, CH= 56019.6934\n",
      "Training epoch 1571, recon_loss:0.770933, zinb_loss:0.638106, cluster_loss:0.152130\n",
      "Clustering   1571: ASW= 0.8314, DB= 0.2306, CH= 55953.1675\n",
      "Training epoch 1572, recon_loss:0.770594, zinb_loss:0.638254, cluster_loss:0.151694\n",
      "Clustering   1572: ASW= 0.8313, DB= 0.2321, CH= 56049.7122\n",
      "Training epoch 1573, recon_loss:0.770832, zinb_loss:0.638156, cluster_loss:0.152134\n",
      "Clustering   1573: ASW= 0.8317, DB= 0.2301, CH= 55888.0989\n",
      "Training epoch 1574, recon_loss:0.770504, zinb_loss:0.638280, cluster_loss:0.151651\n",
      "Clustering   1574: ASW= 0.8310, DB= 0.2326, CH= 56044.7681\n",
      "Training epoch 1575, recon_loss:0.770850, zinb_loss:0.638431, cluster_loss:0.152044\n",
      "Clustering   1575: ASW= 0.8321, DB= 0.2294, CH= 55944.1099\n",
      "Training epoch 1576, recon_loss:0.770529, zinb_loss:0.638475, cluster_loss:0.151575\n",
      "Clustering   1576: ASW= 0.8308, DB= 0.2323, CH= 56118.4888\n",
      "Training epoch 1577, recon_loss:0.770770, zinb_loss:0.638614, cluster_loss:0.152052\n",
      "Clustering   1577: ASW= 0.8320, DB= 0.2299, CH= 55852.9902\n",
      "Training epoch 1578, recon_loss:0.770645, zinb_loss:0.638696, cluster_loss:0.151636\n",
      "Clustering   1578: ASW= 0.8303, DB= 0.2338, CH= 56111.0675\n",
      "Training epoch 1579, recon_loss:0.771273, zinb_loss:0.639163, cluster_loss:0.152047\n",
      "Clustering   1579: ASW= 0.8324, DB= 0.2298, CH= 55731.1507\n",
      "Training epoch 1580, recon_loss:0.771346, zinb_loss:0.639163, cluster_loss:0.151740\n",
      "Clustering   1580: ASW= 0.8300, DB= 0.2345, CH= 56011.9364\n",
      "Training epoch 1581, recon_loss:0.771802, zinb_loss:0.639657, cluster_loss:0.152176\n",
      "Clustering   1581: ASW= 0.8321, DB= 0.2304, CH= 55540.1757\n",
      "Training epoch 1582, recon_loss:0.772081, zinb_loss:0.639690, cluster_loss:0.152007\n",
      "Clustering   1582: ASW= 0.8301, DB= 0.2337, CH= 56093.6077\n",
      "Training epoch 1583, recon_loss:0.772565, zinb_loss:0.640191, cluster_loss:0.152392\n",
      "Clustering   1583: ASW= 0.8321, DB= 0.2301, CH= 55300.5527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1584, recon_loss:0.772544, zinb_loss:0.639764, cluster_loss:0.152314\n",
      "Clustering   1584: ASW= 0.8297, DB= 0.2344, CH= 55777.6551\n",
      "Training epoch 1585, recon_loss:0.772187, zinb_loss:0.639910, cluster_loss:0.152444\n",
      "Clustering   1585: ASW= 0.8319, DB= 0.2304, CH= 54962.6357\n",
      "Training epoch 1586, recon_loss:0.772646, zinb_loss:0.639876, cluster_loss:0.152833\n",
      "Clustering   1586: ASW= 0.8287, DB= 0.2346, CH= 55096.5719\n",
      "Training epoch 1587, recon_loss:0.774406, zinb_loss:0.640095, cluster_loss:0.153536\n",
      "Clustering   1587: ASW= 0.8288, DB= 0.2349, CH= 53272.9338\n",
      "Training epoch 1588, recon_loss:0.772588, zinb_loss:0.638809, cluster_loss:0.152257\n",
      "Clustering   1588: ASW= 0.8292, DB= 0.2360, CH= 54339.3411\n",
      "Training epoch 1589, recon_loss:0.771132, zinb_loss:0.638414, cluster_loss:0.152148\n",
      "Clustering   1589: ASW= 0.8299, DB= 0.2329, CH= 54761.9965\n",
      "Training epoch 1590, recon_loss:0.771052, zinb_loss:0.638576, cluster_loss:0.151766\n",
      "Clustering   1590: ASW= 0.8306, DB= 0.2332, CH= 55340.8002\n",
      "Training epoch 1591, recon_loss:0.770910, zinb_loss:0.638325, cluster_loss:0.152164\n",
      "Clustering   1591: ASW= 0.8301, DB= 0.2334, CH= 55193.5861\n",
      "Training epoch 1592, recon_loss:0.770916, zinb_loss:0.638664, cluster_loss:0.151878\n",
      "Clustering   1592: ASW= 0.8306, DB= 0.2325, CH= 55291.7209\n",
      "Training epoch 1593, recon_loss:0.771125, zinb_loss:0.638385, cluster_loss:0.152154\n",
      "Clustering   1593: ASW= 0.8305, DB= 0.2341, CH= 55634.9919\n",
      "Training epoch 1594, recon_loss:0.771097, zinb_loss:0.638666, cluster_loss:0.152061\n",
      "Clustering   1594: ASW= 0.8305, DB= 0.2316, CH= 55174.5046\n",
      "Training epoch 1595, recon_loss:0.771215, zinb_loss:0.638407, cluster_loss:0.152318\n",
      "Clustering   1595: ASW= 0.8305, DB= 0.2347, CH= 55799.7640\n",
      "Training epoch 1596, recon_loss:0.771186, zinb_loss:0.638574, cluster_loss:0.152400\n",
      "Clustering   1596: ASW= 0.8302, DB= 0.2313, CH= 54857.1873\n",
      "Training epoch 1597, recon_loss:0.771265, zinb_loss:0.638416, cluster_loss:0.152387\n",
      "Clustering   1597: ASW= 0.8309, DB= 0.2345, CH= 56151.9025\n",
      "Training epoch 1598, recon_loss:0.771154, zinb_loss:0.638394, cluster_loss:0.152710\n",
      "Clustering   1598: ASW= 0.8299, DB= 0.2313, CH= 54637.3756\n",
      "Training epoch 1599, recon_loss:0.770842, zinb_loss:0.638340, cluster_loss:0.152211\n",
      "Clustering   1599: ASW= 0.8313, DB= 0.2340, CH= 56393.1242\n",
      "Training epoch 1600, recon_loss:0.770623, zinb_loss:0.638154, cluster_loss:0.152608\n",
      "Clustering   1600: ASW= 0.8299, DB= 0.2314, CH= 54650.1162\n",
      "Training epoch 1601, recon_loss:0.770174, zinb_loss:0.638265, cluster_loss:0.151776\n",
      "Clustering   1601: ASW= 0.8319, DB= 0.2334, CH= 56620.1042\n",
      "Training epoch 1602, recon_loss:0.769887, zinb_loss:0.637980, cluster_loss:0.152255\n",
      "Clustering   1602: ASW= 0.8300, DB= 0.2318, CH= 54877.8790\n",
      "Training epoch 1603, recon_loss:0.769719, zinb_loss:0.638302, cluster_loss:0.151413\n",
      "Clustering   1603: ASW= 0.8324, DB= 0.2321, CH= 56808.8364\n",
      "Training epoch 1604, recon_loss:0.769485, zinb_loss:0.637986, cluster_loss:0.152039\n",
      "Clustering   1604: ASW= 0.8301, DB= 0.2322, CH= 55059.8098\n",
      "Training epoch 1605, recon_loss:0.769699, zinb_loss:0.638531, cluster_loss:0.151232\n",
      "Clustering   1605: ASW= 0.8327, DB= 0.2317, CH= 56951.5883\n",
      "Training epoch 1606, recon_loss:0.769619, zinb_loss:0.638235, cluster_loss:0.152007\n",
      "Clustering   1606: ASW= 0.8302, DB= 0.2322, CH= 55120.6269\n",
      "Training epoch 1607, recon_loss:0.770178, zinb_loss:0.638973, cluster_loss:0.151192\n",
      "Clustering   1607: ASW= 0.8329, DB= 0.2312, CH= 57040.0228\n",
      "Training epoch 1608, recon_loss:0.770262, zinb_loss:0.638647, cluster_loss:0.152109\n",
      "Clustering   1608: ASW= 0.8302, DB= 0.2324, CH= 55126.7691\n",
      "Training epoch 1609, recon_loss:0.770924, zinb_loss:0.639386, cluster_loss:0.151249\n",
      "Clustering   1609: ASW= 0.8328, DB= 0.2305, CH= 57006.8973\n",
      "Training epoch 1610, recon_loss:0.770989, zinb_loss:0.638963, cluster_loss:0.152174\n",
      "Clustering   1610: ASW= 0.8304, DB= 0.2324, CH= 55132.5569\n",
      "Training epoch 1611, recon_loss:0.771444, zinb_loss:0.639637, cluster_loss:0.151273\n",
      "Clustering   1611: ASW= 0.8328, DB= 0.2302, CH= 57061.1098\n",
      "Training epoch 1612, recon_loss:0.771460, zinb_loss:0.639066, cluster_loss:0.152188\n",
      "Clustering   1612: ASW= 0.8305, DB= 0.2326, CH= 55184.3293\n",
      "Training epoch 1613, recon_loss:0.771440, zinb_loss:0.639497, cluster_loss:0.151334\n",
      "Clustering   1613: ASW= 0.8323, DB= 0.2305, CH= 56865.8570\n",
      "Training epoch 1614, recon_loss:0.771446, zinb_loss:0.638826, cluster_loss:0.152046\n",
      "Clustering   1614: ASW= 0.8308, DB= 0.2322, CH= 55368.5911\n",
      "Training epoch 1615, recon_loss:0.771504, zinb_loss:0.639388, cluster_loss:0.151291\n",
      "Clustering   1615: ASW= 0.8324, DB= 0.2297, CH= 57013.2568\n",
      "Training epoch 1616, recon_loss:0.770887, zinb_loss:0.638787, cluster_loss:0.151875\n",
      "Clustering   1616: ASW= 0.8311, DB= 0.2326, CH= 55512.8815\n",
      "Training epoch 1617, recon_loss:0.770613, zinb_loss:0.639145, cluster_loss:0.151268\n",
      "Clustering   1617: ASW= 0.8325, DB= 0.2294, CH= 56900.2553\n",
      "Training epoch 1618, recon_loss:0.770505, zinb_loss:0.638645, cluster_loss:0.151667\n",
      "Clustering   1618: ASW= 0.8314, DB= 0.2318, CH= 55813.7164\n",
      "Training epoch 1619, recon_loss:0.770317, zinb_loss:0.638883, cluster_loss:0.151179\n",
      "Clustering   1619: ASW= 0.8324, DB= 0.2293, CH= 56919.6323\n",
      "Training epoch 1620, recon_loss:0.770200, zinb_loss:0.638511, cluster_loss:0.151517\n",
      "Clustering   1620: ASW= 0.8317, DB= 0.2314, CH= 56069.8930\n",
      "Training epoch 1621, recon_loss:0.770113, zinb_loss:0.638642, cluster_loss:0.151186\n",
      "Clustering   1621: ASW= 0.8323, DB= 0.2296, CH= 56930.2633\n",
      "Training epoch 1622, recon_loss:0.770017, zinb_loss:0.638426, cluster_loss:0.151388\n",
      "Clustering   1622: ASW= 0.8320, DB= 0.2310, CH= 56232.4816\n",
      "Training epoch 1623, recon_loss:0.769977, zinb_loss:0.638439, cluster_loss:0.151257\n",
      "Clustering   1623: ASW= 0.8320, DB= 0.2299, CH= 56857.4420\n",
      "Training epoch 1624, recon_loss:0.769949, zinb_loss:0.638444, cluster_loss:0.151286\n",
      "Clustering   1624: ASW= 0.8323, DB= 0.2304, CH= 56353.4079\n",
      "Training epoch 1625, recon_loss:0.770079, zinb_loss:0.638375, cluster_loss:0.151490\n",
      "Clustering   1625: ASW= 0.8317, DB= 0.2301, CH= 56680.9751\n",
      "Training epoch 1626, recon_loss:0.770317, zinb_loss:0.638654, cluster_loss:0.151407\n",
      "Clustering   1626: ASW= 0.8322, DB= 0.2302, CH= 56172.8841\n",
      "Training epoch 1627, recon_loss:0.770244, zinb_loss:0.638345, cluster_loss:0.151947\n",
      "Clustering   1627: ASW= 0.8310, DB= 0.2310, CH= 56174.6695\n",
      "Training epoch 1628, recon_loss:0.770653, zinb_loss:0.638865, cluster_loss:0.151724\n",
      "Clustering   1628: ASW= 0.8318, DB= 0.2301, CH= 55706.1215\n",
      "Training epoch 1629, recon_loss:0.770169, zinb_loss:0.638277, cluster_loss:0.152341\n",
      "Clustering   1629: ASW= 0.8306, DB= 0.2319, CH= 55708.7751\n",
      "Training epoch 1630, recon_loss:0.770492, zinb_loss:0.638816, cluster_loss:0.151771\n",
      "Clustering   1630: ASW= 0.8313, DB= 0.2304, CH= 55515.6856\n",
      "Training epoch 1631, recon_loss:0.770147, zinb_loss:0.638216, cluster_loss:0.152336\n",
      "Clustering   1631: ASW= 0.8306, DB= 0.2319, CH= 55574.4789\n",
      "Training epoch 1632, recon_loss:0.770352, zinb_loss:0.638617, cluster_loss:0.151528\n",
      "Clustering   1632: ASW= 0.8313, DB= 0.2303, CH= 55658.1852\n",
      "Training epoch 1633, recon_loss:0.770265, zinb_loss:0.638217, cluster_loss:0.152130\n",
      "Clustering   1633: ASW= 0.8309, DB= 0.2313, CH= 55737.8702\n",
      "Training epoch 1634, recon_loss:0.770345, zinb_loss:0.638450, cluster_loss:0.151301\n",
      "Clustering   1634: ASW= 0.8313, DB= 0.2305, CH= 55899.3523\n",
      "Training epoch 1635, recon_loss:0.770231, zinb_loss:0.638102, cluster_loss:0.151893\n",
      "Clustering   1635: ASW= 0.8311, DB= 0.2309, CH= 55949.0231\n",
      "Training epoch 1636, recon_loss:0.770250, zinb_loss:0.638202, cluster_loss:0.151211\n",
      "Clustering   1636: ASW= 0.8314, DB= 0.2306, CH= 56049.1601\n",
      "Training epoch 1637, recon_loss:0.770261, zinb_loss:0.638025, cluster_loss:0.151705\n",
      "Clustering   1637: ASW= 0.8314, DB= 0.2306, CH= 56126.1620\n",
      "Training epoch 1638, recon_loss:0.770425, zinb_loss:0.638042, cluster_loss:0.151154\n",
      "Clustering   1638: ASW= 0.8315, DB= 0.2305, CH= 56279.7242\n",
      "Training epoch 1639, recon_loss:0.770384, zinb_loss:0.637941, cluster_loss:0.151574\n",
      "Clustering   1639: ASW= 0.8314, DB= 0.2303, CH= 56224.6343\n",
      "Training epoch 1640, recon_loss:0.770651, zinb_loss:0.637913, cluster_loss:0.151214\n",
      "Clustering   1640: ASW= 0.8316, DB= 0.2307, CH= 56413.2112\n",
      "Training epoch 1641, recon_loss:0.770648, zinb_loss:0.638014, cluster_loss:0.151547\n",
      "Clustering   1641: ASW= 0.8315, DB= 0.2299, CH= 56204.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1642, recon_loss:0.771019, zinb_loss:0.637913, cluster_loss:0.151418\n",
      "Clustering   1642: ASW= 0.8315, DB= 0.2307, CH= 56456.4862\n",
      "Training epoch 1643, recon_loss:0.771226, zinb_loss:0.638253, cluster_loss:0.151690\n",
      "Clustering   1643: ASW= 0.8315, DB= 0.2292, CH= 56001.4068\n",
      "Training epoch 1644, recon_loss:0.771596, zinb_loss:0.637969, cluster_loss:0.151842\n",
      "Clustering   1644: ASW= 0.8311, DB= 0.2320, CH= 56307.7916\n",
      "Training epoch 1645, recon_loss:0.771955, zinb_loss:0.638502, cluster_loss:0.152037\n",
      "Clustering   1645: ASW= 0.8314, DB= 0.2301, CH= 55545.5936\n",
      "Training epoch 1646, recon_loss:0.771684, zinb_loss:0.637909, cluster_loss:0.152070\n",
      "Clustering   1646: ASW= 0.8310, DB= 0.2332, CH= 56178.3783\n",
      "Training epoch 1647, recon_loss:0.771474, zinb_loss:0.638323, cluster_loss:0.151933\n",
      "Clustering   1647: ASW= 0.8318, DB= 0.2298, CH= 55575.0934\n",
      "Training epoch 1648, recon_loss:0.770580, zinb_loss:0.637706, cluster_loss:0.151750\n",
      "Clustering   1648: ASW= 0.8315, DB= 0.2328, CH= 56385.2826\n",
      "Training epoch 1649, recon_loss:0.770398, zinb_loss:0.637975, cluster_loss:0.151566\n",
      "Clustering   1649: ASW= 0.8322, DB= 0.2284, CH= 55975.2145\n",
      "Training epoch 1650, recon_loss:0.769755, zinb_loss:0.637630, cluster_loss:0.151469\n",
      "Clustering   1650: ASW= 0.8320, DB= 0.2322, CH= 56680.0193\n",
      "Training epoch 1651, recon_loss:0.769968, zinb_loss:0.637812, cluster_loss:0.151480\n",
      "Clustering   1651: ASW= 0.8323, DB= 0.2286, CH= 56212.9997\n",
      "Training epoch 1652, recon_loss:0.769633, zinb_loss:0.637684, cluster_loss:0.151403\n",
      "Clustering   1652: ASW= 0.8324, DB= 0.2319, CH= 56886.3414\n",
      "Training epoch 1653, recon_loss:0.770028, zinb_loss:0.637795, cluster_loss:0.151606\n",
      "Clustering   1653: ASW= 0.8322, DB= 0.2288, CH= 56259.5148\n",
      "Training epoch 1654, recon_loss:0.769829, zinb_loss:0.637825, cluster_loss:0.151461\n",
      "Clustering   1654: ASW= 0.8325, DB= 0.2321, CH= 57001.7429\n",
      "Training epoch 1655, recon_loss:0.770307, zinb_loss:0.637875, cluster_loss:0.151868\n",
      "Clustering   1655: ASW= 0.8320, DB= 0.2295, CH= 56193.8020\n",
      "Training epoch 1656, recon_loss:0.770046, zinb_loss:0.638021, cluster_loss:0.151562\n",
      "Clustering   1656: ASW= 0.8324, DB= 0.2320, CH= 56950.8587\n",
      "Training epoch 1657, recon_loss:0.770651, zinb_loss:0.638036, cluster_loss:0.152069\n",
      "Clustering   1657: ASW= 0.8320, DB= 0.2303, CH= 56129.7716\n",
      "Training epoch 1658, recon_loss:0.770301, zinb_loss:0.638252, cluster_loss:0.151610\n",
      "Clustering   1658: ASW= 0.8321, DB= 0.2320, CH= 56744.8793\n",
      "Training epoch 1659, recon_loss:0.770926, zinb_loss:0.638196, cluster_loss:0.152136\n",
      "Clustering   1659: ASW= 0.8320, DB= 0.2303, CH= 56090.4722\n",
      "Training epoch 1660, recon_loss:0.770471, zinb_loss:0.638473, cluster_loss:0.151543\n",
      "Clustering   1660: ASW= 0.8319, DB= 0.2315, CH= 56618.5035\n",
      "Training epoch 1661, recon_loss:0.770879, zinb_loss:0.638309, cluster_loss:0.152063\n",
      "Clustering   1661: ASW= 0.8321, DB= 0.2303, CH= 56100.3900\n",
      "Training epoch 1662, recon_loss:0.770461, zinb_loss:0.638506, cluster_loss:0.151464\n",
      "Clustering   1662: ASW= 0.8316, DB= 0.2314, CH= 56370.6700\n",
      "Training epoch 1663, recon_loss:0.770582, zinb_loss:0.638233, cluster_loss:0.151930\n",
      "Clustering   1663: ASW= 0.8321, DB= 0.2301, CH= 56113.8545\n",
      "Training epoch 1664, recon_loss:0.770511, zinb_loss:0.638428, cluster_loss:0.151274\n",
      "Clustering   1664: ASW= 0.8319, DB= 0.2315, CH= 56462.8819\n",
      "Training epoch 1665, recon_loss:0.770450, zinb_loss:0.638075, cluster_loss:0.151736\n",
      "Clustering   1665: ASW= 0.8322, DB= 0.2300, CH= 56292.1795\n",
      "Training epoch 1666, recon_loss:0.770594, zinb_loss:0.638202, cluster_loss:0.151203\n",
      "Clustering   1666: ASW= 0.8320, DB= 0.2313, CH= 56472.2717\n",
      "Training epoch 1667, recon_loss:0.770641, zinb_loss:0.637881, cluster_loss:0.151679\n",
      "Clustering   1667: ASW= 0.8321, DB= 0.2299, CH= 56329.6259\n",
      "Training epoch 1668, recon_loss:0.770840, zinb_loss:0.638069, cluster_loss:0.151160\n",
      "Clustering   1668: ASW= 0.8324, DB= 0.2307, CH= 56644.0672\n",
      "Training epoch 1669, recon_loss:0.770923, zinb_loss:0.637721, cluster_loss:0.151718\n",
      "Clustering   1669: ASW= 0.8319, DB= 0.2298, CH= 56333.1850\n",
      "Training epoch 1670, recon_loss:0.770975, zinb_loss:0.637929, cluster_loss:0.151236\n",
      "Clustering   1670: ASW= 0.8326, DB= 0.2303, CH= 56670.7962\n",
      "Training epoch 1671, recon_loss:0.770966, zinb_loss:0.637559, cluster_loss:0.151800\n",
      "Clustering   1671: ASW= 0.8316, DB= 0.2301, CH= 56242.8785\n",
      "Training epoch 1672, recon_loss:0.771193, zinb_loss:0.637922, cluster_loss:0.151271\n",
      "Clustering   1672: ASW= 0.8332, DB= 0.2299, CH= 56861.3801\n",
      "Training epoch 1673, recon_loss:0.770859, zinb_loss:0.637494, cluster_loss:0.151857\n",
      "Clustering   1673: ASW= 0.8313, DB= 0.2308, CH= 56236.5120\n",
      "Training epoch 1674, recon_loss:0.771239, zinb_loss:0.637926, cluster_loss:0.151389\n",
      "Clustering   1674: ASW= 0.8333, DB= 0.2299, CH= 56789.3649\n",
      "Training epoch 1675, recon_loss:0.770702, zinb_loss:0.637491, cluster_loss:0.151864\n",
      "Clustering   1675: ASW= 0.8309, DB= 0.2310, CH= 56188.6124\n",
      "Training epoch 1676, recon_loss:0.771117, zinb_loss:0.637972, cluster_loss:0.151389\n",
      "Clustering   1676: ASW= 0.8337, DB= 0.2294, CH= 56937.3040\n",
      "Training epoch 1677, recon_loss:0.770281, zinb_loss:0.637484, cluster_loss:0.151749\n",
      "Clustering   1677: ASW= 0.8309, DB= 0.2313, CH= 56328.4641\n",
      "Training epoch 1678, recon_loss:0.770687, zinb_loss:0.637886, cluster_loss:0.151421\n",
      "Clustering   1678: ASW= 0.8336, DB= 0.2296, CH= 56861.3095\n",
      "Training epoch 1679, recon_loss:0.770022, zinb_loss:0.637497, cluster_loss:0.151592\n",
      "Clustering   1679: ASW= 0.8310, DB= 0.2308, CH= 56434.3613\n",
      "Training epoch 1680, recon_loss:0.770383, zinb_loss:0.637826, cluster_loss:0.151392\n",
      "Clustering   1680: ASW= 0.8337, DB= 0.2292, CH= 56966.7895\n",
      "Training epoch 1681, recon_loss:0.769784, zinb_loss:0.637552, cluster_loss:0.151421\n",
      "Clustering   1681: ASW= 0.8312, DB= 0.2306, CH= 56619.6128\n",
      "Training epoch 1682, recon_loss:0.770288, zinb_loss:0.637766, cluster_loss:0.151541\n",
      "Clustering   1682: ASW= 0.8335, DB= 0.2295, CH= 56924.8402\n",
      "Training epoch 1683, recon_loss:0.770226, zinb_loss:0.637787, cluster_loss:0.151453\n",
      "Clustering   1683: ASW= 0.8313, DB= 0.2303, CH= 56495.9768\n",
      "Training epoch 1684, recon_loss:0.771132, zinb_loss:0.637815, cluster_loss:0.152191\n",
      "Clustering   1684: ASW= 0.8328, DB= 0.2306, CH= 56423.9918\n",
      "Training epoch 1685, recon_loss:0.771994, zinb_loss:0.638307, cluster_loss:0.152156\n",
      "Clustering   1685: ASW= 0.8307, DB= 0.2324, CH= 55563.3971\n",
      "Training epoch 1686, recon_loss:0.772044, zinb_loss:0.637759, cluster_loss:0.152744\n",
      "Clustering   1686: ASW= 0.8322, DB= 0.2312, CH= 55814.7295\n",
      "Training epoch 1687, recon_loss:0.771631, zinb_loss:0.638233, cluster_loss:0.152089\n",
      "Clustering   1687: ASW= 0.8310, DB= 0.2320, CH= 55516.9378\n",
      "Training epoch 1688, recon_loss:0.770501, zinb_loss:0.637587, cluster_loss:0.152068\n",
      "Clustering   1688: ASW= 0.8326, DB= 0.2315, CH= 55958.3464\n",
      "Training epoch 1689, recon_loss:0.770112, zinb_loss:0.638003, cluster_loss:0.151271\n",
      "Clustering   1689: ASW= 0.8319, DB= 0.2301, CH= 56400.0686\n",
      "Training epoch 1690, recon_loss:0.770101, zinb_loss:0.637936, cluster_loss:0.151650\n",
      "Clustering   1690: ASW= 0.8329, DB= 0.2309, CH= 56344.1191\n",
      "Training epoch 1691, recon_loss:0.770264, zinb_loss:0.638606, cluster_loss:0.151116\n",
      "Clustering   1691: ASW= 0.8323, DB= 0.2291, CH= 56820.6809\n",
      "Training epoch 1692, recon_loss:0.770479, zinb_loss:0.638903, cluster_loss:0.151597\n",
      "Clustering   1692: ASW= 0.8330, DB= 0.2303, CH= 56578.8421\n",
      "Training epoch 1693, recon_loss:0.770542, zinb_loss:0.639426, cluster_loss:0.151177\n",
      "Clustering   1693: ASW= 0.8326, DB= 0.2289, CH= 57008.1041\n",
      "Training epoch 1694, recon_loss:0.770972, zinb_loss:0.639729, cluster_loss:0.151617\n",
      "Clustering   1694: ASW= 0.8332, DB= 0.2300, CH= 56694.0766\n",
      "Training epoch 1695, recon_loss:0.770528, zinb_loss:0.639676, cluster_loss:0.151270\n",
      "Clustering   1695: ASW= 0.8324, DB= 0.2298, CH= 57126.2269\n",
      "Training epoch 1696, recon_loss:0.771007, zinb_loss:0.639874, cluster_loss:0.151655\n",
      "Clustering   1696: ASW= 0.8331, DB= 0.2300, CH= 56673.3355\n",
      "Training epoch 1697, recon_loss:0.770333, zinb_loss:0.639599, cluster_loss:0.151356\n",
      "Clustering   1697: ASW= 0.8321, DB= 0.2306, CH= 57051.2279\n",
      "Training epoch 1698, recon_loss:0.770806, zinb_loss:0.639687, cluster_loss:0.151559\n",
      "Clustering   1698: ASW= 0.8332, DB= 0.2294, CH= 56633.1917\n",
      "Training epoch 1699, recon_loss:0.769994, zinb_loss:0.639262, cluster_loss:0.151346\n",
      "Clustering   1699: ASW= 0.8321, DB= 0.2307, CH= 57105.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1700, recon_loss:0.770634, zinb_loss:0.639445, cluster_loss:0.151503\n",
      "Clustering   1700: ASW= 0.8332, DB= 0.2289, CH= 56669.0357\n",
      "Training epoch 1701, recon_loss:0.770062, zinb_loss:0.639138, cluster_loss:0.151367\n",
      "Clustering   1701: ASW= 0.8320, DB= 0.2313, CH= 56996.3163\n",
      "Training epoch 1702, recon_loss:0.770829, zinb_loss:0.639356, cluster_loss:0.151424\n",
      "Clustering   1702: ASW= 0.8332, DB= 0.2287, CH= 56650.1569\n",
      "Training epoch 1703, recon_loss:0.769805, zinb_loss:0.638908, cluster_loss:0.151425\n",
      "Clustering   1703: ASW= 0.8321, DB= 0.2312, CH= 57078.2940\n",
      "Training epoch 1704, recon_loss:0.770250, zinb_loss:0.639028, cluster_loss:0.151342\n",
      "Clustering   1704: ASW= 0.8334, DB= 0.2282, CH= 56793.4144\n",
      "Training epoch 1705, recon_loss:0.769636, zinb_loss:0.638806, cluster_loss:0.151362\n",
      "Clustering   1705: ASW= 0.8324, DB= 0.2308, CH= 57100.0732\n",
      "Training epoch 1706, recon_loss:0.769985, zinb_loss:0.638849, cluster_loss:0.151314\n",
      "Clustering   1706: ASW= 0.8331, DB= 0.2282, CH= 56880.8331\n",
      "Training epoch 1707, recon_loss:0.769330, zinb_loss:0.638541, cluster_loss:0.151332\n",
      "Clustering   1707: ASW= 0.8326, DB= 0.2304, CH= 57130.8785\n",
      "Training epoch 1708, recon_loss:0.769784, zinb_loss:0.638581, cluster_loss:0.151314\n",
      "Clustering   1708: ASW= 0.8331, DB= 0.2280, CH= 57031.5683\n",
      "Training epoch 1709, recon_loss:0.769406, zinb_loss:0.638501, cluster_loss:0.151261\n",
      "Clustering   1709: ASW= 0.8329, DB= 0.2300, CH= 57198.2714\n",
      "Training epoch 1710, recon_loss:0.769880, zinb_loss:0.638460, cluster_loss:0.151363\n",
      "Clustering   1710: ASW= 0.8329, DB= 0.2280, CH= 57084.8855\n",
      "Training epoch 1711, recon_loss:0.769460, zinb_loss:0.638385, cluster_loss:0.151275\n",
      "Clustering   1711: ASW= 0.8332, DB= 0.2294, CH= 57246.5715\n",
      "Training epoch 1712, recon_loss:0.770049, zinb_loss:0.638273, cluster_loss:0.151533\n",
      "Clustering   1712: ASW= 0.8326, DB= 0.2282, CH= 57143.1718\n",
      "Training epoch 1713, recon_loss:0.769705, zinb_loss:0.638414, cluster_loss:0.151364\n",
      "Clustering   1713: ASW= 0.8334, DB= 0.2291, CH= 57171.6516\n",
      "Training epoch 1714, recon_loss:0.770517, zinb_loss:0.638179, cluster_loss:0.151825\n",
      "Clustering   1714: ASW= 0.8321, DB= 0.2291, CH= 57043.0790\n",
      "Training epoch 1715, recon_loss:0.770015, zinb_loss:0.638460, cluster_loss:0.151515\n",
      "Clustering   1715: ASW= 0.8334, DB= 0.2290, CH= 56924.0429\n",
      "Training epoch 1716, recon_loss:0.770772, zinb_loss:0.638005, cluster_loss:0.152181\n",
      "Clustering   1716: ASW= 0.8314, DB= 0.2302, CH= 56686.8663\n",
      "Training epoch 1717, recon_loss:0.770111, zinb_loss:0.638551, cluster_loss:0.151551\n",
      "Clustering   1717: ASW= 0.8334, DB= 0.2287, CH= 56620.7630\n",
      "Training epoch 1718, recon_loss:0.770507, zinb_loss:0.637865, cluster_loss:0.152153\n",
      "Clustering   1718: ASW= 0.8311, DB= 0.2310, CH= 56495.2593\n",
      "Training epoch 1719, recon_loss:0.769865, zinb_loss:0.638334, cluster_loss:0.151459\n",
      "Clustering   1719: ASW= 0.8329, DB= 0.2287, CH= 56317.1874\n",
      "Training epoch 1720, recon_loss:0.769964, zinb_loss:0.637633, cluster_loss:0.151938\n",
      "Clustering   1720: ASW= 0.8311, DB= 0.2311, CH= 56394.1188\n",
      "Training epoch 1721, recon_loss:0.769569, zinb_loss:0.638262, cluster_loss:0.151122\n",
      "Clustering   1721: ASW= 0.8332, DB= 0.2280, CH= 56629.8850\n",
      "Training epoch 1722, recon_loss:0.769718, zinb_loss:0.637625, cluster_loss:0.151770\n",
      "Clustering   1722: ASW= 0.8311, DB= 0.2314, CH= 56326.3005\n",
      "Training epoch 1723, recon_loss:0.769609, zinb_loss:0.638222, cluster_loss:0.150998\n",
      "Clustering   1723: ASW= 0.8331, DB= 0.2283, CH= 56805.4043\n",
      "Training epoch 1724, recon_loss:0.769947, zinb_loss:0.637663, cluster_loss:0.151817\n",
      "Clustering   1724: ASW= 0.8311, DB= 0.2311, CH= 56187.3206\n",
      "Training epoch 1725, recon_loss:0.770364, zinb_loss:0.638448, cluster_loss:0.150989\n",
      "Clustering   1725: ASW= 0.8332, DB= 0.2280, CH= 57065.2750\n",
      "Training epoch 1726, recon_loss:0.770563, zinb_loss:0.637837, cluster_loss:0.151957\n",
      "Clustering   1726: ASW= 0.8312, DB= 0.2313, CH= 56024.2940\n",
      "Training epoch 1727, recon_loss:0.770891, zinb_loss:0.638546, cluster_loss:0.151130\n",
      "Clustering   1727: ASW= 0.8330, DB= 0.2278, CH= 57181.2366\n",
      "Training epoch 1728, recon_loss:0.770853, zinb_loss:0.637892, cluster_loss:0.152091\n",
      "Clustering   1728: ASW= 0.8314, DB= 0.2312, CH= 55919.7538\n",
      "Training epoch 1729, recon_loss:0.770997, zinb_loss:0.638440, cluster_loss:0.151259\n",
      "Clustering   1729: ASW= 0.8331, DB= 0.2281, CH= 57250.8287\n",
      "Training epoch 1730, recon_loss:0.770641, zinb_loss:0.637790, cluster_loss:0.151974\n",
      "Clustering   1730: ASW= 0.8318, DB= 0.2308, CH= 56059.5386\n",
      "Training epoch 1731, recon_loss:0.770615, zinb_loss:0.638202, cluster_loss:0.151227\n",
      "Clustering   1731: ASW= 0.8332, DB= 0.2284, CH= 57414.6606\n",
      "Training epoch 1732, recon_loss:0.770219, zinb_loss:0.637669, cluster_loss:0.151714\n",
      "Clustering   1732: ASW= 0.8324, DB= 0.2305, CH= 56335.8564\n",
      "Training epoch 1733, recon_loss:0.770168, zinb_loss:0.637949, cluster_loss:0.151188\n",
      "Clustering   1733: ASW= 0.8333, DB= 0.2281, CH= 57458.7569\n",
      "Training epoch 1734, recon_loss:0.770043, zinb_loss:0.637600, cluster_loss:0.151509\n",
      "Clustering   1734: ASW= 0.8327, DB= 0.2304, CH= 56593.9155\n",
      "Training epoch 1735, recon_loss:0.770049, zinb_loss:0.637762, cluster_loss:0.151293\n",
      "Clustering   1735: ASW= 0.8332, DB= 0.2284, CH= 57320.9731\n",
      "Training epoch 1736, recon_loss:0.770272, zinb_loss:0.637587, cluster_loss:0.151511\n",
      "Clustering   1736: ASW= 0.8328, DB= 0.2308, CH= 56621.0243\n",
      "Training epoch 1737, recon_loss:0.770246, zinb_loss:0.637596, cluster_loss:0.151601\n",
      "Clustering   1737: ASW= 0.8329, DB= 0.2288, CH= 56922.6867\n",
      "Training epoch 1738, recon_loss:0.770538, zinb_loss:0.637534, cluster_loss:0.151515\n",
      "Clustering   1738: ASW= 0.8326, DB= 0.2308, CH= 56589.5566\n",
      "Training epoch 1739, recon_loss:0.770084, zinb_loss:0.637317, cluster_loss:0.151801\n",
      "Clustering   1739: ASW= 0.8328, DB= 0.2293, CH= 56611.3928\n",
      "Training epoch 1740, recon_loss:0.770415, zinb_loss:0.637355, cluster_loss:0.151493\n",
      "Clustering   1740: ASW= 0.8325, DB= 0.2309, CH= 56695.3959\n",
      "Training epoch 1741, recon_loss:0.769763, zinb_loss:0.637174, cluster_loss:0.151845\n",
      "Clustering   1741: ASW= 0.8325, DB= 0.2303, CH= 56309.4346\n",
      "Training epoch 1742, recon_loss:0.770074, zinb_loss:0.637317, cluster_loss:0.151378\n",
      "Clustering   1742: ASW= 0.8324, DB= 0.2310, CH= 56820.0197\n",
      "Training epoch 1743, recon_loss:0.770198, zinb_loss:0.637345, cluster_loss:0.152173\n",
      "Clustering   1743: ASW= 0.8328, DB= 0.2294, CH= 56339.2180\n",
      "Training epoch 1744, recon_loss:0.770368, zinb_loss:0.637496, cluster_loss:0.151252\n",
      "Clustering   1744: ASW= 0.8327, DB= 0.2304, CH= 57002.2047\n",
      "Training epoch 1745, recon_loss:0.769823, zinb_loss:0.637283, cluster_loss:0.151761\n",
      "Clustering   1745: ASW= 0.8331, DB= 0.2289, CH= 56564.1329\n",
      "Training epoch 1746, recon_loss:0.770033, zinb_loss:0.637492, cluster_loss:0.151111\n",
      "Clustering   1746: ASW= 0.8330, DB= 0.2296, CH= 57157.3011\n",
      "Training epoch 1747, recon_loss:0.769821, zinb_loss:0.637243, cluster_loss:0.151610\n",
      "Clustering   1747: ASW= 0.8331, DB= 0.2298, CH= 56690.4776\n",
      "Training epoch 1748, recon_loss:0.770299, zinb_loss:0.637612, cluster_loss:0.151100\n",
      "Clustering   1748: ASW= 0.8330, DB= 0.2299, CH= 57131.9893\n",
      "Training epoch 1749, recon_loss:0.770294, zinb_loss:0.637496, cluster_loss:0.151671\n",
      "Clustering   1749: ASW= 0.8331, DB= 0.2295, CH= 56774.1517\n",
      "Training epoch 1750, recon_loss:0.770533, zinb_loss:0.637789, cluster_loss:0.151096\n",
      "Clustering   1750: ASW= 0.8331, DB= 0.2288, CH= 57116.1055\n",
      "Training epoch 1751, recon_loss:0.770181, zinb_loss:0.637480, cluster_loss:0.151518\n",
      "Clustering   1751: ASW= 0.8332, DB= 0.2303, CH= 56962.0872\n",
      "Training epoch 1752, recon_loss:0.770406, zinb_loss:0.637840, cluster_loss:0.151025\n",
      "Clustering   1752: ASW= 0.8335, DB= 0.2282, CH= 57255.2126\n",
      "Training epoch 1753, recon_loss:0.770021, zinb_loss:0.637445, cluster_loss:0.151429\n",
      "Clustering   1753: ASW= 0.8332, DB= 0.2302, CH= 57098.0949\n",
      "Training epoch 1754, recon_loss:0.770175, zinb_loss:0.637786, cluster_loss:0.151080\n",
      "Clustering   1754: ASW= 0.8334, DB= 0.2281, CH= 57189.4197\n",
      "Training epoch 1755, recon_loss:0.769785, zinb_loss:0.637417, cluster_loss:0.151466\n",
      "Clustering   1755: ASW= 0.8332, DB= 0.2310, CH= 57184.8633\n",
      "Training epoch 1756, recon_loss:0.770161, zinb_loss:0.637993, cluster_loss:0.151084\n",
      "Clustering   1756: ASW= 0.8340, DB= 0.2275, CH= 57250.2176\n",
      "Training epoch 1757, recon_loss:0.769760, zinb_loss:0.637576, cluster_loss:0.151561\n",
      "Clustering   1757: ASW= 0.8328, DB= 0.2314, CH= 57070.2410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1758, recon_loss:0.770159, zinb_loss:0.638231, cluster_loss:0.151252\n",
      "Clustering   1758: ASW= 0.8340, DB= 0.2270, CH= 57232.0298\n",
      "Training epoch 1759, recon_loss:0.769833, zinb_loss:0.637822, cluster_loss:0.151714\n",
      "Clustering   1759: ASW= 0.8326, DB= 0.2323, CH= 57053.3032\n",
      "Training epoch 1760, recon_loss:0.770333, zinb_loss:0.638551, cluster_loss:0.151379\n",
      "Clustering   1760: ASW= 0.8341, DB= 0.2268, CH= 57234.4912\n",
      "Training epoch 1761, recon_loss:0.769898, zinb_loss:0.638173, cluster_loss:0.151759\n",
      "Clustering   1761: ASW= 0.8323, DB= 0.2324, CH= 56976.8936\n",
      "Training epoch 1762, recon_loss:0.770391, zinb_loss:0.639049, cluster_loss:0.151395\n",
      "Clustering   1762: ASW= 0.8343, DB= 0.2267, CH= 57299.4520\n",
      "Training epoch 1763, recon_loss:0.769949, zinb_loss:0.638796, cluster_loss:0.151667\n",
      "Clustering   1763: ASW= 0.8323, DB= 0.2325, CH= 56973.2918\n",
      "Training epoch 1764, recon_loss:0.770345, zinb_loss:0.639431, cluster_loss:0.151456\n",
      "Clustering   1764: ASW= 0.8340, DB= 0.2266, CH= 57388.6577\n",
      "Training epoch 1765, recon_loss:0.770261, zinb_loss:0.639231, cluster_loss:0.151697\n",
      "Clustering   1765: ASW= 0.8320, DB= 0.2329, CH= 56778.8489\n",
      "Training epoch 1766, recon_loss:0.770710, zinb_loss:0.639908, cluster_loss:0.151413\n",
      "Clustering   1766: ASW= 0.8341, DB= 0.2269, CH= 57494.5579\n",
      "Training epoch 1767, recon_loss:0.770630, zinb_loss:0.639695, cluster_loss:0.151603\n",
      "Clustering   1767: ASW= 0.8322, DB= 0.2324, CH= 56739.6303\n",
      "Training epoch 1768, recon_loss:0.770541, zinb_loss:0.639980, cluster_loss:0.151394\n",
      "Clustering   1768: ASW= 0.8338, DB= 0.2271, CH= 57634.3328\n",
      "Training epoch 1769, recon_loss:0.770442, zinb_loss:0.639657, cluster_loss:0.151602\n",
      "Clustering   1769: ASW= 0.8320, DB= 0.2320, CH= 56620.7073\n",
      "Training epoch 1770, recon_loss:0.770532, zinb_loss:0.640203, cluster_loss:0.151285\n",
      "Clustering   1770: ASW= 0.8343, DB= 0.2272, CH= 57750.3631\n",
      "Training epoch 1771, recon_loss:0.770319, zinb_loss:0.639763, cluster_loss:0.151466\n",
      "Clustering   1771: ASW= 0.8322, DB= 0.2316, CH= 56602.7831\n",
      "Training epoch 1772, recon_loss:0.770424, zinb_loss:0.640182, cluster_loss:0.151231\n",
      "Clustering   1772: ASW= 0.8341, DB= 0.2275, CH= 58029.9758\n",
      "Training epoch 1773, recon_loss:0.770311, zinb_loss:0.639737, cluster_loss:0.151441\n",
      "Clustering   1773: ASW= 0.8327, DB= 0.2306, CH= 56778.3773\n",
      "Training epoch 1774, recon_loss:0.770355, zinb_loss:0.640149, cluster_loss:0.151353\n",
      "Clustering   1774: ASW= 0.8341, DB= 0.2275, CH= 57996.2578\n",
      "Training epoch 1775, recon_loss:0.770441, zinb_loss:0.639817, cluster_loss:0.151614\n",
      "Clustering   1775: ASW= 0.8319, DB= 0.2310, CH= 56476.7423\n",
      "Training epoch 1776, recon_loss:0.770646, zinb_loss:0.640112, cluster_loss:0.151457\n",
      "Clustering   1776: ASW= 0.8341, DB= 0.2278, CH= 57699.5953\n",
      "Training epoch 1777, recon_loss:0.770432, zinb_loss:0.639699, cluster_loss:0.151544\n",
      "Clustering   1777: ASW= 0.8330, DB= 0.2292, CH= 57002.5827\n",
      "Training epoch 1778, recon_loss:0.770038, zinb_loss:0.639744, cluster_loss:0.151588\n",
      "Clustering   1778: ASW= 0.8336, DB= 0.2285, CH= 57637.0496\n",
      "Training epoch 1779, recon_loss:0.770440, zinb_loss:0.639646, cluster_loss:0.151640\n",
      "Clustering   1779: ASW= 0.8331, DB= 0.2285, CH= 56877.1449\n",
      "Training epoch 1780, recon_loss:0.770247, zinb_loss:0.639570, cluster_loss:0.151700\n",
      "Clustering   1780: ASW= 0.8335, DB= 0.2289, CH= 57609.6320\n",
      "Training epoch 1781, recon_loss:0.770928, zinb_loss:0.639619, cluster_loss:0.151887\n",
      "Clustering   1781: ASW= 0.8328, DB= 0.2293, CH= 56657.7491\n",
      "Training epoch 1782, recon_loss:0.770621, zinb_loss:0.639222, cluster_loss:0.152202\n",
      "Clustering   1782: ASW= 0.8321, DB= 0.2299, CH= 56966.8947\n",
      "Training epoch 1783, recon_loss:0.770900, zinb_loss:0.639294, cluster_loss:0.151900\n",
      "Clustering   1783: ASW= 0.8328, DB= 0.2285, CH= 56514.4549\n",
      "Training epoch 1784, recon_loss:0.770164, zinb_loss:0.638671, cluster_loss:0.151982\n",
      "Clustering   1784: ASW= 0.8327, DB= 0.2290, CH= 57147.4709\n",
      "Training epoch 1785, recon_loss:0.769959, zinb_loss:0.638587, cluster_loss:0.151582\n",
      "Clustering   1785: ASW= 0.8330, DB= 0.2290, CH= 56745.4120\n",
      "Training epoch 1786, recon_loss:0.769380, zinb_loss:0.638118, cluster_loss:0.151437\n",
      "Clustering   1786: ASW= 0.8332, DB= 0.2287, CH= 57358.9526\n",
      "Training epoch 1787, recon_loss:0.769625, zinb_loss:0.638470, cluster_loss:0.151796\n",
      "Clustering   1787: ASW= 0.8322, DB= 0.2295, CH= 56728.0998\n",
      "Training epoch 1788, recon_loss:0.770239, zinb_loss:0.638301, cluster_loss:0.151735\n",
      "Clustering   1788: ASW= 0.8324, DB= 0.2307, CH= 56012.7127\n",
      "Training epoch 1789, recon_loss:0.769760, zinb_loss:0.637721, cluster_loss:0.151679\n",
      "Clustering   1789: ASW= 0.8326, DB= 0.2295, CH= 56793.4254\n",
      "Training epoch 1790, recon_loss:0.769888, zinb_loss:0.638009, cluster_loss:0.151400\n",
      "Clustering   1790: ASW= 0.8335, DB= 0.2290, CH= 56807.9043\n",
      "Training epoch 1791, recon_loss:0.769793, zinb_loss:0.637625, cluster_loss:0.151874\n",
      "Clustering   1791: ASW= 0.8322, DB= 0.2299, CH= 56572.6451\n",
      "Training epoch 1792, recon_loss:0.770219, zinb_loss:0.638021, cluster_loss:0.151619\n",
      "Clustering   1792: ASW= 0.8338, DB= 0.2296, CH= 56804.5054\n",
      "Training epoch 1793, recon_loss:0.769529, zinb_loss:0.637267, cluster_loss:0.151472\n",
      "Clustering   1793: ASW= 0.8316, DB= 0.2316, CH= 56491.5108\n",
      "Training epoch 1794, recon_loss:0.769446, zinb_loss:0.637440, cluster_loss:0.151020\n",
      "Clustering   1794: ASW= 0.8333, DB= 0.2289, CH= 57083.6928\n",
      "Training epoch 1795, recon_loss:0.769803, zinb_loss:0.637147, cluster_loss:0.151593\n",
      "Clustering   1795: ASW= 0.8320, DB= 0.2318, CH= 56638.8992\n",
      "Training epoch 1796, recon_loss:0.770086, zinb_loss:0.637549, cluster_loss:0.151097\n",
      "Clustering   1796: ASW= 0.8333, DB= 0.2288, CH= 57081.3641\n",
      "Training epoch 1797, recon_loss:0.769904, zinb_loss:0.637227, cluster_loss:0.151778\n",
      "Clustering   1797: ASW= 0.8326, DB= 0.2309, CH= 56842.8822\n",
      "Training epoch 1798, recon_loss:0.770386, zinb_loss:0.637652, cluster_loss:0.151235\n",
      "Clustering   1798: ASW= 0.8337, DB= 0.2288, CH= 57250.7113\n",
      "Training epoch 1799, recon_loss:0.770231, zinb_loss:0.637427, cluster_loss:0.152110\n",
      "Clustering   1799: ASW= 0.8324, DB= 0.2304, CH= 56774.4181\n",
      "Training epoch 1800, recon_loss:0.770621, zinb_loss:0.637746, cluster_loss:0.151341\n",
      "Clustering   1800: ASW= 0.8340, DB= 0.2283, CH= 57261.9369\n",
      "Training epoch 1801, recon_loss:0.770169, zinb_loss:0.637437, cluster_loss:0.152122\n",
      "Clustering   1801: ASW= 0.8322, DB= 0.2303, CH= 56776.4431\n",
      "Training epoch 1802, recon_loss:0.770400, zinb_loss:0.637633, cluster_loss:0.151238\n",
      "Clustering   1802: ASW= 0.8343, DB= 0.2282, CH= 57392.0723\n",
      "Training epoch 1803, recon_loss:0.769730, zinb_loss:0.637262, cluster_loss:0.151719\n",
      "Clustering   1803: ASW= 0.8324, DB= 0.2296, CH= 57002.6752\n",
      "Training epoch 1804, recon_loss:0.770026, zinb_loss:0.637443, cluster_loss:0.151069\n",
      "Clustering   1804: ASW= 0.8346, DB= 0.2280, CH= 57555.6880\n",
      "Training epoch 1805, recon_loss:0.769463, zinb_loss:0.637117, cluster_loss:0.151334\n",
      "Clustering   1805: ASW= 0.8326, DB= 0.2294, CH= 57332.5751\n",
      "Training epoch 1806, recon_loss:0.769946, zinb_loss:0.637345, cluster_loss:0.150998\n",
      "Clustering   1806: ASW= 0.8346, DB= 0.2280, CH= 57608.0179\n",
      "Training epoch 1807, recon_loss:0.769655, zinb_loss:0.637184, cluster_loss:0.151168\n",
      "Clustering   1807: ASW= 0.8328, DB= 0.2289, CH= 57516.0318\n",
      "Training epoch 1808, recon_loss:0.770328, zinb_loss:0.637464, cluster_loss:0.151061\n",
      "Clustering   1808: ASW= 0.8344, DB= 0.2282, CH= 57555.6134\n",
      "Training epoch 1809, recon_loss:0.770149, zinb_loss:0.637425, cluster_loss:0.151157\n",
      "Clustering   1809: ASW= 0.8328, DB= 0.2288, CH= 57626.0272\n",
      "Training epoch 1810, recon_loss:0.770809, zinb_loss:0.637655, cluster_loss:0.151276\n",
      "Clustering   1810: ASW= 0.8340, DB= 0.2289, CH= 57348.9826\n",
      "Training epoch 1811, recon_loss:0.770629, zinb_loss:0.637756, cluster_loss:0.151249\n",
      "Clustering   1811: ASW= 0.8328, DB= 0.2281, CH= 57578.9874\n",
      "Training epoch 1812, recon_loss:0.770942, zinb_loss:0.637795, cluster_loss:0.151525\n",
      "Clustering   1812: ASW= 0.8336, DB= 0.2297, CH= 57048.2373\n",
      "Training epoch 1813, recon_loss:0.770591, zinb_loss:0.638002, cluster_loss:0.151296\n",
      "Clustering   1813: ASW= 0.8329, DB= 0.2281, CH= 57558.5392\n",
      "Training epoch 1814, recon_loss:0.770477, zinb_loss:0.637800, cluster_loss:0.151606\n",
      "Clustering   1814: ASW= 0.8334, DB= 0.2295, CH= 56827.4333\n",
      "Training epoch 1815, recon_loss:0.770106, zinb_loss:0.638139, cluster_loss:0.151148\n",
      "Clustering   1815: ASW= 0.8333, DB= 0.2284, CH= 57752.6969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1816, recon_loss:0.769861, zinb_loss:0.637810, cluster_loss:0.151534\n",
      "Clustering   1816: ASW= 0.8335, DB= 0.2292, CH= 56828.4266\n",
      "Training epoch 1817, recon_loss:0.769705, zinb_loss:0.638238, cluster_loss:0.150946\n",
      "Clustering   1817: ASW= 0.8340, DB= 0.2279, CH= 58099.7173\n",
      "Training epoch 1818, recon_loss:0.769549, zinb_loss:0.637877, cluster_loss:0.151508\n",
      "Clustering   1818: ASW= 0.8335, DB= 0.2291, CH= 56873.1990\n",
      "Training epoch 1819, recon_loss:0.769553, zinb_loss:0.638346, cluster_loss:0.150848\n",
      "Clustering   1819: ASW= 0.8344, DB= 0.2279, CH= 58398.4806\n",
      "Training epoch 1820, recon_loss:0.769627, zinb_loss:0.637922, cluster_loss:0.151640\n",
      "Clustering   1820: ASW= 0.8333, DB= 0.2291, CH= 56766.2153\n",
      "Training epoch 1821, recon_loss:0.769624, zinb_loss:0.638292, cluster_loss:0.150871\n",
      "Clustering   1821: ASW= 0.8348, DB= 0.2280, CH= 58584.1731\n",
      "Training epoch 1822, recon_loss:0.769789, zinb_loss:0.637743, cluster_loss:0.151807\n",
      "Clustering   1822: ASW= 0.8332, DB= 0.2286, CH= 56668.2015\n",
      "Training epoch 1823, recon_loss:0.769491, zinb_loss:0.638081, cluster_loss:0.150918\n",
      "Clustering   1823: ASW= 0.8350, DB= 0.2277, CH= 58692.6577\n",
      "Training epoch 1824, recon_loss:0.769534, zinb_loss:0.637487, cluster_loss:0.151923\n",
      "Clustering   1824: ASW= 0.8330, DB= 0.2284, CH= 56547.8546\n",
      "Training epoch 1825, recon_loss:0.769130, zinb_loss:0.637792, cluster_loss:0.150936\n",
      "Clustering   1825: ASW= 0.8351, DB= 0.2275, CH= 58702.8390\n",
      "Training epoch 1826, recon_loss:0.769094, zinb_loss:0.637199, cluster_loss:0.151884\n",
      "Clustering   1826: ASW= 0.8330, DB= 0.2283, CH= 56573.7753\n",
      "Training epoch 1827, recon_loss:0.768816, zinb_loss:0.637528, cluster_loss:0.150956\n",
      "Clustering   1827: ASW= 0.8351, DB= 0.2276, CH= 58659.6510\n",
      "Training epoch 1828, recon_loss:0.768897, zinb_loss:0.636998, cluster_loss:0.151783\n",
      "Clustering   1828: ASW= 0.8330, DB= 0.2281, CH= 56648.8920\n",
      "Training epoch 1829, recon_loss:0.768791, zinb_loss:0.637352, cluster_loss:0.150965\n",
      "Clustering   1829: ASW= 0.8352, DB= 0.2276, CH= 58688.0276\n",
      "Training epoch 1830, recon_loss:0.768976, zinb_loss:0.636864, cluster_loss:0.151695\n",
      "Clustering   1830: ASW= 0.8331, DB= 0.2281, CH= 56765.6304\n",
      "Training epoch 1831, recon_loss:0.768952, zinb_loss:0.637204, cluster_loss:0.151050\n",
      "Clustering   1831: ASW= 0.8351, DB= 0.2276, CH= 58584.6865\n",
      "Training epoch 1832, recon_loss:0.769167, zinb_loss:0.636809, cluster_loss:0.151620\n",
      "Clustering   1832: ASW= 0.8333, DB= 0.2279, CH= 56854.8939\n",
      "Training epoch 1833, recon_loss:0.769083, zinb_loss:0.637113, cluster_loss:0.151117\n",
      "Clustering   1833: ASW= 0.8349, DB= 0.2278, CH= 58531.9392\n",
      "Training epoch 1834, recon_loss:0.769327, zinb_loss:0.636848, cluster_loss:0.151534\n",
      "Clustering   1834: ASW= 0.8335, DB= 0.2278, CH= 56901.0920\n",
      "Training epoch 1835, recon_loss:0.769088, zinb_loss:0.637022, cluster_loss:0.151219\n",
      "Clustering   1835: ASW= 0.8346, DB= 0.2283, CH= 58382.8635\n",
      "Training epoch 1836, recon_loss:0.769471, zinb_loss:0.636946, cluster_loss:0.151504\n",
      "Clustering   1836: ASW= 0.8337, DB= 0.2273, CH= 56751.1283\n",
      "Training epoch 1837, recon_loss:0.769169, zinb_loss:0.636934, cluster_loss:0.151389\n",
      "Clustering   1837: ASW= 0.8341, DB= 0.2293, CH= 58148.7729\n",
      "Training epoch 1838, recon_loss:0.769668, zinb_loss:0.637042, cluster_loss:0.151546\n",
      "Clustering   1838: ASW= 0.8338, DB= 0.2284, CH= 56546.2786\n",
      "Training epoch 1839, recon_loss:0.769155, zinb_loss:0.636737, cluster_loss:0.151460\n",
      "Clustering   1839: ASW= 0.8337, DB= 0.2300, CH= 57874.7191\n",
      "Training epoch 1840, recon_loss:0.769416, zinb_loss:0.636952, cluster_loss:0.151300\n",
      "Clustering   1840: ASW= 0.8341, DB= 0.2275, CH= 56760.4241\n",
      "Training epoch 1841, recon_loss:0.768838, zinb_loss:0.636557, cluster_loss:0.151284\n",
      "Clustering   1841: ASW= 0.8333, DB= 0.2301, CH= 57638.2748\n",
      "Training epoch 1842, recon_loss:0.768852, zinb_loss:0.636856, cluster_loss:0.150984\n",
      "Clustering   1842: ASW= 0.8344, DB= 0.2271, CH= 57108.2309\n",
      "Training epoch 1843, recon_loss:0.768589, zinb_loss:0.636625, cluster_loss:0.151145\n",
      "Clustering   1843: ASW= 0.8334, DB= 0.2301, CH= 57782.7571\n",
      "Training epoch 1844, recon_loss:0.768844, zinb_loss:0.637070, cluster_loss:0.150928\n",
      "Clustering   1844: ASW= 0.8345, DB= 0.2271, CH= 57273.2021\n",
      "Training epoch 1845, recon_loss:0.768759, zinb_loss:0.636960, cluster_loss:0.151193\n",
      "Clustering   1845: ASW= 0.8334, DB= 0.2302, CH= 57790.2959\n",
      "Training epoch 1846, recon_loss:0.769053, zinb_loss:0.637479, cluster_loss:0.151021\n",
      "Clustering   1846: ASW= 0.8343, DB= 0.2265, CH= 57304.7055\n",
      "Training epoch 1847, recon_loss:0.768928, zinb_loss:0.637469, cluster_loss:0.151304\n",
      "Clustering   1847: ASW= 0.8334, DB= 0.2305, CH= 57777.7847\n",
      "Training epoch 1848, recon_loss:0.769206, zinb_loss:0.637887, cluster_loss:0.151187\n",
      "Clustering   1848: ASW= 0.8340, DB= 0.2269, CH= 57350.2003\n",
      "Training epoch 1849, recon_loss:0.769162, zinb_loss:0.638012, cluster_loss:0.151345\n",
      "Clustering   1849: ASW= 0.8336, DB= 0.2306, CH= 57755.0674\n",
      "Training epoch 1850, recon_loss:0.769796, zinb_loss:0.638226, cluster_loss:0.151376\n",
      "Clustering   1850: ASW= 0.8336, DB= 0.2277, CH= 57436.4788\n",
      "Training epoch 1851, recon_loss:0.770242, zinb_loss:0.638581, cluster_loss:0.151400\n",
      "Clustering   1851: ASW= 0.8340, DB= 0.2298, CH= 57696.0330\n",
      "Training epoch 1852, recon_loss:0.770748, zinb_loss:0.638542, cluster_loss:0.151603\n",
      "Clustering   1852: ASW= 0.8333, DB= 0.2286, CH= 57508.9618\n",
      "Training epoch 1853, recon_loss:0.770692, zinb_loss:0.639021, cluster_loss:0.151513\n",
      "Clustering   1853: ASW= 0.8342, DB= 0.2284, CH= 57385.3486\n",
      "Training epoch 1854, recon_loss:0.770391, zinb_loss:0.638633, cluster_loss:0.151707\n",
      "Clustering   1854: ASW= 0.8331, DB= 0.2287, CH= 57438.2441\n",
      "Training epoch 1855, recon_loss:0.770239, zinb_loss:0.639024, cluster_loss:0.151446\n",
      "Clustering   1855: ASW= 0.8343, DB= 0.2278, CH= 57115.5095\n",
      "Training epoch 1856, recon_loss:0.769859, zinb_loss:0.638352, cluster_loss:0.151636\n",
      "Clustering   1856: ASW= 0.8332, DB= 0.2294, CH= 57460.4348\n",
      "Training epoch 1857, recon_loss:0.769911, zinb_loss:0.638656, cluster_loss:0.151315\n",
      "Clustering   1857: ASW= 0.8343, DB= 0.2273, CH= 57194.1458\n",
      "Training epoch 1858, recon_loss:0.769246, zinb_loss:0.637891, cluster_loss:0.151470\n",
      "Clustering   1858: ASW= 0.8336, DB= 0.2292, CH= 57556.0447\n",
      "Training epoch 1859, recon_loss:0.769144, zinb_loss:0.638173, cluster_loss:0.151046\n",
      "Clustering   1859: ASW= 0.8344, DB= 0.2265, CH= 57495.7504\n",
      "Training epoch 1860, recon_loss:0.768723, zinb_loss:0.637583, cluster_loss:0.151277\n",
      "Clustering   1860: ASW= 0.8340, DB= 0.2292, CH= 57706.0064\n",
      "Training epoch 1861, recon_loss:0.768940, zinb_loss:0.637911, cluster_loss:0.150900\n",
      "Clustering   1861: ASW= 0.8345, DB= 0.2264, CH= 57764.6823\n",
      "Training epoch 1862, recon_loss:0.768603, zinb_loss:0.637460, cluster_loss:0.151204\n",
      "Clustering   1862: ASW= 0.8342, DB= 0.2290, CH= 57789.4737\n",
      "Training epoch 1863, recon_loss:0.769142, zinb_loss:0.637825, cluster_loss:0.150919\n",
      "Clustering   1863: ASW= 0.8344, DB= 0.2255, CH= 57931.8225\n",
      "Training epoch 1864, recon_loss:0.768689, zinb_loss:0.637477, cluster_loss:0.151216\n",
      "Clustering   1864: ASW= 0.8345, DB= 0.2290, CH= 57844.7758\n",
      "Training epoch 1865, recon_loss:0.769411, zinb_loss:0.637801, cluster_loss:0.151007\n",
      "Clustering   1865: ASW= 0.8343, DB= 0.2259, CH= 57988.0656\n",
      "Training epoch 1866, recon_loss:0.768848, zinb_loss:0.637625, cluster_loss:0.151221\n",
      "Clustering   1866: ASW= 0.8348, DB= 0.2288, CH= 57937.7945\n",
      "Training epoch 1867, recon_loss:0.769739, zinb_loss:0.637848, cluster_loss:0.151192\n",
      "Clustering   1867: ASW= 0.8340, DB= 0.2256, CH= 58075.0533\n",
      "Training epoch 1868, recon_loss:0.769078, zinb_loss:0.637803, cluster_loss:0.151265\n",
      "Clustering   1868: ASW= 0.8352, DB= 0.2285, CH= 58057.1561\n",
      "Training epoch 1869, recon_loss:0.769967, zinb_loss:0.637900, cluster_loss:0.151458\n",
      "Clustering   1869: ASW= 0.8338, DB= 0.2263, CH= 58007.5293\n",
      "Training epoch 1870, recon_loss:0.769353, zinb_loss:0.638154, cluster_loss:0.151307\n",
      "Clustering   1870: ASW= 0.8353, DB= 0.2278, CH= 57935.3167\n",
      "Training epoch 1871, recon_loss:0.770041, zinb_loss:0.638029, cluster_loss:0.151724\n",
      "Clustering   1871: ASW= 0.8335, DB= 0.2269, CH= 57808.9607\n",
      "Training epoch 1872, recon_loss:0.769574, zinb_loss:0.638508, cluster_loss:0.151386\n",
      "Clustering   1872: ASW= 0.8348, DB= 0.2276, CH= 57581.0448\n",
      "Training epoch 1873, recon_loss:0.770116, zinb_loss:0.638154, cluster_loss:0.151969\n",
      "Clustering   1873: ASW= 0.8332, DB= 0.2274, CH= 57426.4847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1874, recon_loss:0.769822, zinb_loss:0.638970, cluster_loss:0.151447\n",
      "Clustering   1874: ASW= 0.8348, DB= 0.2276, CH= 57528.0434\n",
      "Training epoch 1875, recon_loss:0.770381, zinb_loss:0.638312, cluster_loss:0.152162\n",
      "Clustering   1875: ASW= 0.8327, DB= 0.2303, CH= 56815.9054\n",
      "Training epoch 1876, recon_loss:0.769683, zinb_loss:0.639021, cluster_loss:0.151487\n",
      "Clustering   1876: ASW= 0.8333, DB= 0.2305, CH= 57059.6606\n",
      "Training epoch 1877, recon_loss:0.771052, zinb_loss:0.638785, cluster_loss:0.152958\n",
      "Clustering   1877: ASW= 0.8322, DB= 0.2281, CH= 56177.8559\n",
      "Training epoch 1878, recon_loss:0.770320, zinb_loss:0.639217, cluster_loss:0.151445\n",
      "Clustering   1878: ASW= 0.8337, DB= 0.2297, CH= 57046.4740\n",
      "Training epoch 1879, recon_loss:0.770510, zinb_loss:0.638672, cluster_loss:0.152240\n",
      "Clustering   1879: ASW= 0.8330, DB= 0.2270, CH= 56694.2280\n",
      "Training epoch 1880, recon_loss:0.769889, zinb_loss:0.639014, cluster_loss:0.151023\n",
      "Clustering   1880: ASW= 0.8341, DB= 0.2285, CH= 57548.6577\n",
      "Training epoch 1881, recon_loss:0.770245, zinb_loss:0.638985, cluster_loss:0.151494\n",
      "Clustering   1881: ASW= 0.8342, DB= 0.2263, CH= 57538.3953\n",
      "Training epoch 1882, recon_loss:0.769861, zinb_loss:0.639180, cluster_loss:0.150864\n",
      "Clustering   1882: ASW= 0.8343, DB= 0.2289, CH= 57905.8923\n",
      "Training epoch 1883, recon_loss:0.770359, zinb_loss:0.639495, cluster_loss:0.151303\n",
      "Clustering   1883: ASW= 0.8347, DB= 0.2265, CH= 57862.8488\n",
      "Training epoch 1884, recon_loss:0.770375, zinb_loss:0.639652, cluster_loss:0.151002\n",
      "Clustering   1884: ASW= 0.8342, DB= 0.2288, CH= 57988.6702\n",
      "Training epoch 1885, recon_loss:0.771170, zinb_loss:0.640054, cluster_loss:0.151274\n",
      "Clustering   1885: ASW= 0.8348, DB= 0.2258, CH= 57915.5406\n",
      "Training epoch 1886, recon_loss:0.771230, zinb_loss:0.640061, cluster_loss:0.151215\n",
      "Clustering   1886: ASW= 0.8339, DB= 0.2290, CH= 58140.0550\n",
      "Training epoch 1887, recon_loss:0.771156, zinb_loss:0.640337, cluster_loss:0.151313\n",
      "Clustering   1887: ASW= 0.8350, DB= 0.2263, CH= 57403.3491\n",
      "Training epoch 1888, recon_loss:0.769877, zinb_loss:0.639244, cluster_loss:0.151215\n",
      "Clustering   1888: ASW= 0.8335, DB= 0.2295, CH= 57803.0566\n",
      "Training epoch 1889, recon_loss:0.769620, zinb_loss:0.639305, cluster_loss:0.151075\n",
      "Clustering   1889: ASW= 0.8349, DB= 0.2262, CH= 57712.3329\n",
      "Training epoch 1890, recon_loss:0.768785, zinb_loss:0.638573, cluster_loss:0.151003\n",
      "Clustering   1890: ASW= 0.8338, DB= 0.2284, CH= 57952.7402\n",
      "Training epoch 1891, recon_loss:0.768828, zinb_loss:0.638643, cluster_loss:0.150870\n",
      "Clustering   1891: ASW= 0.8355, DB= 0.2257, CH= 58019.6368\n",
      "Training epoch 1892, recon_loss:0.768418, zinb_loss:0.638177, cluster_loss:0.150806\n",
      "Clustering   1892: ASW= 0.8342, DB= 0.2281, CH= 58169.3590\n",
      "Training epoch 1893, recon_loss:0.768573, zinb_loss:0.638353, cluster_loss:0.150780\n",
      "Clustering   1893: ASW= 0.8354, DB= 0.2253, CH= 58321.1122\n",
      "Training epoch 1894, recon_loss:0.768252, zinb_loss:0.638179, cluster_loss:0.150635\n",
      "Clustering   1894: ASW= 0.8344, DB= 0.2272, CH= 58341.9631\n",
      "Training epoch 1895, recon_loss:0.768608, zinb_loss:0.638188, cluster_loss:0.150848\n",
      "Clustering   1895: ASW= 0.8355, DB= 0.2256, CH= 58438.5488\n",
      "Training epoch 1896, recon_loss:0.768623, zinb_loss:0.638151, cluster_loss:0.150663\n",
      "Clustering   1896: ASW= 0.8339, DB= 0.2281, CH= 58166.6390\n",
      "Training epoch 1897, recon_loss:0.768828, zinb_loss:0.638090, cluster_loss:0.150815\n",
      "Clustering   1897: ASW= 0.8357, DB= 0.2267, CH= 58420.1429\n",
      "Training epoch 1898, recon_loss:0.769114, zinb_loss:0.638411, cluster_loss:0.150746\n",
      "Clustering   1898: ASW= 0.8344, DB= 0.2261, CH= 58284.9541\n",
      "Training epoch 1899, recon_loss:0.769860, zinb_loss:0.638481, cluster_loss:0.151499\n",
      "Clustering   1899: ASW= 0.8352, DB= 0.2283, CH= 58301.3694\n",
      "Training epoch 1900, recon_loss:0.771110, zinb_loss:0.639330, cluster_loss:0.151623\n",
      "Clustering   1900: ASW= 0.8334, DB= 0.2271, CH= 57283.2700\n",
      "Training epoch 1901, recon_loss:0.771246, zinb_loss:0.638918, cluster_loss:0.152406\n",
      "Clustering   1901: ASW= 0.8346, DB= 0.2297, CH= 57920.2612\n",
      "Training epoch 1902, recon_loss:0.771318, zinb_loss:0.638888, cluster_loss:0.152250\n",
      "Clustering   1902: ASW= 0.8316, DB= 0.2321, CH= 55923.5794\n",
      "Training epoch 1903, recon_loss:0.771595, zinb_loss:0.637806, cluster_loss:0.153064\n",
      "Clustering   1903: ASW= 0.8322, DB= 0.2309, CH= 57122.7869\n",
      "Training epoch 1904, recon_loss:0.770531, zinb_loss:0.638521, cluster_loss:0.152605\n",
      "Clustering   1904: ASW= 0.8322, DB= 0.2291, CH= 55735.5701\n",
      "Training epoch 1905, recon_loss:0.769562, zinb_loss:0.638074, cluster_loss:0.151690\n",
      "Clustering   1905: ASW= 0.8328, DB= 0.2313, CH= 56507.4032\n",
      "Training epoch 1906, recon_loss:0.768438, zinb_loss:0.637894, cluster_loss:0.150965\n",
      "Clustering   1906: ASW= 0.8331, DB= 0.2281, CH= 57168.1096\n",
      "Training epoch 1907, recon_loss:0.768838, zinb_loss:0.637536, cluster_loss:0.151542\n",
      "Clustering   1907: ASW= 0.8337, DB= 0.2293, CH= 57473.0762\n",
      "Training epoch 1908, recon_loss:0.769733, zinb_loss:0.638150, cluster_loss:0.151365\n",
      "Clustering   1908: ASW= 0.8338, DB= 0.2276, CH= 57178.5670\n",
      "Training epoch 1909, recon_loss:0.771277, zinb_loss:0.637744, cluster_loss:0.152642\n",
      "Clustering   1909: ASW= 0.8321, DB= 0.2298, CH= 56918.8256\n",
      "Training epoch 1910, recon_loss:0.771203, zinb_loss:0.638284, cluster_loss:0.151649\n",
      "Clustering   1910: ASW= 0.8347, DB= 0.2276, CH= 56877.9455\n",
      "Training epoch 1911, recon_loss:0.769809, zinb_loss:0.637315, cluster_loss:0.151994\n",
      "Clustering   1911: ASW= 0.8317, DB= 0.2299, CH= 56634.2748\n",
      "Training epoch 1912, recon_loss:0.769400, zinb_loss:0.637639, cluster_loss:0.151039\n",
      "Clustering   1912: ASW= 0.8348, DB= 0.2271, CH= 57437.9563\n",
      "Training epoch 1913, recon_loss:0.768396, zinb_loss:0.636904, cluster_loss:0.151298\n",
      "Clustering   1913: ASW= 0.8331, DB= 0.2281, CH= 57239.4991\n",
      "Training epoch 1914, recon_loss:0.768542, zinb_loss:0.637246, cluster_loss:0.150704\n",
      "Clustering   1914: ASW= 0.8354, DB= 0.2261, CH= 58041.3084\n",
      "Training epoch 1915, recon_loss:0.767969, zinb_loss:0.636753, cluster_loss:0.151022\n",
      "Clustering   1915: ASW= 0.8337, DB= 0.2275, CH= 57672.1955\n",
      "Training epoch 1916, recon_loss:0.768326, zinb_loss:0.637119, cluster_loss:0.150627\n",
      "Clustering   1916: ASW= 0.8356, DB= 0.2259, CH= 58308.3585\n",
      "Training epoch 1917, recon_loss:0.768204, zinb_loss:0.636830, cluster_loss:0.150910\n",
      "Clustering   1917: ASW= 0.8340, DB= 0.2271, CH= 57923.3822\n",
      "Training epoch 1918, recon_loss:0.768757, zinb_loss:0.637211, cluster_loss:0.150618\n",
      "Clustering   1918: ASW= 0.8357, DB= 0.2262, CH= 58479.7139\n",
      "Training epoch 1919, recon_loss:0.768940, zinb_loss:0.637086, cluster_loss:0.150870\n",
      "Clustering   1919: ASW= 0.8343, DB= 0.2271, CH= 58173.3248\n",
      "Training epoch 1920, recon_loss:0.769651, zinb_loss:0.637442, cluster_loss:0.150733\n",
      "Clustering   1920: ASW= 0.8356, DB= 0.2267, CH= 58464.6462\n",
      "Training epoch 1921, recon_loss:0.769700, zinb_loss:0.637450, cluster_loss:0.150884\n",
      "Clustering   1921: ASW= 0.8345, DB= 0.2268, CH= 58339.9083\n",
      "Training epoch 1922, recon_loss:0.770076, zinb_loss:0.637649, cluster_loss:0.150840\n",
      "Clustering   1922: ASW= 0.8354, DB= 0.2274, CH= 58373.8267\n",
      "Training epoch 1923, recon_loss:0.769739, zinb_loss:0.637666, cluster_loss:0.150839\n",
      "Clustering   1923: ASW= 0.8347, DB= 0.2269, CH= 58463.2502\n",
      "Training epoch 1924, recon_loss:0.769951, zinb_loss:0.637678, cluster_loss:0.150865\n",
      "Clustering   1924: ASW= 0.8352, DB= 0.2274, CH= 58340.8963\n",
      "Training epoch 1925, recon_loss:0.769473, zinb_loss:0.637668, cluster_loss:0.150759\n",
      "Clustering   1925: ASW= 0.8349, DB= 0.2267, CH= 58534.8114\n",
      "Training epoch 1926, recon_loss:0.769647, zinb_loss:0.637560, cluster_loss:0.150851\n",
      "Clustering   1926: ASW= 0.8350, DB= 0.2277, CH= 58389.8548\n",
      "Training epoch 1927, recon_loss:0.769146, zinb_loss:0.637557, cluster_loss:0.150686\n",
      "Clustering   1927: ASW= 0.8351, DB= 0.2264, CH= 58565.5004\n",
      "Training epoch 1928, recon_loss:0.769357, zinb_loss:0.637387, cluster_loss:0.150837\n",
      "Clustering   1928: ASW= 0.8347, DB= 0.2277, CH= 58439.4814\n",
      "Training epoch 1929, recon_loss:0.768900, zinb_loss:0.637467, cluster_loss:0.150606\n",
      "Clustering   1929: ASW= 0.8353, DB= 0.2262, CH= 58606.3940\n",
      "Training epoch 1930, recon_loss:0.769195, zinb_loss:0.637258, cluster_loss:0.150918\n",
      "Clustering   1930: ASW= 0.8344, DB= 0.2282, CH= 58383.9437\n",
      "Training epoch 1931, recon_loss:0.768864, zinb_loss:0.637498, cluster_loss:0.150567\n",
      "Clustering   1931: ASW= 0.8354, DB= 0.2258, CH= 58533.0763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1932, recon_loss:0.769345, zinb_loss:0.637183, cluster_loss:0.151275\n",
      "Clustering   1932: ASW= 0.8337, DB= 0.2290, CH= 57940.4366\n",
      "Training epoch 1933, recon_loss:0.769448, zinb_loss:0.637685, cluster_loss:0.150879\n",
      "Clustering   1933: ASW= 0.8350, DB= 0.2265, CH= 57884.4448\n",
      "Training epoch 1934, recon_loss:0.769667, zinb_loss:0.637067, cluster_loss:0.151962\n",
      "Clustering   1934: ASW= 0.8329, DB= 0.2303, CH= 57138.2561\n",
      "Training epoch 1935, recon_loss:0.769983, zinb_loss:0.637742, cluster_loss:0.151414\n",
      "Clustering   1935: ASW= 0.8341, DB= 0.2278, CH= 56855.0262\n",
      "Training epoch 1936, recon_loss:0.768838, zinb_loss:0.636645, cluster_loss:0.151873\n",
      "Clustering   1936: ASW= 0.8331, DB= 0.2301, CH= 57009.0463\n",
      "Training epoch 1937, recon_loss:0.768502, zinb_loss:0.637271, cluster_loss:0.150768\n",
      "Clustering   1937: ASW= 0.8350, DB= 0.2267, CH= 57790.5302\n",
      "Training epoch 1938, recon_loss:0.768035, zinb_loss:0.636544, cluster_loss:0.151435\n",
      "Clustering   1938: ASW= 0.8341, DB= 0.2289, CH= 57499.0628\n",
      "Training epoch 1939, recon_loss:0.768166, zinb_loss:0.637109, cluster_loss:0.150602\n",
      "Clustering   1939: ASW= 0.8350, DB= 0.2263, CH= 58189.1817\n",
      "Training epoch 1940, recon_loss:0.768120, zinb_loss:0.636658, cluster_loss:0.151448\n",
      "Clustering   1940: ASW= 0.8345, DB= 0.2281, CH= 57666.8782\n",
      "Training epoch 1941, recon_loss:0.768190, zinb_loss:0.637130, cluster_loss:0.150593\n",
      "Clustering   1941: ASW= 0.8351, DB= 0.2259, CH= 58368.7032\n",
      "Training epoch 1942, recon_loss:0.768077, zinb_loss:0.636696, cluster_loss:0.151205\n",
      "Clustering   1942: ASW= 0.8350, DB= 0.2276, CH= 58024.6285\n",
      "Training epoch 1943, recon_loss:0.768108, zinb_loss:0.637025, cluster_loss:0.150518\n",
      "Clustering   1943: ASW= 0.8352, DB= 0.2254, CH= 58539.7395\n",
      "Training epoch 1944, recon_loss:0.768417, zinb_loss:0.636841, cluster_loss:0.151025\n",
      "Clustering   1944: ASW= 0.8353, DB= 0.2274, CH= 58317.0514\n",
      "Training epoch 1945, recon_loss:0.768635, zinb_loss:0.637237, cluster_loss:0.150503\n",
      "Clustering   1945: ASW= 0.8353, DB= 0.2262, CH= 58623.3216\n",
      "Training epoch 1946, recon_loss:0.768946, zinb_loss:0.637434, cluster_loss:0.150854\n",
      "Clustering   1946: ASW= 0.8358, DB= 0.2258, CH= 58771.9008\n",
      "Training epoch 1947, recon_loss:0.769082, zinb_loss:0.637665, cluster_loss:0.150659\n",
      "Clustering   1947: ASW= 0.8349, DB= 0.2265, CH= 58557.4373\n",
      "Training epoch 1948, recon_loss:0.769596, zinb_loss:0.638126, cluster_loss:0.150810\n",
      "Clustering   1948: ASW= 0.8357, DB= 0.2255, CH= 58876.0789\n",
      "Training epoch 1949, recon_loss:0.769739, zinb_loss:0.638175, cluster_loss:0.150893\n",
      "Clustering   1949: ASW= 0.8345, DB= 0.2277, CH= 58295.7341\n",
      "Training epoch 1950, recon_loss:0.770230, zinb_loss:0.638775, cluster_loss:0.150852\n",
      "Clustering   1950: ASW= 0.8353, DB= 0.2252, CH= 58793.8559\n",
      "Training epoch 1951, recon_loss:0.770341, zinb_loss:0.638566, cluster_loss:0.151180\n",
      "Clustering   1951: ASW= 0.8338, DB= 0.2294, CH= 57830.1228\n",
      "Training epoch 1952, recon_loss:0.770628, zinb_loss:0.639164, cluster_loss:0.150896\n",
      "Clustering   1952: ASW= 0.8351, DB= 0.2245, CH= 58759.0652\n",
      "Training epoch 1953, recon_loss:0.770226, zinb_loss:0.638675, cluster_loss:0.151439\n",
      "Clustering   1953: ASW= 0.8336, DB= 0.2303, CH= 57562.2314\n",
      "Training epoch 1954, recon_loss:0.770253, zinb_loss:0.639192, cluster_loss:0.150933\n",
      "Clustering   1954: ASW= 0.8349, DB= 0.2248, CH= 58777.1035\n",
      "Training epoch 1955, recon_loss:0.770004, zinb_loss:0.638594, cluster_loss:0.151637\n",
      "Clustering   1955: ASW= 0.8335, DB= 0.2306, CH= 57339.8506\n",
      "Training epoch 1956, recon_loss:0.770061, zinb_loss:0.639223, cluster_loss:0.150979\n",
      "Clustering   1956: ASW= 0.8348, DB= 0.2255, CH= 58774.4743\n",
      "Training epoch 1957, recon_loss:0.769850, zinb_loss:0.638702, cluster_loss:0.151656\n",
      "Clustering   1957: ASW= 0.8339, DB= 0.2302, CH= 57388.6280\n",
      "Training epoch 1958, recon_loss:0.769705, zinb_loss:0.639285, cluster_loss:0.150950\n",
      "Clustering   1958: ASW= 0.8349, DB= 0.2252, CH= 58868.6603\n",
      "Training epoch 1959, recon_loss:0.769485, zinb_loss:0.638779, cluster_loss:0.151531\n",
      "Clustering   1959: ASW= 0.8344, DB= 0.2296, CH= 57587.7969\n",
      "Training epoch 1960, recon_loss:0.769251, zinb_loss:0.639165, cluster_loss:0.150893\n",
      "Clustering   1960: ASW= 0.8349, DB= 0.2254, CH= 58966.3313\n",
      "Training epoch 1961, recon_loss:0.769107, zinb_loss:0.638671, cluster_loss:0.151382\n",
      "Clustering   1961: ASW= 0.8349, DB= 0.2288, CH= 57816.4580\n",
      "Training epoch 1962, recon_loss:0.768931, zinb_loss:0.638873, cluster_loss:0.150876\n",
      "Clustering   1962: ASW= 0.8349, DB= 0.2253, CH= 59086.9159\n",
      "Training epoch 1963, recon_loss:0.768864, zinb_loss:0.638486, cluster_loss:0.151271\n",
      "Clustering   1963: ASW= 0.8354, DB= 0.2281, CH= 58028.2366\n",
      "Training epoch 1964, recon_loss:0.768827, zinb_loss:0.638589, cluster_loss:0.150983\n",
      "Clustering   1964: ASW= 0.8347, DB= 0.2255, CH= 59105.2638\n",
      "Training epoch 1965, recon_loss:0.768868, zinb_loss:0.638321, cluster_loss:0.151234\n",
      "Clustering   1965: ASW= 0.8357, DB= 0.2273, CH= 58079.1630\n",
      "Training epoch 1966, recon_loss:0.769073, zinb_loss:0.638275, cluster_loss:0.151289\n",
      "Clustering   1966: ASW= 0.8343, DB= 0.2264, CH= 58850.4577\n",
      "Training epoch 1967, recon_loss:0.769323, zinb_loss:0.638193, cluster_loss:0.151389\n",
      "Clustering   1967: ASW= 0.8354, DB= 0.2272, CH= 57514.3187\n",
      "Training epoch 1968, recon_loss:0.769329, zinb_loss:0.637766, cluster_loss:0.151686\n",
      "Clustering   1968: ASW= 0.8337, DB= 0.2279, CH= 58342.7523\n",
      "Training epoch 1969, recon_loss:0.769399, zinb_loss:0.637743, cluster_loss:0.151545\n",
      "Clustering   1969: ASW= 0.8349, DB= 0.2274, CH= 56975.8918\n",
      "Training epoch 1970, recon_loss:0.768877, zinb_loss:0.637227, cluster_loss:0.151441\n",
      "Clustering   1970: ASW= 0.8343, DB= 0.2266, CH= 58345.5725\n",
      "Training epoch 1971, recon_loss:0.768982, zinb_loss:0.637519, cluster_loss:0.151008\n",
      "Clustering   1971: ASW= 0.8354, DB= 0.2271, CH= 57649.7366\n",
      "Training epoch 1972, recon_loss:0.768708, zinb_loss:0.637044, cluster_loss:0.151429\n",
      "Clustering   1972: ASW= 0.8340, DB= 0.2276, CH= 58177.2214\n",
      "Training epoch 1973, recon_loss:0.768233, zinb_loss:0.637078, cluster_loss:0.150718\n",
      "Clustering   1973: ASW= 0.8350, DB= 0.2264, CH= 57787.2610\n",
      "Training epoch 1974, recon_loss:0.767982, zinb_loss:0.636650, cluster_loss:0.151035\n",
      "Clustering   1974: ASW= 0.8348, DB= 0.2260, CH= 58412.4888\n",
      "Training epoch 1975, recon_loss:0.768030, zinb_loss:0.636830, cluster_loss:0.150470\n",
      "Clustering   1975: ASW= 0.8352, DB= 0.2265, CH= 58306.6470\n",
      "Training epoch 1976, recon_loss:0.768113, zinb_loss:0.636543, cluster_loss:0.151014\n",
      "Clustering   1976: ASW= 0.8351, DB= 0.2256, CH= 58390.4302\n",
      "Training epoch 1977, recon_loss:0.768544, zinb_loss:0.636857, cluster_loss:0.150472\n",
      "Clustering   1977: ASW= 0.8352, DB= 0.2262, CH= 58619.3071\n",
      "Training epoch 1978, recon_loss:0.768970, zinb_loss:0.636646, cluster_loss:0.151316\n",
      "Clustering   1978: ASW= 0.8352, DB= 0.2255, CH= 58123.3548\n",
      "Training epoch 1979, recon_loss:0.769354, zinb_loss:0.637115, cluster_loss:0.150693\n",
      "Clustering   1979: ASW= 0.8350, DB= 0.2269, CH= 58811.6559\n",
      "Training epoch 1980, recon_loss:0.769724, zinb_loss:0.636973, cluster_loss:0.151797\n",
      "Clustering   1980: ASW= 0.8351, DB= 0.2253, CH= 57710.9823\n",
      "Training epoch 1981, recon_loss:0.769637, zinb_loss:0.637460, cluster_loss:0.150914\n",
      "Clustering   1981: ASW= 0.8349, DB= 0.2273, CH= 58934.2546\n",
      "Training epoch 1982, recon_loss:0.769869, zinb_loss:0.637328, cluster_loss:0.152082\n",
      "Clustering   1982: ASW= 0.8350, DB= 0.2260, CH= 57427.7765\n",
      "Training epoch 1983, recon_loss:0.769195, zinb_loss:0.637654, cluster_loss:0.150925\n",
      "Clustering   1983: ASW= 0.8351, DB= 0.2274, CH= 58991.0786\n",
      "Training epoch 1984, recon_loss:0.769311, zinb_loss:0.637486, cluster_loss:0.151933\n",
      "Clustering   1984: ASW= 0.8352, DB= 0.2260, CH= 57544.3600\n",
      "Training epoch 1985, recon_loss:0.768646, zinb_loss:0.637767, cluster_loss:0.150808\n",
      "Clustering   1985: ASW= 0.8353, DB= 0.2273, CH= 59076.0323\n",
      "Training epoch 1986, recon_loss:0.768776, zinb_loss:0.637717, cluster_loss:0.151531\n",
      "Clustering   1986: ASW= 0.8357, DB= 0.2253, CH= 57931.4493\n",
      "Training epoch 1987, recon_loss:0.768227, zinb_loss:0.637892, cluster_loss:0.150613\n",
      "Clustering   1987: ASW= 0.8355, DB= 0.2271, CH= 59236.2225\n",
      "Training epoch 1988, recon_loss:0.768627, zinb_loss:0.638033, cluster_loss:0.151187\n",
      "Clustering   1988: ASW= 0.8361, DB= 0.2248, CH= 58292.1086\n",
      "Training epoch 1989, recon_loss:0.768340, zinb_loss:0.638237, cluster_loss:0.150560\n",
      "Clustering   1989: ASW= 0.8354, DB= 0.2273, CH= 59283.7958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1990, recon_loss:0.769171, zinb_loss:0.638558, cluster_loss:0.150972\n",
      "Clustering   1990: ASW= 0.8365, DB= 0.2245, CH= 58624.7474\n",
      "Training epoch 1991, recon_loss:0.768784, zinb_loss:0.638496, cluster_loss:0.150656\n",
      "Clustering   1991: ASW= 0.8353, DB= 0.2274, CH= 59176.4936\n",
      "Training epoch 1992, recon_loss:0.769866, zinb_loss:0.638848, cluster_loss:0.150976\n",
      "Clustering   1992: ASW= 0.8367, DB= 0.2238, CH= 58882.8603\n",
      "Training epoch 1993, recon_loss:0.769482, zinb_loss:0.638755, cluster_loss:0.150743\n",
      "Clustering   1993: ASW= 0.8351, DB= 0.2280, CH= 59055.9768\n",
      "Training epoch 1994, recon_loss:0.770182, zinb_loss:0.639064, cluster_loss:0.150861\n",
      "Clustering   1994: ASW= 0.8368, DB= 0.2241, CH= 58971.7132\n",
      "Training epoch 1995, recon_loss:0.769485, zinb_loss:0.638650, cluster_loss:0.150787\n",
      "Clustering   1995: ASW= 0.8352, DB= 0.2273, CH= 58983.8484\n",
      "Training epoch 1996, recon_loss:0.770352, zinb_loss:0.638888, cluster_loss:0.150945\n",
      "Clustering   1996: ASW= 0.8368, DB= 0.2240, CH= 59127.6146\n",
      "Training epoch 1997, recon_loss:0.769486, zinb_loss:0.638490, cluster_loss:0.150750\n",
      "Clustering   1997: ASW= 0.8350, DB= 0.2276, CH= 58808.2613\n",
      "Training epoch 1998, recon_loss:0.770200, zinb_loss:0.638635, cluster_loss:0.150786\n",
      "Clustering   1998: ASW= 0.8367, DB= 0.2239, CH= 59205.3270\n",
      "Training epoch 1999, recon_loss:0.769249, zinb_loss:0.638136, cluster_loss:0.150793\n",
      "Clustering   1999: ASW= 0.8350, DB= 0.2275, CH= 58725.9285\n",
      "Training epoch 2000, recon_loss:0.769908, zinb_loss:0.638326, cluster_loss:0.150852\n",
      "Clustering   2000: ASW= 0.8367, DB= 0.2243, CH= 59218.0401\n",
      "Final Result : ASW= 0.8367, DB= 0.2243, CH= 59218.0401\n"
     ]
    }
   ],
   "source": [
    "y_pred, final_latent = model.fit(y=y, n_clusters=-1, num_epochs=2000, file='GSM4949911_tea', \n",
    "                                 pretrain_latent=pretrain_latent, resolution=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
