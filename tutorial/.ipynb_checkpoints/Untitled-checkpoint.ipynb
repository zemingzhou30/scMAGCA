{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe4d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2ffa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scMAGCA import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1aa98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "random.seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.manual_seed(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef832ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(sc.read_h5ad('../datasets/10x1kpbmc/10x1kpbmc_adt.h5ad').to_df()).astype('float32')\n",
    "x2 = np.array(sc.read_h5ad('../datasets/10x1kpbmc/10x1kpbmc_rna.h5ad').to_df()).astype('float32')\n",
    "y = np.array(pd.read_csv('../datasets/10x1kpbmc/10x1kpbmc_label.csv')['Cluster']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b6ae37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3.000e+00, 6.000e+00, 2.000e+00, ..., 3.000e+00, 4.000e+00,\n",
       "         1.000e+00],\n",
       "        [3.444e+03, 3.995e+03, 4.900e+01, ..., 6.000e+00, 8.000e+00,\n",
       "         2.000e+00],\n",
       "        [1.733e+03, 3.401e+03, 2.300e+01, ..., 1.000e+00, 6.000e+00,\n",
       "         0.000e+00],\n",
       "        ...,\n",
       "        [3.000e+00, 1.190e+02, 1.800e+01, ..., 1.000e+00, 1.000e+00,\n",
       "         2.000e+00],\n",
       "        [1.700e+01, 8.190e+02, 1.100e+01, ..., 3.000e+00, 2.000e+00,\n",
       "         2.000e+00],\n",
       "        [9.000e+00, 1.300e+01, 1.000e+01, ..., 4.000e+00, 1.100e+01,\n",
       "         5.000e+00]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5., 1., 1., 1., 1., 3., 3., 3., 1., 1., 3., 1., 1., 1., 5., 3., 3.,\n",
       "        3., 3., 2., 3., 1., 2., 1., 3., 4., 2., 5., 1., 1., 2., 5., 1., 4.,\n",
       "        2., 1., 3., 5., 2., 4., 1., 1., 3., 1., 1., 5., 1., 1., 1., 5., 5.,\n",
       "        2., 4., 1., 3., 3., 1., 4., 2., 5., 2., 1., 1., 1., 5., 1., 3., 2.,\n",
       "        3., 4., 2., 1., 2., 4., 3., 1., 3., 1., 1., 1., 1., 3., 4., 4., 2.,\n",
       "        4., 2., 3., 5., 5., 2., 4., 3., 4., 1., 1., 5., 2., 1., 1., 2., 1.,\n",
       "        1., 3., 1., 4., 1., 4., 1., 3., 4., 2., 5., 1., 1., 4., 1., 4., 2.,\n",
       "        5., 4., 4., 4., 2., 1., 5., 2., 1., 1., 3., 2., 2., 1., 1., 4., 3.,\n",
       "        4., 1., 4., 3., 1., 2., 1., 1., 5., 1., 3., 3., 3., 3., 5., 4., 3.,\n",
       "        4., 1., 1., 1., 2., 3., 3., 3., 2., 2., 1., 4., 3., 3., 4., 4., 4.,\n",
       "        1., 4., 1., 3., 5., 3., 5., 2., 3., 5., 1., 1., 2., 1., 3., 1., 2.,\n",
       "        1., 1., 5., 4., 4., 4., 2., 2., 5., 1., 2., 2., 2., 5., 2., 2., 1.,\n",
       "        2., 4., 3., 4., 1., 4., 3., 4., 1., 4., 3., 3., 2., 1., 2., 3., 4.,\n",
       "        3., 1., 2., 2., 2., 3., 4., 3., 5., 1., 2., 2., 3., 1., 2., 1., 4.,\n",
       "        5., 4., 2., 5., 1., 5., 2., 2., 3., 1., 1., 1., 1., 4., 5., 3., 4.,\n",
       "        3., 3., 4., 4., 1., 1., 1., 1., 4., 2., 1., 1., 1., 1., 3., 2., 2.,\n",
       "        1., 3., 2., 1., 4., 4., 4., 4., 2., 4., 3., 2., 1., 1., 5., 4., 1.,\n",
       "        2., 1., 2., 2., 1., 4., 1., 4., 1., 1., 3., 1., 4., 3., 1., 5., 2.,\n",
       "        3., 2., 1., 2., 2., 1., 4., 4., 1., 2., 2., 5., 1., 2., 1., 1., 3.,\n",
       "        2., 1., 1., 1., 4., 2., 2., 4., 4., 4., 4., 4., 4., 1., 2., 4., 3.,\n",
       "        4., 4., 2., 3., 4., 4., 1., 2., 1., 1., 1., 1., 5., 3., 2., 3., 1.,\n",
       "        1., 5., 3., 4., 1., 1., 3., 5., 2., 1., 5., 2., 1., 2., 3., 1., 1.,\n",
       "        1., 4., 3., 3., 2., 1., 3., 4., 2., 3., 1., 3., 1., 2., 2., 4., 1.,\n",
       "        3., 3., 4., 1., 1., 1., 1., 1., 5., 2., 1., 5., 5., 1., 3., 2., 1.,\n",
       "        1., 4., 4., 4., 3., 2., 5., 1., 3., 1., 1., 2., 2., 1., 1., 4., 3.,\n",
       "        1., 5., 5., 1., 4., 4., 1., 2., 1., 2., 2., 3., 1., 1., 2., 1., 1.,\n",
       "        5., 4., 5., 3., 4., 5., 1., 2., 1., 2., 5., 4., 3., 3., 1., 1., 3.,\n",
       "        1., 2., 5., 1., 3., 2., 5., 1., 3., 2., 1., 2., 5., 2., 1., 2., 1.,\n",
       "        1., 2., 3., 1., 2., 3., 2., 2., 2., 5., 1., 4., 5., 1., 3., 4., 5.,\n",
       "        4., 2., 1., 1., 1., 2., 5., 2., 3., 4., 4., 5., 1., 2., 1., 2., 1.,\n",
       "        4., 5., 5., 1., 3., 5., 3., 1., 2., 1., 4., 1., 1., 5., 5., 2., 4.,\n",
       "        2., 1., 4., 2., 3., 4., 3., 4., 3., 2., 5., 4., 1., 3., 3., 3., 2.,\n",
       "        2., 3., 1., 1., 1., 1., 2., 1., 4., 2., 3., 3., 1., 2., 4., 2., 4.,\n",
       "        2., 1., 4., 4., 1., 1., 2., 5., 3., 1., 1., 3., 2., 4., 1., 4., 2.,\n",
       "        1., 4., 4., 2., 4., 1., 4., 4., 1., 5., 4., 2., 3., 1., 5., 1., 4.,\n",
       "        2., 1., 1., 1., 3., 5., 3., 2., 3., 1., 1., 2., 5., 4., 1., 3., 4.,\n",
       "        4., 1., 2., 4., 2., 5., 1., 3., 3., 3., 3., 2., 1., 4., 2., 2., 3.,\n",
       "        5., 3., 1., 1., 3., 5., 2., 5., 4., 3., 4., 1., 2., 1., 2., 5., 2.,\n",
       "        5., 3., 1., 3., 1., 1., 1., 2., 4., 2., 1., 4., 2., 2., 3., 1., 3.,\n",
       "        1., 3., 2., 1., 3., 5., 5., 4., 2., 3., 2., 5., 3., 1., 2., 4., 1.,\n",
       "        3., 2., 3., 1., 4., 1., 4., 2., 2., 2., 1., 1., 2., 5., 1., 1., 2.,\n",
       "        2., 1., 5., 5., 5., 1., 3., 2., 1., 1., 4., 1., 2., 4., 3., 5.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,x2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a63055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen offset: 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importantGenes = geneSelection(x2, n=2000)\n",
    "x2 = x2[:, importantGenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c842d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 17 features and 713 cells.\n"
     ]
    }
   ],
   "source": [
    "adata1 = sc.AnnData(x1)\n",
    "adata1 = read_dataset(adata1, copy=True)\n",
    "adata1 = preprocess_dataset(adata1, normalize_input=True, logtrans_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be802900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 713 × 17\n",
       "    obs: 'DCA_split', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f6d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 1999 features and 713 cells.\n"
     ]
    }
   ],
   "source": [
    "adata2 = sc.AnnData(x2)\n",
    "adata2 = read_dataset(adata2, copy=True)\n",
    "adata2 = preprocess_dataset(adata2, normalize_input=True, logtrans_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94e4b188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 713 × 1999\n",
       "    obs: 'DCA_split', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0c4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scMultiCluster(input_dim1=adata1.n_vars, input_dim2=adata2.n_vars, device='cuda').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246da131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scMultiCluster(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_gnn): ModuleList(\n",
       "      (0): GCNConv(2016, 1024)\n",
       "      (1): GCNConv(1024, 256)\n",
       "      (2): GCNConv(256, 64)\n",
       "      (3): GCNConv(64, 32)\n",
       "    )\n",
       "    (stacked_bns): ModuleList(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (stacked_prelus): ModuleList(\n",
       "      (0-3): 4 x PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): PReLU(num_parameters=1)\n",
       "    (6): Linear(in_features=1024, out_features=2016, bias=True)\n",
       "  )\n",
       "  (dec_mean): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=2016, bias=True)\n",
       "    (3): MeanAct()\n",
       "  )\n",
       "  (dec_disp): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=2016, bias=True)\n",
       "    (3): DispAct()\n",
       "  )\n",
       "  (dec_pi): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=2016, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (zinb_loss): ZINBLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdad7ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining stage\n",
      "Pretrain epoch 1, recon_loss:1.179670, zinb_loss:3.310244, adversial_loss:1.358601\n",
      "Pretrain epoch 2, recon_loss:1.062955, zinb_loss:2.777894, adversial_loss:1.378060\n",
      "Pretrain epoch 3, recon_loss:0.871985, zinb_loss:2.382323, adversial_loss:1.354497\n",
      "Pretrain epoch 4, recon_loss:0.773689, zinb_loss:2.086404, adversial_loss:1.358659\n",
      "Pretrain epoch 5, recon_loss:0.754927, zinb_loss:1.846736, adversial_loss:1.359718\n",
      "Pretrain epoch 6, recon_loss:0.738602, zinb_loss:1.645473, adversial_loss:1.362173\n",
      "Pretrain epoch 7, recon_loss:0.726697, zinb_loss:1.482812, adversial_loss:1.364967\n",
      "Pretrain epoch 8, recon_loss:0.719544, zinb_loss:1.358742, adversial_loss:1.366764\n",
      "Pretrain epoch 9, recon_loss:0.709003, zinb_loss:1.273030, adversial_loss:1.367524\n",
      "Pretrain epoch 10, recon_loss:0.696582, zinb_loss:1.220350, adversial_loss:1.366722\n",
      "Pretrain epoch 11, recon_loss:0.687043, zinb_loss:1.190452, adversial_loss:1.364308\n",
      "Pretrain epoch 12, recon_loss:0.682340, zinb_loss:1.172816, adversial_loss:1.360751\n",
      "Pretrain epoch 13, recon_loss:0.681515, zinb_loss:1.159930, adversial_loss:1.357939\n",
      "Pretrain epoch 14, recon_loss:0.681320, zinb_loss:1.148781, adversial_loss:1.356138\n",
      "Pretrain epoch 15, recon_loss:0.679157, zinb_loss:1.139289, adversial_loss:1.354849\n",
      "Pretrain epoch 16, recon_loss:0.675021, zinb_loss:1.131675, adversial_loss:1.352798\n",
      "Pretrain epoch 17, recon_loss:0.670823, zinb_loss:1.125151, adversial_loss:1.351018\n",
      "Pretrain epoch 18, recon_loss:0.667283, zinb_loss:1.118971, adversial_loss:1.349602\n",
      "Pretrain epoch 19, recon_loss:0.664511, zinb_loss:1.112700, adversial_loss:1.348588\n",
      "Pretrain epoch 20, recon_loss:0.662663, zinb_loss:1.106089, adversial_loss:1.348076\n",
      "Pretrain epoch 21, recon_loss:0.661110, zinb_loss:1.099249, adversial_loss:1.348316\n",
      "Pretrain epoch 22, recon_loss:0.658951, zinb_loss:1.092384, adversial_loss:1.348800\n",
      "Pretrain epoch 23, recon_loss:0.656703, zinb_loss:1.085614, adversial_loss:1.348673\n",
      "Pretrain epoch 24, recon_loss:0.655294, zinb_loss:1.079023, adversial_loss:1.348054\n",
      "Pretrain epoch 25, recon_loss:0.654199, zinb_loss:1.072597, adversial_loss:1.347170\n",
      "Pretrain epoch 26, recon_loss:0.652574, zinb_loss:1.066297, adversial_loss:1.346145\n",
      "Pretrain epoch 27, recon_loss:0.650772, zinb_loss:1.060202, adversial_loss:1.345213\n",
      "Pretrain epoch 28, recon_loss:0.649435, zinb_loss:1.054532, adversial_loss:1.344336\n",
      "Pretrain epoch 29, recon_loss:0.648298, zinb_loss:1.049417, adversial_loss:1.343489\n",
      "Pretrain epoch 30, recon_loss:0.646829, zinb_loss:1.044772, adversial_loss:1.342972\n",
      "Pretrain epoch 31, recon_loss:0.645332, zinb_loss:1.040430, adversial_loss:1.342796\n",
      "Pretrain epoch 32, recon_loss:0.644241, zinb_loss:1.036328, adversial_loss:1.342809\n",
      "Pretrain epoch 33, recon_loss:0.643215, zinb_loss:1.032467, adversial_loss:1.342788\n",
      "Pretrain epoch 34, recon_loss:0.642074, zinb_loss:1.028769, adversial_loss:1.342638\n",
      "Pretrain epoch 35, recon_loss:0.641071, zinb_loss:1.025113, adversial_loss:1.342482\n",
      "Pretrain epoch 36, recon_loss:0.640163, zinb_loss:1.021522, adversial_loss:1.342397\n",
      "Pretrain epoch 37, recon_loss:0.639288, zinb_loss:1.018017, adversial_loss:1.342303\n",
      "Pretrain epoch 38, recon_loss:0.638426, zinb_loss:1.014622, adversial_loss:1.342135\n",
      "Pretrain epoch 39, recon_loss:0.637576, zinb_loss:1.011409, adversial_loss:1.341934\n",
      "Pretrain epoch 40, recon_loss:0.636694, zinb_loss:1.008438, adversial_loss:1.341720\n",
      "Pretrain epoch 41, recon_loss:0.635766, zinb_loss:1.005659, adversial_loss:1.341465\n",
      "Pretrain epoch 42, recon_loss:0.634905, zinb_loss:1.003025, adversial_loss:1.341187\n",
      "Pretrain epoch 43, recon_loss:0.634058, zinb_loss:1.000516, adversial_loss:1.341050\n",
      "Pretrain epoch 44, recon_loss:0.633229, zinb_loss:0.998147, adversial_loss:1.341024\n",
      "Pretrain epoch 45, recon_loss:0.632442, zinb_loss:0.995903, adversial_loss:1.340829\n",
      "Pretrain epoch 46, recon_loss:0.631599, zinb_loss:0.993769, adversial_loss:1.340635\n",
      "Pretrain epoch 47, recon_loss:0.630659, zinb_loss:0.991710, adversial_loss:1.340537\n",
      "Pretrain epoch 48, recon_loss:0.629662, zinb_loss:0.989720, adversial_loss:1.340269\n",
      "Pretrain epoch 49, recon_loss:0.628648, zinb_loss:0.987807, adversial_loss:1.339893\n",
      "Pretrain epoch 50, recon_loss:0.627626, zinb_loss:0.985928, adversial_loss:1.339688\n",
      "Pretrain epoch 51, recon_loss:0.626739, zinb_loss:0.984143, adversial_loss:1.339351\n",
      "Pretrain epoch 52, recon_loss:0.625884, zinb_loss:0.982436, adversial_loss:1.338999\n",
      "Pretrain epoch 53, recon_loss:0.625040, zinb_loss:0.980834, adversial_loss:1.338772\n",
      "Pretrain epoch 54, recon_loss:0.624311, zinb_loss:0.979301, adversial_loss:1.338627\n",
      "Pretrain epoch 55, recon_loss:0.623501, zinb_loss:0.977937, adversial_loss:1.338070\n",
      "Pretrain epoch 56, recon_loss:0.623189, zinb_loss:0.977042, adversial_loss:1.338550\n",
      "Pretrain epoch 57, recon_loss:0.624883, zinb_loss:0.978697, adversial_loss:1.337574\n",
      "Pretrain epoch 58, recon_loss:0.625808, zinb_loss:0.979505, adversial_loss:1.339315\n",
      "Pretrain epoch 59, recon_loss:0.621240, zinb_loss:0.973353, adversial_loss:1.337903\n",
      "Pretrain epoch 60, recon_loss:0.624503, zinb_loss:0.976641, adversial_loss:1.336915\n",
      "Pretrain epoch 61, recon_loss:0.619977, zinb_loss:0.971225, adversial_loss:1.336525\n",
      "Pretrain epoch 62, recon_loss:0.621111, zinb_loss:0.972933, adversial_loss:1.336990\n",
      "Pretrain epoch 63, recon_loss:0.617995, zinb_loss:0.969049, adversial_loss:1.335981\n",
      "Pretrain epoch 64, recon_loss:0.620273, zinb_loss:0.970239, adversial_loss:1.334970\n",
      "Pretrain epoch 65, recon_loss:0.617281, zinb_loss:0.967614, adversial_loss:1.334586\n",
      "Pretrain epoch 66, recon_loss:0.617422, zinb_loss:0.967453, adversial_loss:1.335011\n",
      "Pretrain epoch 67, recon_loss:0.616019, zinb_loss:0.966292, adversial_loss:1.334306\n",
      "Pretrain epoch 68, recon_loss:0.615860, zinb_loss:0.964980, adversial_loss:1.332789\n",
      "Pretrain epoch 69, recon_loss:0.615344, zinb_loss:0.964691, adversial_loss:1.332406\n",
      "Pretrain epoch 70, recon_loss:0.614034, zinb_loss:0.963093, adversial_loss:1.332628\n",
      "Pretrain epoch 71, recon_loss:0.614142, zinb_loss:0.963050, adversial_loss:1.332156\n",
      "Pretrain epoch 72, recon_loss:0.612645, zinb_loss:0.961575, adversial_loss:1.331144\n",
      "Pretrain epoch 73, recon_loss:0.612783, zinb_loss:0.961119, adversial_loss:1.330740\n",
      "Pretrain epoch 74, recon_loss:0.611739, zinb_loss:0.960154, adversial_loss:1.330763\n",
      "Pretrain epoch 75, recon_loss:0.611251, zinb_loss:0.959587, adversial_loss:1.330719\n",
      "Pretrain epoch 76, recon_loss:0.610718, zinb_loss:0.959092, adversial_loss:1.330442\n",
      "Pretrain epoch 77, recon_loss:0.611144, zinb_loss:0.959081, adversial_loss:1.329869\n",
      "Pretrain epoch 78, recon_loss:0.610642, zinb_loss:0.958956, adversial_loss:1.329478\n",
      "Pretrain epoch 79, recon_loss:0.609389, zinb_loss:0.957217, adversial_loss:1.329208\n",
      "Pretrain epoch 80, recon_loss:0.608553, zinb_loss:0.955916, adversial_loss:1.328561\n",
      "Pretrain epoch 81, recon_loss:0.608286, zinb_loss:0.955695, adversial_loss:1.328209\n",
      "Pretrain epoch 82, recon_loss:0.608493, zinb_loss:0.955506, adversial_loss:1.328288\n",
      "Pretrain epoch 83, recon_loss:0.607743, zinb_loss:0.955074, adversial_loss:1.327965\n",
      "Pretrain epoch 84, recon_loss:0.606722, zinb_loss:0.953834, adversial_loss:1.327609\n",
      "Pretrain epoch 85, recon_loss:0.606245, zinb_loss:0.953079, adversial_loss:1.327005\n",
      "Pretrain epoch 86, recon_loss:0.605927, zinb_loss:0.953123, adversial_loss:1.326808\n",
      "Pretrain epoch 87, recon_loss:0.606177, zinb_loss:0.952994, adversial_loss:1.327340\n",
      "Pretrain epoch 88, recon_loss:0.605816, zinb_loss:0.952921, adversial_loss:1.327046\n",
      "Pretrain epoch 89, recon_loss:0.604161, zinb_loss:0.951316, adversial_loss:1.326727\n",
      "Pretrain epoch 90, recon_loss:0.603773, zinb_loss:0.950757, adversial_loss:1.326112\n",
      "Pretrain epoch 91, recon_loss:0.603061, zinb_loss:0.950318, adversial_loss:1.326290\n",
      "Pretrain epoch 92, recon_loss:0.602661, zinb_loss:0.950190, adversial_loss:1.326443\n",
      "Pretrain epoch 93, recon_loss:0.602419, zinb_loss:0.949436, adversial_loss:1.325972\n",
      "Pretrain epoch 94, recon_loss:0.601898, zinb_loss:0.948998, adversial_loss:1.326147\n",
      "Pretrain epoch 95, recon_loss:0.600912, zinb_loss:0.948375, adversial_loss:1.326035\n",
      "Pretrain epoch 96, recon_loss:0.600444, zinb_loss:0.947877, adversial_loss:1.325948\n",
      "Pretrain epoch 97, recon_loss:0.600224, zinb_loss:0.947271, adversial_loss:1.325759\n",
      "Pretrain epoch 98, recon_loss:0.599508, zinb_loss:0.946916, adversial_loss:1.326016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 99, recon_loss:0.599251, zinb_loss:0.946953, adversial_loss:1.325801\n",
      "Pretrain epoch 100, recon_loss:0.599830, zinb_loss:0.947820, adversial_loss:1.326051\n",
      "Pretrain epoch 101, recon_loss:0.603356, zinb_loss:0.953134, adversial_loss:1.325571\n",
      "Pretrain epoch 102, recon_loss:0.601549, zinb_loss:0.951658, adversial_loss:1.326208\n",
      "Pretrain epoch 103, recon_loss:0.598446, zinb_loss:0.946781, adversial_loss:1.326028\n",
      "Pretrain epoch 104, recon_loss:0.600954, zinb_loss:0.947707, adversial_loss:1.325926\n",
      "Pretrain epoch 105, recon_loss:0.599181, zinb_loss:0.948031, adversial_loss:1.326072\n",
      "Pretrain epoch 106, recon_loss:0.597465, zinb_loss:0.945269, adversial_loss:1.325966\n",
      "Pretrain epoch 107, recon_loss:0.596991, zinb_loss:0.945863, adversial_loss:1.325786\n",
      "Pretrain epoch 108, recon_loss:0.596227, zinb_loss:0.944915, adversial_loss:1.325327\n",
      "Pretrain epoch 109, recon_loss:0.595574, zinb_loss:0.943895, adversial_loss:1.324937\n",
      "Pretrain epoch 110, recon_loss:0.594890, zinb_loss:0.944395, adversial_loss:1.325032\n",
      "Pretrain epoch 111, recon_loss:0.594437, zinb_loss:0.943066, adversial_loss:1.325293\n",
      "Pretrain epoch 112, recon_loss:0.593870, zinb_loss:0.943514, adversial_loss:1.325300\n",
      "Pretrain epoch 113, recon_loss:0.594452, zinb_loss:0.943376, adversial_loss:1.324573\n",
      "Pretrain epoch 114, recon_loss:0.592870, zinb_loss:0.942202, adversial_loss:1.324600\n",
      "Pretrain epoch 115, recon_loss:0.592531, zinb_loss:0.942643, adversial_loss:1.324774\n",
      "Pretrain epoch 116, recon_loss:0.592751, zinb_loss:0.941981, adversial_loss:1.324517\n",
      "Pretrain epoch 117, recon_loss:0.591123, zinb_loss:0.941256, adversial_loss:1.324218\n",
      "Pretrain epoch 118, recon_loss:0.591629, zinb_loss:0.941334, adversial_loss:1.324183\n",
      "Pretrain epoch 119, recon_loss:0.590549, zinb_loss:0.941117, adversial_loss:1.323596\n",
      "Pretrain epoch 120, recon_loss:0.590122, zinb_loss:0.940474, adversial_loss:1.323805\n",
      "Pretrain epoch 121, recon_loss:0.589967, zinb_loss:0.940925, adversial_loss:1.323893\n",
      "Pretrain epoch 122, recon_loss:0.588659, zinb_loss:0.940049, adversial_loss:1.322962\n",
      "Pretrain epoch 123, recon_loss:0.588362, zinb_loss:0.939506, adversial_loss:1.323459\n",
      "Pretrain epoch 124, recon_loss:0.588067, zinb_loss:0.939818, adversial_loss:1.323779\n",
      "Pretrain epoch 125, recon_loss:0.587759, zinb_loss:0.939649, adversial_loss:1.323031\n",
      "Pretrain epoch 126, recon_loss:0.587280, zinb_loss:0.938874, adversial_loss:1.323188\n",
      "Pretrain epoch 127, recon_loss:0.586054, zinb_loss:0.938369, adversial_loss:1.323246\n",
      "Pretrain epoch 128, recon_loss:0.585966, zinb_loss:0.938435, adversial_loss:1.322833\n",
      "Pretrain epoch 129, recon_loss:0.585517, zinb_loss:0.938540, adversial_loss:1.323108\n",
      "Pretrain epoch 130, recon_loss:0.585000, zinb_loss:0.938212, adversial_loss:1.322369\n",
      "Pretrain epoch 131, recon_loss:0.584143, zinb_loss:0.937455, adversial_loss:1.322422\n",
      "Pretrain epoch 132, recon_loss:0.583565, zinb_loss:0.937159, adversial_loss:1.322457\n",
      "Pretrain epoch 133, recon_loss:0.583039, zinb_loss:0.937191, adversial_loss:1.322075\n",
      "Pretrain epoch 134, recon_loss:0.583330, zinb_loss:0.937278, adversial_loss:1.322357\n",
      "Pretrain epoch 135, recon_loss:0.583211, zinb_loss:0.938083, adversial_loss:1.321299\n",
      "Pretrain epoch 136, recon_loss:0.585884, zinb_loss:0.939603, adversial_loss:1.322456\n",
      "Pretrain epoch 137, recon_loss:0.591888, zinb_loss:0.944675, adversial_loss:1.322145\n",
      "Pretrain epoch 138, recon_loss:0.588202, zinb_loss:0.939074, adversial_loss:1.321496\n",
      "Pretrain epoch 139, recon_loss:0.584226, zinb_loss:0.940763, adversial_loss:1.321704\n",
      "Pretrain epoch 140, recon_loss:0.589514, zinb_loss:0.941872, adversial_loss:1.321858\n",
      "Pretrain epoch 141, recon_loss:0.585116, zinb_loss:0.937335, adversial_loss:1.321042\n",
      "Pretrain epoch 142, recon_loss:0.588871, zinb_loss:0.940078, adversial_loss:1.321000\n",
      "Pretrain epoch 143, recon_loss:0.587389, zinb_loss:0.940116, adversial_loss:1.320054\n",
      "Pretrain epoch 144, recon_loss:0.587335, zinb_loss:0.938293, adversial_loss:1.321286\n",
      "Pretrain epoch 145, recon_loss:0.586244, zinb_loss:0.938670, adversial_loss:1.320997\n",
      "Pretrain epoch 146, recon_loss:0.585595, zinb_loss:0.936979, adversial_loss:1.320920\n",
      "Pretrain epoch 147, recon_loss:0.585582, zinb_loss:0.937532, adversial_loss:1.320655\n",
      "Pretrain epoch 148, recon_loss:0.582787, zinb_loss:0.935678, adversial_loss:1.320554\n",
      "Pretrain epoch 149, recon_loss:0.583903, zinb_loss:0.936797, adversial_loss:1.320807\n",
      "Pretrain epoch 150, recon_loss:0.581280, zinb_loss:0.935497, adversial_loss:1.320568\n",
      "Pretrain epoch 151, recon_loss:0.580431, zinb_loss:0.935619, adversial_loss:1.320076\n",
      "Pretrain epoch 152, recon_loss:0.580515, zinb_loss:0.935298, adversial_loss:1.320134\n",
      "Pretrain epoch 153, recon_loss:0.579342, zinb_loss:0.934485, adversial_loss:1.320492\n",
      "Pretrain epoch 154, recon_loss:0.578965, zinb_loss:0.934790, adversial_loss:1.320196\n",
      "Pretrain epoch 155, recon_loss:0.577320, zinb_loss:0.933857, adversial_loss:1.319620\n",
      "Pretrain epoch 156, recon_loss:0.577659, zinb_loss:0.934269, adversial_loss:1.319570\n",
      "Pretrain epoch 157, recon_loss:0.575932, zinb_loss:0.933481, adversial_loss:1.319721\n",
      "Pretrain epoch 158, recon_loss:0.576145, zinb_loss:0.933484, adversial_loss:1.319808\n",
      "Pretrain epoch 159, recon_loss:0.574786, zinb_loss:0.933279, adversial_loss:1.320073\n",
      "Pretrain epoch 160, recon_loss:0.573829, zinb_loss:0.932811, adversial_loss:1.319939\n",
      "Pretrain epoch 161, recon_loss:0.573508, zinb_loss:0.932906, adversial_loss:1.319846\n",
      "Pretrain epoch 162, recon_loss:0.572435, zinb_loss:0.932418, adversial_loss:1.320066\n",
      "Pretrain epoch 163, recon_loss:0.572124, zinb_loss:0.932359, adversial_loss:1.319934\n",
      "Pretrain epoch 164, recon_loss:0.571390, zinb_loss:0.932270, adversial_loss:1.319542\n",
      "Pretrain epoch 165, recon_loss:0.570449, zinb_loss:0.931862, adversial_loss:1.319675\n",
      "Pretrain epoch 166, recon_loss:0.569934, zinb_loss:0.931864, adversial_loss:1.319872\n",
      "Pretrain epoch 167, recon_loss:0.569513, zinb_loss:0.931678, adversial_loss:1.319690\n",
      "Pretrain epoch 168, recon_loss:0.568565, zinb_loss:0.931405, adversial_loss:1.319634\n",
      "Pretrain epoch 169, recon_loss:0.568135, zinb_loss:0.931305, adversial_loss:1.319482\n",
      "Pretrain epoch 170, recon_loss:0.567389, zinb_loss:0.931162, adversial_loss:1.319424\n",
      "Pretrain epoch 171, recon_loss:0.566791, zinb_loss:0.930929, adversial_loss:1.319420\n",
      "Pretrain epoch 172, recon_loss:0.566742, zinb_loss:0.930844, adversial_loss:1.319378\n",
      "Pretrain epoch 173, recon_loss:0.566946, zinb_loss:0.931119, adversial_loss:1.319516\n",
      "Pretrain epoch 174, recon_loss:0.568139, zinb_loss:0.932459, adversial_loss:1.318569\n",
      "Pretrain epoch 175, recon_loss:0.571693, zinb_loss:0.938967, adversial_loss:1.320055\n",
      "Pretrain epoch 176, recon_loss:0.578599, zinb_loss:0.946456, adversial_loss:1.316493\n",
      "Pretrain epoch 177, recon_loss:0.574376, zinb_loss:0.933795, adversial_loss:1.319191\n",
      "Pretrain epoch 178, recon_loss:0.572003, zinb_loss:0.935200, adversial_loss:1.319939\n",
      "Pretrain epoch 179, recon_loss:0.573550, zinb_loss:0.936789, adversial_loss:1.318466\n",
      "Pretrain epoch 180, recon_loss:0.571959, zinb_loss:0.932289, adversial_loss:1.318486\n",
      "Pretrain epoch 181, recon_loss:0.571457, zinb_loss:0.935055, adversial_loss:1.319376\n",
      "Pretrain epoch 182, recon_loss:0.571979, zinb_loss:0.931832, adversial_loss:1.319013\n",
      "Pretrain epoch 183, recon_loss:0.568420, zinb_loss:0.933174, adversial_loss:1.318471\n",
      "Pretrain epoch 184, recon_loss:0.569827, zinb_loss:0.931224, adversial_loss:1.318296\n",
      "Pretrain epoch 185, recon_loss:0.568148, zinb_loss:0.932147, adversial_loss:1.318788\n",
      "Pretrain epoch 186, recon_loss:0.566385, zinb_loss:0.930525, adversial_loss:1.319118\n",
      "Pretrain epoch 187, recon_loss:0.566219, zinb_loss:0.931619, adversial_loss:1.318574\n",
      "Pretrain epoch 188, recon_loss:0.564485, zinb_loss:0.929994, adversial_loss:1.318262\n",
      "Pretrain epoch 189, recon_loss:0.564128, zinb_loss:0.930798, adversial_loss:1.318450\n",
      "Pretrain epoch 190, recon_loss:0.562395, zinb_loss:0.929507, adversial_loss:1.318254\n",
      "Pretrain epoch 191, recon_loss:0.562345, zinb_loss:0.930308, adversial_loss:1.318470\n",
      "Pretrain epoch 192, recon_loss:0.560477, zinb_loss:0.929261, adversial_loss:1.318296\n",
      "Pretrain epoch 193, recon_loss:0.560974, zinb_loss:0.929577, adversial_loss:1.318433\n",
      "Pretrain epoch 194, recon_loss:0.558242, zinb_loss:0.928910, adversial_loss:1.318018\n",
      "Pretrain epoch 195, recon_loss:0.559158, zinb_loss:0.929054, adversial_loss:1.317947\n",
      "Pretrain epoch 196, recon_loss:0.556651, zinb_loss:0.928675, adversial_loss:1.318423\n",
      "Pretrain epoch 197, recon_loss:0.557648, zinb_loss:0.928549, adversial_loss:1.318386\n",
      "Pretrain epoch 198, recon_loss:0.555929, zinb_loss:0.928417, adversial_loss:1.318300\n",
      "Pretrain epoch 199, recon_loss:0.555953, zinb_loss:0.928338, adversial_loss:1.318035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 200, recon_loss:0.556295, zinb_loss:0.928326, adversial_loss:1.318726\n",
      "Pretrain epoch 201, recon_loss:0.556826, zinb_loss:0.928433, adversial_loss:1.317938\n",
      "Pretrain epoch 202, recon_loss:0.557281, zinb_loss:0.928662, adversial_loss:1.318517\n",
      "Pretrain epoch 203, recon_loss:0.553383, zinb_loss:0.927767, adversial_loss:1.318458\n",
      "Pretrain epoch 204, recon_loss:0.554826, zinb_loss:0.927991, adversial_loss:1.318504\n",
      "Pretrain epoch 205, recon_loss:0.554522, zinb_loss:0.928039, adversial_loss:1.318592\n",
      "Pretrain epoch 206, recon_loss:0.553223, zinb_loss:0.927900, adversial_loss:1.317863\n",
      "Pretrain epoch 207, recon_loss:0.552966, zinb_loss:0.927852, adversial_loss:1.318486\n",
      "Pretrain epoch 208, recon_loss:0.550836, zinb_loss:0.927182, adversial_loss:1.318747\n",
      "Pretrain epoch 209, recon_loss:0.550463, zinb_loss:0.927405, adversial_loss:1.318093\n",
      "Pretrain epoch 210, recon_loss:0.551010, zinb_loss:0.927393, adversial_loss:1.318170\n",
      "Pretrain epoch 211, recon_loss:0.548634, zinb_loss:0.926882, adversial_loss:1.318250\n",
      "Pretrain epoch 212, recon_loss:0.548539, zinb_loss:0.926851, adversial_loss:1.318599\n",
      "Pretrain epoch 213, recon_loss:0.547597, zinb_loss:0.926589, adversial_loss:1.318089\n",
      "Pretrain epoch 214, recon_loss:0.546600, zinb_loss:0.926519, adversial_loss:1.317812\n",
      "Pretrain epoch 215, recon_loss:0.546891, zinb_loss:0.926602, adversial_loss:1.318441\n",
      "Pretrain epoch 216, recon_loss:0.545596, zinb_loss:0.926449, adversial_loss:1.317914\n",
      "Pretrain epoch 217, recon_loss:0.545260, zinb_loss:0.926222, adversial_loss:1.317746\n",
      "Pretrain epoch 218, recon_loss:0.544113, zinb_loss:0.926168, adversial_loss:1.317684\n",
      "Pretrain epoch 219, recon_loss:0.543740, zinb_loss:0.925994, adversial_loss:1.318053\n",
      "Pretrain epoch 220, recon_loss:0.543161, zinb_loss:0.925795, adversial_loss:1.317388\n",
      "Pretrain epoch 221, recon_loss:0.543730, zinb_loss:0.925878, adversial_loss:1.318129\n",
      "Pretrain epoch 222, recon_loss:0.544425, zinb_loss:0.926189, adversial_loss:1.317281\n",
      "Pretrain epoch 223, recon_loss:0.544687, zinb_loss:0.926471, adversial_loss:1.318306\n",
      "Pretrain epoch 224, recon_loss:0.546995, zinb_loss:0.927224, adversial_loss:1.317300\n",
      "Pretrain epoch 225, recon_loss:0.542279, zinb_loss:0.926076, adversial_loss:1.317069\n",
      "Pretrain epoch 226, recon_loss:0.545609, zinb_loss:0.926624, adversial_loss:1.318129\n",
      "Pretrain epoch 227, recon_loss:0.548064, zinb_loss:0.927746, adversial_loss:1.318268\n",
      "Pretrain epoch 228, recon_loss:0.546001, zinb_loss:0.926407, adversial_loss:1.316518\n",
      "Pretrain epoch 229, recon_loss:0.542279, zinb_loss:0.925806, adversial_loss:1.316952\n",
      "Pretrain epoch 230, recon_loss:0.543594, zinb_loss:0.926451, adversial_loss:1.317672\n",
      "Pretrain epoch 231, recon_loss:0.541450, zinb_loss:0.925328, adversial_loss:1.317301\n",
      "Pretrain epoch 232, recon_loss:0.541059, zinb_loss:0.925699, adversial_loss:1.316569\n",
      "Pretrain epoch 233, recon_loss:0.540221, zinb_loss:0.925569, adversial_loss:1.317716\n",
      "Pretrain epoch 234, recon_loss:0.538457, zinb_loss:0.924863, adversial_loss:1.317414\n",
      "Pretrain epoch 235, recon_loss:0.537611, zinb_loss:0.924987, adversial_loss:1.316994\n",
      "Pretrain epoch 236, recon_loss:0.537525, zinb_loss:0.924812, adversial_loss:1.317146\n",
      "Pretrain epoch 237, recon_loss:0.536222, zinb_loss:0.924583, adversial_loss:1.317614\n",
      "Pretrain epoch 238, recon_loss:0.535985, zinb_loss:0.924527, adversial_loss:1.317148\n",
      "Pretrain epoch 239, recon_loss:0.535932, zinb_loss:0.924623, adversial_loss:1.317532\n",
      "Pretrain epoch 240, recon_loss:0.537979, zinb_loss:0.924789, adversial_loss:1.316965\n",
      "Pretrain epoch 241, recon_loss:0.538552, zinb_loss:0.925525, adversial_loss:1.317767\n",
      "Pretrain epoch 242, recon_loss:0.537650, zinb_loss:0.925833, adversial_loss:1.317055\n",
      "Pretrain epoch 243, recon_loss:0.536864, zinb_loss:0.925185, adversial_loss:1.317904\n",
      "Pretrain epoch 244, recon_loss:0.532933, zinb_loss:0.924555, adversial_loss:1.317111\n",
      "Pretrain epoch 245, recon_loss:0.529799, zinb_loss:0.923923, adversial_loss:1.317291\n",
      "Pretrain epoch 246, recon_loss:0.531587, zinb_loss:0.923998, adversial_loss:1.317889\n",
      "Pretrain epoch 247, recon_loss:0.529088, zinb_loss:0.924273, adversial_loss:1.317168\n",
      "Pretrain epoch 248, recon_loss:0.527879, zinb_loss:0.924031, adversial_loss:1.317170\n",
      "Pretrain epoch 249, recon_loss:0.528247, zinb_loss:0.923883, adversial_loss:1.317500\n",
      "Pretrain epoch 250, recon_loss:0.527663, zinb_loss:0.923636, adversial_loss:1.317287\n",
      "Pretrain epoch 251, recon_loss:0.527162, zinb_loss:0.923433, adversial_loss:1.317235\n",
      "Pretrain epoch 252, recon_loss:0.527396, zinb_loss:0.923465, adversial_loss:1.317456\n",
      "Pretrain epoch 253, recon_loss:0.524822, zinb_loss:0.923198, adversial_loss:1.317127\n",
      "Pretrain epoch 254, recon_loss:0.523319, zinb_loss:0.923117, adversial_loss:1.317246\n",
      "Pretrain epoch 255, recon_loss:0.522798, zinb_loss:0.922944, adversial_loss:1.317391\n",
      "Pretrain epoch 256, recon_loss:0.521215, zinb_loss:0.922773, adversial_loss:1.316932\n",
      "Pretrain epoch 257, recon_loss:0.520658, zinb_loss:0.922773, adversial_loss:1.317104\n",
      "Pretrain epoch 258, recon_loss:0.520514, zinb_loss:0.922834, adversial_loss:1.317351\n",
      "Pretrain epoch 259, recon_loss:0.519256, zinb_loss:0.922616, adversial_loss:1.316832\n",
      "Pretrain epoch 260, recon_loss:0.519342, zinb_loss:0.922528, adversial_loss:1.317010\n",
      "Pretrain epoch 261, recon_loss:0.519725, zinb_loss:0.922337, adversial_loss:1.317155\n",
      "Pretrain epoch 262, recon_loss:0.519089, zinb_loss:0.922267, adversial_loss:1.316971\n",
      "Pretrain epoch 263, recon_loss:0.519837, zinb_loss:0.922366, adversial_loss:1.316986\n",
      "Pretrain epoch 264, recon_loss:0.524179, zinb_loss:0.923032, adversial_loss:1.317157\n",
      "Pretrain epoch 265, recon_loss:0.523875, zinb_loss:0.923220, adversial_loss:1.316804\n",
      "Pretrain epoch 266, recon_loss:0.521955, zinb_loss:0.923852, adversial_loss:1.317354\n",
      "Pretrain epoch 267, recon_loss:0.524000, zinb_loss:0.924772, adversial_loss:1.316683\n",
      "Pretrain epoch 268, recon_loss:0.522340, zinb_loss:0.923961, adversial_loss:1.317206\n",
      "Pretrain epoch 269, recon_loss:0.518644, zinb_loss:0.922934, adversial_loss:1.316741\n",
      "Pretrain epoch 270, recon_loss:0.517740, zinb_loss:0.922370, adversial_loss:1.317008\n",
      "Pretrain epoch 271, recon_loss:0.518627, zinb_loss:0.922516, adversial_loss:1.316924\n",
      "Pretrain epoch 272, recon_loss:0.517051, zinb_loss:0.922320, adversial_loss:1.316862\n",
      "Pretrain epoch 273, recon_loss:0.519194, zinb_loss:0.922144, adversial_loss:1.316944\n",
      "Pretrain epoch 274, recon_loss:0.516150, zinb_loss:0.922100, adversial_loss:1.316448\n",
      "Pretrain epoch 275, recon_loss:0.513899, zinb_loss:0.921746, adversial_loss:1.317122\n",
      "Pretrain epoch 276, recon_loss:0.513299, zinb_loss:0.921512, adversial_loss:1.316811\n",
      "Pretrain epoch 277, recon_loss:0.512642, zinb_loss:0.921424, adversial_loss:1.316843\n",
      "Pretrain epoch 278, recon_loss:0.510695, zinb_loss:0.921419, adversial_loss:1.317004\n",
      "Pretrain epoch 279, recon_loss:0.508264, zinb_loss:0.921364, adversial_loss:1.316523\n",
      "Pretrain epoch 280, recon_loss:0.508498, zinb_loss:0.921560, adversial_loss:1.316703\n",
      "Pretrain epoch 281, recon_loss:0.509618, zinb_loss:0.921853, adversial_loss:1.317027\n",
      "Pretrain epoch 282, recon_loss:0.513050, zinb_loss:0.922330, adversial_loss:1.316376\n",
      "Pretrain epoch 283, recon_loss:0.511610, zinb_loss:0.922252, adversial_loss:1.317172\n",
      "Pretrain epoch 284, recon_loss:0.507078, zinb_loss:0.921214, adversial_loss:1.316872\n",
      "Pretrain epoch 285, recon_loss:0.505531, zinb_loss:0.920937, adversial_loss:1.316401\n",
      "Pretrain epoch 286, recon_loss:0.505513, zinb_loss:0.921213, adversial_loss:1.316840\n",
      "Pretrain epoch 287, recon_loss:0.507545, zinb_loss:0.921162, adversial_loss:1.316440\n",
      "Pretrain epoch 288, recon_loss:0.506105, zinb_loss:0.920782, adversial_loss:1.317108\n",
      "Pretrain epoch 289, recon_loss:0.505419, zinb_loss:0.921109, adversial_loss:1.316038\n",
      "Pretrain epoch 290, recon_loss:0.506616, zinb_loss:0.921939, adversial_loss:1.317173\n",
      "Pretrain epoch 291, recon_loss:0.505529, zinb_loss:0.921815, adversial_loss:1.316478\n",
      "Pretrain epoch 292, recon_loss:0.505110, zinb_loss:0.921468, adversial_loss:1.316756\n",
      "Pretrain epoch 293, recon_loss:0.505608, zinb_loss:0.920986, adversial_loss:1.316142\n",
      "Pretrain epoch 294, recon_loss:0.505306, zinb_loss:0.921074, adversial_loss:1.317090\n",
      "Pretrain epoch 295, recon_loss:0.506906, zinb_loss:0.921153, adversial_loss:1.316524\n",
      "Pretrain epoch 296, recon_loss:0.511468, zinb_loss:0.921335, adversial_loss:1.316224\n",
      "Pretrain epoch 297, recon_loss:0.501392, zinb_loss:0.920377, adversial_loss:1.316484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 298, recon_loss:0.504579, zinb_loss:0.920536, adversial_loss:1.316699\n",
      "Pretrain epoch 299, recon_loss:0.501819, zinb_loss:0.920494, adversial_loss:1.316207\n",
      "Pretrain epoch 300, recon_loss:0.502742, zinb_loss:0.920309, adversial_loss:1.315534\n",
      "Pretrain epoch 301, recon_loss:0.498152, zinb_loss:0.920250, adversial_loss:1.316682\n",
      "Pretrain epoch 302, recon_loss:0.499818, zinb_loss:0.920063, adversial_loss:1.316222\n",
      "Pretrain epoch 303, recon_loss:0.497229, zinb_loss:0.920042, adversial_loss:1.315893\n",
      "Pretrain epoch 304, recon_loss:0.497872, zinb_loss:0.920081, adversial_loss:1.315936\n",
      "Pretrain epoch 305, recon_loss:0.495712, zinb_loss:0.920197, adversial_loss:1.316161\n",
      "Pretrain epoch 306, recon_loss:0.495108, zinb_loss:0.920478, adversial_loss:1.316109\n",
      "Pretrain epoch 307, recon_loss:0.494740, zinb_loss:0.920314, adversial_loss:1.316329\n",
      "Pretrain epoch 308, recon_loss:0.492965, zinb_loss:0.919857, adversial_loss:1.315697\n",
      "Pretrain epoch 309, recon_loss:0.489529, zinb_loss:0.919440, adversial_loss:1.316327\n",
      "Pretrain epoch 310, recon_loss:0.487808, zinb_loss:0.919173, adversial_loss:1.316113\n",
      "Pretrain epoch 311, recon_loss:0.485521, zinb_loss:0.919095, adversial_loss:1.316097\n",
      "Pretrain epoch 312, recon_loss:0.486179, zinb_loss:0.919246, adversial_loss:1.315838\n",
      "Pretrain epoch 313, recon_loss:0.487542, zinb_loss:0.919392, adversial_loss:1.316431\n",
      "Pretrain epoch 314, recon_loss:0.493211, zinb_loss:0.919868, adversial_loss:1.316113\n",
      "Pretrain epoch 315, recon_loss:0.490868, zinb_loss:0.919789, adversial_loss:1.315854\n",
      "Pretrain epoch 316, recon_loss:0.486745, zinb_loss:0.919250, adversial_loss:1.316330\n",
      "Pretrain epoch 317, recon_loss:0.486327, zinb_loss:0.919163, adversial_loss:1.315580\n",
      "Pretrain epoch 318, recon_loss:0.493551, zinb_loss:0.919754, adversial_loss:1.316877\n",
      "Pretrain epoch 319, recon_loss:0.492165, zinb_loss:0.919524, adversial_loss:1.315524\n",
      "Pretrain epoch 320, recon_loss:0.483543, zinb_loss:0.918743, adversial_loss:1.316434\n",
      "Pretrain epoch 321, recon_loss:0.483798, zinb_loss:0.918870, adversial_loss:1.315937\n",
      "Pretrain epoch 322, recon_loss:0.483566, zinb_loss:0.918912, adversial_loss:1.315669\n",
      "Pretrain epoch 323, recon_loss:0.480374, zinb_loss:0.918655, adversial_loss:1.316381\n",
      "Pretrain epoch 324, recon_loss:0.479277, zinb_loss:0.918554, adversial_loss:1.315806\n",
      "Pretrain epoch 325, recon_loss:0.478434, zinb_loss:0.918513, adversial_loss:1.316006\n",
      "Pretrain epoch 326, recon_loss:0.476349, zinb_loss:0.918342, adversial_loss:1.316175\n",
      "Pretrain epoch 327, recon_loss:0.476252, zinb_loss:0.918546, adversial_loss:1.315907\n",
      "Pretrain epoch 328, recon_loss:0.477026, zinb_loss:0.918768, adversial_loss:1.315566\n",
      "Pretrain epoch 329, recon_loss:0.474876, zinb_loss:0.918467, adversial_loss:1.316380\n",
      "Pretrain epoch 330, recon_loss:0.478095, zinb_loss:0.918713, adversial_loss:1.315314\n",
      "Pretrain epoch 331, recon_loss:0.483041, zinb_loss:0.919150, adversial_loss:1.316255\n",
      "Pretrain epoch 332, recon_loss:0.493432, zinb_loss:0.920625, adversial_loss:1.315266\n",
      "Pretrain epoch 333, recon_loss:0.492134, zinb_loss:0.920343, adversial_loss:1.316202\n",
      "Pretrain epoch 334, recon_loss:0.487001, zinb_loss:0.919232, adversial_loss:1.315973\n",
      "Pretrain epoch 335, recon_loss:0.482334, zinb_loss:0.918334, adversial_loss:1.314915\n",
      "Pretrain epoch 336, recon_loss:0.479596, zinb_loss:0.918429, adversial_loss:1.316236\n",
      "Pretrain epoch 337, recon_loss:0.479513, zinb_loss:0.918395, adversial_loss:1.315440\n",
      "Pretrain epoch 338, recon_loss:0.474574, zinb_loss:0.917958, adversial_loss:1.314971\n",
      "Pretrain epoch 339, recon_loss:0.475439, zinb_loss:0.917919, adversial_loss:1.316367\n",
      "Pretrain epoch 340, recon_loss:0.473957, zinb_loss:0.918038, adversial_loss:1.315297\n",
      "Pretrain epoch 341, recon_loss:0.470696, zinb_loss:0.918220, adversial_loss:1.315433\n",
      "Pretrain epoch 342, recon_loss:0.475894, zinb_loss:0.919046, adversial_loss:1.315469\n",
      "Pretrain epoch 343, recon_loss:0.475328, zinb_loss:0.919523, adversial_loss:1.316094\n",
      "Pretrain epoch 344, recon_loss:0.477536, zinb_loss:0.919628, adversial_loss:1.315055\n",
      "Pretrain epoch 345, recon_loss:0.473704, zinb_loss:0.919144, adversial_loss:1.316263\n",
      "Pretrain epoch 346, recon_loss:0.471960, zinb_loss:0.918387, adversial_loss:1.315215\n",
      "Pretrain epoch 347, recon_loss:0.471955, zinb_loss:0.918104, adversial_loss:1.316029\n",
      "Pretrain epoch 348, recon_loss:0.467907, zinb_loss:0.917627, adversial_loss:1.315676\n",
      "Pretrain epoch 349, recon_loss:0.464877, zinb_loss:0.917377, adversial_loss:1.315572\n",
      "Pretrain epoch 350, recon_loss:0.465813, zinb_loss:0.917567, adversial_loss:1.315661\n",
      "Pretrain epoch 351, recon_loss:0.464266, zinb_loss:0.917478, adversial_loss:1.315469\n",
      "Pretrain epoch 352, recon_loss:0.460796, zinb_loss:0.917181, adversial_loss:1.315619\n",
      "Pretrain epoch 353, recon_loss:0.458481, zinb_loss:0.917135, adversial_loss:1.315719\n",
      "Pretrain epoch 354, recon_loss:0.457424, zinb_loss:0.917027, adversial_loss:1.315029\n",
      "Pretrain epoch 355, recon_loss:0.456368, zinb_loss:0.916966, adversial_loss:1.315818\n",
      "Pretrain epoch 356, recon_loss:0.454883, zinb_loss:0.916875, adversial_loss:1.315193\n",
      "Pretrain epoch 357, recon_loss:0.453456, zinb_loss:0.916733, adversial_loss:1.315723\n",
      "Pretrain epoch 358, recon_loss:0.453091, zinb_loss:0.916732, adversial_loss:1.315237\n",
      "Pretrain epoch 359, recon_loss:0.455560, zinb_loss:0.916942, adversial_loss:1.315333\n",
      "Pretrain epoch 360, recon_loss:0.457770, zinb_loss:0.917186, adversial_loss:1.315678\n",
      "Pretrain epoch 361, recon_loss:0.456999, zinb_loss:0.917371, adversial_loss:1.315232\n",
      "Pretrain epoch 362, recon_loss:0.451816, zinb_loss:0.916946, adversial_loss:1.314946\n",
      "Pretrain epoch 363, recon_loss:0.449474, zinb_loss:0.916953, adversial_loss:1.315576\n",
      "Pretrain epoch 364, recon_loss:0.450918, zinb_loss:0.917036, adversial_loss:1.314645\n",
      "Pretrain epoch 365, recon_loss:0.450310, zinb_loss:0.916671, adversial_loss:1.315144\n",
      "Pretrain epoch 366, recon_loss:0.448690, zinb_loss:0.916414, adversial_loss:1.315175\n",
      "Pretrain epoch 367, recon_loss:0.446805, zinb_loss:0.916271, adversial_loss:1.314755\n",
      "Pretrain epoch 368, recon_loss:0.449454, zinb_loss:0.916481, adversial_loss:1.315070\n",
      "Pretrain epoch 369, recon_loss:0.452361, zinb_loss:0.916807, adversial_loss:1.315162\n",
      "Pretrain epoch 370, recon_loss:0.452025, zinb_loss:0.916952, adversial_loss:1.314653\n",
      "Pretrain epoch 371, recon_loss:0.450536, zinb_loss:0.917295, adversial_loss:1.315140\n",
      "Pretrain epoch 372, recon_loss:0.452883, zinb_loss:0.918052, adversial_loss:1.314835\n",
      "Pretrain epoch 373, recon_loss:0.448106, zinb_loss:0.917795, adversial_loss:1.314907\n",
      "Pretrain epoch 374, recon_loss:0.444514, zinb_loss:0.917003, adversial_loss:1.314712\n",
      "Pretrain epoch 375, recon_loss:0.444899, zinb_loss:0.916350, adversial_loss:1.314808\n",
      "Pretrain epoch 376, recon_loss:0.443418, zinb_loss:0.916221, adversial_loss:1.314990\n",
      "Pretrain epoch 377, recon_loss:0.446117, zinb_loss:0.916390, adversial_loss:1.315006\n",
      "Pretrain epoch 378, recon_loss:0.442416, zinb_loss:0.916373, adversial_loss:1.314442\n",
      "Pretrain epoch 379, recon_loss:0.441176, zinb_loss:0.916062, adversial_loss:1.315529\n",
      "Pretrain epoch 380, recon_loss:0.440152, zinb_loss:0.915806, adversial_loss:1.314229\n",
      "Pretrain epoch 381, recon_loss:0.442317, zinb_loss:0.915975, adversial_loss:1.315477\n",
      "Pretrain epoch 382, recon_loss:0.448286, zinb_loss:0.916076, adversial_loss:1.314018\n",
      "Pretrain epoch 383, recon_loss:0.445813, zinb_loss:0.915906, adversial_loss:1.315852\n",
      "Pretrain epoch 384, recon_loss:0.442290, zinb_loss:0.915804, adversial_loss:1.314125\n",
      "Pretrain epoch 385, recon_loss:0.437342, zinb_loss:0.915675, adversial_loss:1.314848\n",
      "Pretrain epoch 386, recon_loss:0.440979, zinb_loss:0.915799, adversial_loss:1.314934\n",
      "Pretrain epoch 387, recon_loss:0.439087, zinb_loss:0.915975, adversial_loss:1.314718\n",
      "Pretrain epoch 388, recon_loss:0.440426, zinb_loss:0.916505, adversial_loss:1.314297\n",
      "Pretrain epoch 389, recon_loss:0.443784, zinb_loss:0.916742, adversial_loss:1.315327\n",
      "Pretrain epoch 390, recon_loss:0.434254, zinb_loss:0.916135, adversial_loss:1.314359\n",
      "Pretrain epoch 391, recon_loss:0.428102, zinb_loss:0.915405, adversial_loss:1.314656\n",
      "Pretrain epoch 392, recon_loss:0.429111, zinb_loss:0.915391, adversial_loss:1.314852\n",
      "Pretrain epoch 393, recon_loss:0.428276, zinb_loss:0.915497, adversial_loss:1.314504\n",
      "Pretrain epoch 394, recon_loss:0.427279, zinb_loss:0.915248, adversial_loss:1.314618\n",
      "Pretrain epoch 395, recon_loss:0.422697, zinb_loss:0.914847, adversial_loss:1.314638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 396, recon_loss:0.422991, zinb_loss:0.914830, adversial_loss:1.314517\n",
      "Pretrain epoch 397, recon_loss:0.424634, zinb_loss:0.914902, adversial_loss:1.314249\n",
      "Pretrain epoch 398, recon_loss:0.425407, zinb_loss:0.914882, adversial_loss:1.314864\n",
      "Pretrain epoch 399, recon_loss:0.425893, zinb_loss:0.914864, adversial_loss:1.314222\n",
      "Pretrain epoch 400, recon_loss:0.423472, zinb_loss:0.914745, adversial_loss:1.314429\n"
     ]
    }
   ],
   "source": [
    "pretrain_latent = model.pretrain_autoencoder(\n",
    "                        X1=adata1.X, X2=adata2.X, X1_raw=adata1.raw.X, X2_raw=adata2.raw.X, \n",
    "                        epochs=400, file='10x1kpbmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e8f185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stage\n",
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: AMI= 0.8044, NMI= 0.8058, ARI= 0.7955, ACC= 0.8906\n",
      "Training epoch 1, recon_loss:0.427788, zinb_loss:0.915055, cluster_loss:0.161027\n",
      "Clustering   1: AMI= 0.8044, NMI= 0.8058, ARI= 0.7955, ACC= 0.8906\n",
      "0.0\n",
      "Training epoch 2, recon_loss:0.872027, zinb_loss:1.037849, cluster_loss:0.175631\n",
      "Clustering   2: AMI= 0.7893, NMI= 0.7908, ARI= 0.7676, ACC= 0.8598\n",
      "0.03506311360448808\n",
      "Training epoch 3, recon_loss:0.772488, zinb_loss:1.157997, cluster_loss:0.185614\n",
      "Clustering   3: AMI= 0.8156, NMI= 0.8170, ARI= 0.8212, ACC= 0.9130\n",
      "0.05750350631136045\n",
      "Training epoch 4, recon_loss:0.802469, zinb_loss:1.415427, cluster_loss:0.176955\n",
      "Clustering   4: AMI= 0.7936, NMI= 0.7951, ARI= 0.7769, ACC= 0.8738\n",
      "0.04628330995792426\n",
      "Training epoch 5, recon_loss:0.851953, zinb_loss:1.493593, cluster_loss:0.181987\n",
      "Clustering   5: AMI= 0.7884, NMI= 0.7900, ARI= 0.7710, ACC= 0.8654\n",
      "0.011220196353436185\n",
      "Training epoch 6, recon_loss:0.762861, zinb_loss:1.223554, cluster_loss:0.183558\n",
      "Clustering   6: AMI= 0.8120, NMI= 0.8133, ARI= 0.8141, ACC= 0.9102\n",
      "0.05750350631136045\n",
      "Training epoch 7, recon_loss:0.737973, zinb_loss:1.212528, cluster_loss:0.183818\n",
      "Clustering   7: AMI= 0.8219, NMI= 0.8232, ARI= 0.8242, ACC= 0.9144\n",
      "0.004207573632538569\n",
      "Training epoch 8, recon_loss:0.743957, zinb_loss:1.054938, cluster_loss:0.173220\n",
      "Clustering   8: AMI= 0.8371, NMI= 0.8383, ARI= 0.8578, ACC= 0.9383\n",
      "0.043478260869565216\n",
      "Training epoch 9, recon_loss:0.702995, zinb_loss:1.014665, cluster_loss:0.171818\n",
      "Clustering   9: AMI= 0.8116, NMI= 0.8130, ARI= 0.8085, ACC= 0.9032\n",
      "0.056100981767180924\n",
      "Training epoch 10, recon_loss:0.695617, zinb_loss:1.026221, cluster_loss:0.162742\n",
      "Clustering   10: AMI= 0.8308, NMI= 0.8321, ARI= 0.8466, ACC= 0.9313\n",
      "0.04067321178120617\n",
      "Training epoch 11, recon_loss:0.691325, zinb_loss:1.035385, cluster_loss:0.165509\n",
      "Clustering   11: AMI= 0.7923, NMI= 0.7939, ARI= 0.7842, ACC= 0.8836\n",
      "0.06030855539971949\n",
      "Training epoch 12, recon_loss:0.664986, zinb_loss:1.025016, cluster_loss:0.157903\n",
      "Clustering   12: AMI= 0.8076, NMI= 0.8091, ARI= 0.8079, ACC= 0.9046\n",
      "0.028050490883590462\n",
      "Training epoch 13, recon_loss:0.649964, zinb_loss:1.014876, cluster_loss:0.159983\n",
      "Clustering   13: AMI= 0.8068, NMI= 0.8082, ARI= 0.8097, ACC= 0.9060\n",
      "0.004207573632538569\n",
      "Training epoch 14, recon_loss:0.644292, zinb_loss:0.991982, cluster_loss:0.155603\n",
      "Clustering   14: AMI= 0.8097, NMI= 0.8111, ARI= 0.8115, ACC= 0.9074\n",
      "0.001402524544179523\n",
      "Training epoch 15, recon_loss:0.654204, zinb_loss:0.989654, cluster_loss:0.156678\n",
      "Clustering   15: AMI= 0.8015, NMI= 0.8029, ARI= 0.7924, ACC= 0.8906\n",
      "0.023842917251051893\n",
      "Training epoch 16, recon_loss:0.630564, zinb_loss:0.976496, cluster_loss:0.154078\n",
      "Clustering   16: AMI= 0.8076, NMI= 0.8091, ARI= 0.8079, ACC= 0.9046\n",
      "0.021037868162692847\n",
      "Training epoch 17, recon_loss:0.625106, zinb_loss:0.965822, cluster_loss:0.154051\n",
      "Clustering   17: AMI= 0.8058, NMI= 0.8072, ARI= 0.8013, ACC= 0.8990\n",
      "0.008415147265077139\n",
      "Training epoch 18, recon_loss:0.619398, zinb_loss:0.962830, cluster_loss:0.151553\n",
      "Clustering   18: AMI= 0.8076, NMI= 0.8091, ARI= 0.8079, ACC= 0.9046\n",
      "0.008415147265077139\n",
      "Training epoch 19, recon_loss:0.624661, zinb_loss:0.960819, cluster_loss:0.152612\n",
      "Clustering   19: AMI= 0.7965, NMI= 0.7980, ARI= 0.7855, ACC= 0.8850\n",
      "0.02664796633941094\n",
      "Training epoch 20, recon_loss:0.624268, zinb_loss:0.961495, cluster_loss:0.152379\n",
      "Clustering   20: AMI= 0.8150, NMI= 0.8163, ARI= 0.8164, ACC= 0.9102\n",
      "0.029453015427769985\n",
      "Training epoch 21, recon_loss:0.633005, zinb_loss:0.962817, cluster_loss:0.151675\n",
      "Clustering   21: AMI= 0.7937, NMI= 0.7952, ARI= 0.7802, ACC= 0.8794\n",
      "0.03506311360448808\n",
      "Training epoch 22, recon_loss:0.619075, zinb_loss:0.956141, cluster_loss:0.151573\n",
      "Clustering   22: AMI= 0.8118, NMI= 0.8132, ARI= 0.8110, ACC= 0.9060\n",
      "0.030855539971949508\n",
      "Training epoch 23, recon_loss:0.612375, zinb_loss:0.950345, cluster_loss:0.150224\n",
      "Clustering   23: AMI= 0.7965, NMI= 0.7980, ARI= 0.7879, ACC= 0.8878\n",
      "0.02244039270687237\n",
      "Training epoch 24, recon_loss:0.614001, zinb_loss:0.952195, cluster_loss:0.149742\n",
      "Clustering   24: AMI= 0.8118, NMI= 0.8132, ARI= 0.8110, ACC= 0.9060\n",
      "0.02244039270687237\n",
      "Training epoch 25, recon_loss:0.616745, zinb_loss:0.950423, cluster_loss:0.149848\n",
      "Clustering   25: AMI= 0.7950, NMI= 0.7965, ARI= 0.7851, ACC= 0.8850\n",
      "0.025245441795231416\n",
      "Training epoch 26, recon_loss:0.620402, zinb_loss:0.954937, cluster_loss:0.149734\n",
      "Clustering   26: AMI= 0.8099, NMI= 0.8113, ARI= 0.8109, ACC= 0.9060\n",
      "0.025245441795231416\n",
      "Training epoch 27, recon_loss:0.623375, zinb_loss:0.956694, cluster_loss:0.148665\n",
      "Clustering   27: AMI= 0.7965, NMI= 0.7980, ARI= 0.7855, ACC= 0.8850\n",
      "0.025245441795231416\n",
      "Training epoch 28, recon_loss:0.607770, zinb_loss:0.954326, cluster_loss:0.148879\n",
      "Clustering   28: AMI= 0.8118, NMI= 0.8132, ARI= 0.8110, ACC= 0.9060\n",
      "0.025245441795231416\n",
      "Training epoch 29, recon_loss:0.601741, zinb_loss:0.952608, cluster_loss:0.148056\n",
      "Clustering   29: AMI= 0.7957, NMI= 0.7972, ARI= 0.7865, ACC= 0.8864\n",
      "0.023842917251051893\n",
      "Training epoch 30, recon_loss:0.607758, zinb_loss:0.957023, cluster_loss:0.148669\n",
      "Clustering   30: AMI= 0.8128, NMI= 0.8142, ARI= 0.8127, ACC= 0.9074\n",
      "0.025245441795231416\n",
      "Training epoch 31, recon_loss:0.614691, zinb_loss:0.967767, cluster_loss:0.148506\n",
      "Clustering   31: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.023842917251051893\n",
      "Training epoch 32, recon_loss:0.601296, zinb_loss:0.973557, cluster_loss:0.149344\n",
      "Clustering   32: AMI= 0.8054, NMI= 0.8068, ARI= 0.8033, ACC= 0.9018\n",
      "0.021037868162692847\n",
      "Training epoch 33, recon_loss:0.607411, zinb_loss:0.981902, cluster_loss:0.149179\n",
      "Clustering   33: AMI= 0.8026, NMI= 0.8041, ARI= 0.8016, ACC= 0.8990\n",
      "0.0070126227208976155\n",
      "Training epoch 34, recon_loss:0.603015, zinb_loss:0.974039, cluster_loss:0.148835\n",
      "Clustering   34: AMI= 0.8074, NMI= 0.8089, ARI= 0.8068, ACC= 0.9046\n",
      "0.009817671809256662\n",
      "Training epoch 35, recon_loss:0.605766, zinb_loss:0.972500, cluster_loss:0.147968\n",
      "Clustering   35: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.023842917251051893\n",
      "Training epoch 36, recon_loss:0.602187, zinb_loss:0.965171, cluster_loss:0.149204\n",
      "Clustering   36: AMI= 0.8074, NMI= 0.8089, ARI= 0.8068, ACC= 0.9046\n",
      "0.023842917251051893\n",
      "Training epoch 37, recon_loss:0.599118, zinb_loss:0.960134, cluster_loss:0.148186\n",
      "Clustering   37: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.023842917251051893\n",
      "Training epoch 38, recon_loss:0.592715, zinb_loss:0.955998, cluster_loss:0.147191\n",
      "Clustering   38: AMI= 0.8064, NMI= 0.8078, ARI= 0.8051, ACC= 0.9032\n",
      "0.02244039270687237\n",
      "Training epoch 39, recon_loss:0.591897, zinb_loss:0.954167, cluster_loss:0.147082\n",
      "Clustering   39: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.02244039270687237\n",
      "Training epoch 40, recon_loss:0.595020, zinb_loss:0.952417, cluster_loss:0.146503\n",
      "Clustering   40: AMI= 0.8064, NMI= 0.8078, ARI= 0.8051, ACC= 0.9032\n",
      "0.02244039270687237\n",
      "Training epoch 41, recon_loss:0.602681, zinb_loss:0.951833, cluster_loss:0.148037\n",
      "Clustering   41: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.02244039270687237\n",
      "Training epoch 42, recon_loss:0.602251, zinb_loss:0.953059, cluster_loss:0.146572\n",
      "Clustering   42: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.02244039270687237\n",
      "Training epoch 43, recon_loss:0.595920, zinb_loss:0.950158, cluster_loss:0.147027\n",
      "Clustering   43: AMI= 0.8076, NMI= 0.8090, ARI= 0.8035, ACC= 0.9004\n",
      "0.009817671809256662\n",
      "Training epoch 44, recon_loss:0.590808, zinb_loss:0.949260, cluster_loss:0.146189\n",
      "Clustering   44: AMI= 0.7984, NMI= 0.7998, ARI= 0.7953, ACC= 0.8962\n",
      "0.005610098176718092\n",
      "Training epoch 45, recon_loss:0.588686, zinb_loss:0.948610, cluster_loss:0.146366\n",
      "Clustering   45: AMI= 0.8076, NMI= 0.8090, ARI= 0.8035, ACC= 0.9004\n",
      "0.005610098176718092\n",
      "Training epoch 46, recon_loss:0.591624, zinb_loss:0.948653, cluster_loss:0.146179\n",
      "Clustering   46: AMI= 0.8031, NMI= 0.8045, ARI= 0.7955, ACC= 0.8934\n",
      "0.0070126227208976155\n",
      "Training epoch 47, recon_loss:0.594331, zinb_loss:0.950306, cluster_loss:0.146628\n",
      "Clustering   47: AMI= 0.8037, NMI= 0.8052, ARI= 0.8044, ACC= 0.9018\n",
      "0.015427769985974754\n",
      "Training epoch 48, recon_loss:0.595154, zinb_loss:0.951949, cluster_loss:0.146202\n",
      "Clustering   48: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.021037868162692847\n",
      "Training epoch 49, recon_loss:0.587361, zinb_loss:0.952617, cluster_loss:0.146149\n",
      "Clustering   49: AMI= 0.8079, NMI= 0.8093, ARI= 0.8075, ACC= 0.9032\n",
      "0.019635343618513323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 50, recon_loss:0.588604, zinb_loss:0.952701, cluster_loss:0.146320\n",
      "Clustering   50: AMI= 0.7957, NMI= 0.7972, ARI= 0.7865, ACC= 0.8864\n",
      "0.021037868162692847\n",
      "Training epoch 51, recon_loss:0.601839, zinb_loss:0.957328, cluster_loss:0.146832\n",
      "Clustering   51: AMI= 0.8093, NMI= 0.8107, ARI= 0.8068, ACC= 0.9046\n",
      "0.025245441795231416\n",
      "Training epoch 52, recon_loss:0.586133, zinb_loss:0.956503, cluster_loss:0.145842\n",
      "Clustering   52: AMI= 0.7957, NMI= 0.7972, ARI= 0.7865, ACC= 0.8864\n",
      "0.025245441795231416\n",
      "Training epoch 53, recon_loss:0.593161, zinb_loss:0.959234, cluster_loss:0.145659\n",
      "Clustering   53: AMI= 0.8064, NMI= 0.8078, ARI= 0.8051, ACC= 0.9032\n",
      "0.023842917251051893\n",
      "Training epoch 54, recon_loss:0.596088, zinb_loss:0.959458, cluster_loss:0.145619\n",
      "Clustering   54: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.023842917251051893\n",
      "Training epoch 55, recon_loss:0.591080, zinb_loss:0.958723, cluster_loss:0.146207\n",
      "Clustering   55: AMI= 0.8047, NMI= 0.8062, ARI= 0.8061, ACC= 0.9032\n",
      "0.023842917251051893\n",
      "Training epoch 56, recon_loss:0.599603, zinb_loss:0.962224, cluster_loss:0.146418\n",
      "Clustering   56: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.023842917251051893\n",
      "Training epoch 57, recon_loss:0.587605, zinb_loss:0.960002, cluster_loss:0.145664\n",
      "Clustering   57: AMI= 0.8076, NMI= 0.8090, ARI= 0.8035, ACC= 0.9004\n",
      "0.014025245441795231\n",
      "Training epoch 58, recon_loss:0.592603, zinb_loss:0.961957, cluster_loss:0.145919\n",
      "Clustering   58: AMI= 0.8007, NMI= 0.8021, ARI= 0.7909, ACC= 0.8892\n",
      "0.011220196353436185\n",
      "Training epoch 59, recon_loss:0.583416, zinb_loss:0.960595, cluster_loss:0.144896\n",
      "Clustering   59: AMI= 0.8007, NMI= 0.8021, ARI= 0.7956, ACC= 0.8948\n",
      "0.008415147265077139\n",
      "Training epoch 60, recon_loss:0.589490, zinb_loss:0.961778, cluster_loss:0.145347\n",
      "Clustering   60: AMI= 0.8048, NMI= 0.8063, ARI= 0.7986, ACC= 0.8962\n",
      "0.004207573632538569\n",
      "Training epoch 61, recon_loss:0.588725, zinb_loss:0.960479, cluster_loss:0.144503\n",
      "Clustering   61: AMI= 0.8016, NMI= 0.8030, ARI= 0.7972, ACC= 0.8962\n",
      "0.005610098176718092\n",
      "Training epoch 62, recon_loss:0.592149, zinb_loss:0.959622, cluster_loss:0.145034\n",
      "Clustering   62: AMI= 0.8066, NMI= 0.8081, ARI= 0.8019, ACC= 0.8990\n",
      "0.002805049088359046\n",
      "Training epoch 63, recon_loss:0.580424, zinb_loss:0.954357, cluster_loss:0.144002\n",
      "Clustering   63: AMI= 0.8000, NMI= 0.8015, ARI= 0.7947, ACC= 0.8962\n",
      "0.002805049088359046\n",
      "Training epoch 64, recon_loss:0.585225, zinb_loss:0.953657, cluster_loss:0.144129\n",
      "Clustering   64: AMI= 0.8015, NMI= 0.8029, ARI= 0.7924, ACC= 0.8906\n",
      "0.011220196353436185\n",
      "Training epoch 65, recon_loss:0.576661, zinb_loss:0.949110, cluster_loss:0.144271\n",
      "Clustering   65: AMI= 0.7948, NMI= 0.7963, ARI= 0.7853, ACC= 0.8878\n",
      "0.002805049088359046\n",
      "Training epoch 66, recon_loss:0.578369, zinb_loss:0.949564, cluster_loss:0.143733\n",
      "Clustering   66: AMI= 0.8048, NMI= 0.8063, ARI= 0.7986, ACC= 0.8962\n",
      "0.008415147265077139\n",
      "Training epoch 67, recon_loss:0.573413, zinb_loss:0.946228, cluster_loss:0.144202\n",
      "Clustering   67: AMI= 0.7948, NMI= 0.7963, ARI= 0.7853, ACC= 0.8878\n",
      "0.008415147265077139\n",
      "Training epoch 68, recon_loss:0.579824, zinb_loss:0.947838, cluster_loss:0.143680\n",
      "Clustering   68: AMI= 0.8048, NMI= 0.8063, ARI= 0.7986, ACC= 0.8962\n",
      "0.008415147265077139\n",
      "Training epoch 69, recon_loss:0.578802, zinb_loss:0.945562, cluster_loss:0.144582\n",
      "Clustering   69: AMI= 0.7973, NMI= 0.7988, ARI= 0.7899, ACC= 0.8920\n",
      "0.0070126227208976155\n",
      "Training epoch 70, recon_loss:0.583734, zinb_loss:0.948775, cluster_loss:0.144136\n",
      "Clustering   70: AMI= 0.8015, NMI= 0.8029, ARI= 0.7924, ACC= 0.8906\n",
      "0.0070126227208976155\n",
      "Training epoch 71, recon_loss:0.581839, zinb_loss:0.951289, cluster_loss:0.144062\n",
      "Clustering   71: AMI= 0.8009, NMI= 0.8024, ARI= 0.7964, ACC= 0.8976\n",
      "0.012622720897615708\n",
      "Training epoch 72, recon_loss:0.589894, zinb_loss:0.964318, cluster_loss:0.144476\n",
      "Clustering   72: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.015427769985974754\n",
      "Training epoch 73, recon_loss:0.596969, zinb_loss:0.981150, cluster_loss:0.145677\n",
      "Clustering   73: AMI= 0.8074, NMI= 0.8089, ARI= 0.8068, ACC= 0.9046\n",
      "0.023842917251051893\n",
      "Training epoch 74, recon_loss:0.588003, zinb_loss:0.996500, cluster_loss:0.144314\n",
      "Clustering   74: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.025245441795231416\n",
      "Training epoch 75, recon_loss:0.595995, zinb_loss:0.989295, cluster_loss:0.145486\n",
      "Clustering   75: AMI= 0.8090, NMI= 0.8104, ARI= 0.8063, ACC= 0.9060\n",
      "0.029453015427769985\n",
      "Training epoch 76, recon_loss:0.589745, zinb_loss:0.988745, cluster_loss:0.143557\n",
      "Clustering   76: AMI= 0.7965, NMI= 0.7980, ARI= 0.7855, ACC= 0.8850\n",
      "0.030855539971949508\n",
      "Training epoch 77, recon_loss:0.597836, zinb_loss:0.976317, cluster_loss:0.144623\n",
      "Clustering   77: AMI= 0.8101, NMI= 0.8115, ARI= 0.8082, ACC= 0.9074\n",
      "0.03225806451612903\n",
      "Training epoch 78, recon_loss:0.586687, zinb_loss:0.970886, cluster_loss:0.142959\n",
      "Clustering   78: AMI= 0.7965, NMI= 0.7980, ARI= 0.7855, ACC= 0.8850\n",
      "0.03225806451612903\n",
      "Training epoch 79, recon_loss:0.580718, zinb_loss:0.964579, cluster_loss:0.143673\n",
      "Clustering   79: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.028050490883590462\n",
      "Training epoch 80, recon_loss:0.577025, zinb_loss:0.962893, cluster_loss:0.142707\n",
      "Clustering   80: AMI= 0.7974, NMI= 0.7989, ARI= 0.7854, ACC= 0.8864\n",
      "0.023842917251051893\n",
      "Training epoch 81, recon_loss:0.574229, zinb_loss:0.958285, cluster_loss:0.143327\n",
      "Clustering   81: AMI= 0.8054, NMI= 0.8068, ARI= 0.8033, ACC= 0.9018\n",
      "0.019635343618513323\n",
      "Training epoch 82, recon_loss:0.577493, zinb_loss:0.956699, cluster_loss:0.142654\n",
      "Clustering   82: AMI= 0.8042, NMI= 0.8056, ARI= 0.7977, ACC= 0.8976\n",
      "0.008415147265077139\n",
      "Training epoch 83, recon_loss:0.584718, zinb_loss:0.954907, cluster_loss:0.144015\n",
      "Clustering   83: AMI= 0.8061, NMI= 0.8075, ARI= 0.8011, ACC= 0.9004\n",
      "0.002805049088359046\n",
      "Training epoch 84, recon_loss:0.581224, zinb_loss:0.953543, cluster_loss:0.143126\n",
      "Clustering   84: AMI= 0.8079, NMI= 0.8093, ARI= 0.8075, ACC= 0.9032\n",
      "0.0070126227208976155\n",
      "Training epoch 85, recon_loss:0.581790, zinb_loss:0.949014, cluster_loss:0.142979\n",
      "Clustering   85: AMI= 0.7998, NMI= 0.8013, ARI= 0.7898, ACC= 0.8906\n",
      "0.016830294530154277\n",
      "Training epoch 86, recon_loss:0.575096, zinb_loss:0.948070, cluster_loss:0.142271\n",
      "Clustering   86: AMI= 0.8011, NMI= 0.8025, ARI= 0.7992, ACC= 0.8990\n",
      "0.011220196353436185\n",
      "Training epoch 87, recon_loss:0.574283, zinb_loss:0.946117, cluster_loss:0.142564\n",
      "Clustering   87: AMI= 0.7998, NMI= 0.8013, ARI= 0.7898, ACC= 0.8906\n",
      "0.011220196353436185\n",
      "Training epoch 88, recon_loss:0.576720, zinb_loss:0.947449, cluster_loss:0.143047\n",
      "Clustering   88: AMI= 0.8026, NMI= 0.8041, ARI= 0.8016, ACC= 0.8990\n",
      "0.011220196353436185\n",
      "Training epoch 89, recon_loss:0.580010, zinb_loss:0.949000, cluster_loss:0.142947\n",
      "Clustering   89: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.012622720897615708\n",
      "Training epoch 90, recon_loss:0.577357, zinb_loss:0.952125, cluster_loss:0.143367\n",
      "Clustering   90: AMI= 0.8057, NMI= 0.8072, ARI= 0.8002, ACC= 0.8976\n",
      "0.008415147265077139\n",
      "Training epoch 91, recon_loss:0.576907, zinb_loss:0.953980, cluster_loss:0.142254\n",
      "Clustering   91: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.005610098176718092\n",
      "Training epoch 92, recon_loss:0.580425, zinb_loss:0.957351, cluster_loss:0.143278\n",
      "Clustering   92: AMI= 0.8007, NMI= 0.8021, ARI= 0.7909, ACC= 0.8892\n",
      "0.014025245441795231\n",
      "Training epoch 93, recon_loss:0.584027, zinb_loss:0.961610, cluster_loss:0.142553\n",
      "Clustering   93: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 94, recon_loss:0.582717, zinb_loss:0.962629, cluster_loss:0.143135\n",
      "Clustering   94: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.023842917251051893\n",
      "Training epoch 95, recon_loss:0.581614, zinb_loss:0.964241, cluster_loss:0.142651\n",
      "Clustering   95: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.02664796633941094\n",
      "Training epoch 96, recon_loss:0.572320, zinb_loss:0.961603, cluster_loss:0.142146\n",
      "Clustering   96: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.02664796633941094\n",
      "Training epoch 97, recon_loss:0.564760, zinb_loss:0.959266, cluster_loss:0.141922\n",
      "Clustering   97: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.023842917251051893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 98, recon_loss:0.562242, zinb_loss:0.957486, cluster_loss:0.141468\n",
      "Clustering   98: AMI= 0.7974, NMI= 0.7989, ARI= 0.7854, ACC= 0.8864\n",
      "0.021037868162692847\n",
      "Training epoch 99, recon_loss:0.569988, zinb_loss:0.957222, cluster_loss:0.142154\n",
      "Clustering   99: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.02244039270687237\n",
      "Training epoch 100, recon_loss:0.580200, zinb_loss:0.957662, cluster_loss:0.142221\n",
      "Clustering   100: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.025245441795231416\n",
      "Training epoch 101, recon_loss:0.582866, zinb_loss:0.957628, cluster_loss:0.142941\n",
      "Clustering   101: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.02664796633941094\n",
      "Training epoch 102, recon_loss:0.580805, zinb_loss:0.955079, cluster_loss:0.142215\n",
      "Clustering   102: AMI= 0.7967, NMI= 0.7982, ARI= 0.7840, ACC= 0.8850\n",
      "0.025245441795231416\n",
      "Training epoch 103, recon_loss:0.581828, zinb_loss:0.953852, cluster_loss:0.143081\n",
      "Clustering   103: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.025245441795231416\n",
      "Training epoch 104, recon_loss:0.572797, zinb_loss:0.950400, cluster_loss:0.141341\n",
      "Clustering   104: AMI= 0.7982, NMI= 0.7997, ARI= 0.7868, ACC= 0.8878\n",
      "0.02244039270687237\n",
      "Training epoch 105, recon_loss:0.576882, zinb_loss:0.950388, cluster_loss:0.142694\n",
      "Clustering   105: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 106, recon_loss:0.572698, zinb_loss:0.951130, cluster_loss:0.141604\n",
      "Clustering   106: AMI= 0.7990, NMI= 0.8005, ARI= 0.7883, ACC= 0.8892\n",
      "0.0182328190743338\n",
      "Training epoch 107, recon_loss:0.577765, zinb_loss:0.954506, cluster_loss:0.143002\n",
      "Clustering   107: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.016830294530154277\n",
      "Training epoch 108, recon_loss:0.573308, zinb_loss:0.958901, cluster_loss:0.141628\n",
      "Clustering   108: AMI= 0.8015, NMI= 0.8029, ARI= 0.7924, ACC= 0.8906\n",
      "0.0182328190743338\n",
      "Training epoch 109, recon_loss:0.575178, zinb_loss:0.959521, cluster_loss:0.143096\n",
      "Clustering   109: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.0182328190743338\n",
      "Training epoch 110, recon_loss:0.568933, zinb_loss:0.963594, cluster_loss:0.141560\n",
      "Clustering   110: AMI= 0.8023, NMI= 0.8037, ARI= 0.7939, ACC= 0.8920\n",
      "0.016830294530154277\n",
      "Training epoch 111, recon_loss:0.571905, zinb_loss:0.960422, cluster_loss:0.142540\n",
      "Clustering   111: AMI= 0.8017, NMI= 0.8031, ARI= 0.8003, ACC= 0.9018\n",
      "0.0182328190743338\n",
      "Training epoch 112, recon_loss:0.572863, zinb_loss:0.964372, cluster_loss:0.141528\n",
      "Clustering   112: AMI= 0.8007, NMI= 0.8021, ARI= 0.7909, ACC= 0.8892\n",
      "0.021037868162692847\n",
      "Training epoch 113, recon_loss:0.575701, zinb_loss:0.959053, cluster_loss:0.142022\n",
      "Clustering   113: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.02244039270687237\n",
      "Training epoch 114, recon_loss:0.584166, zinb_loss:0.961498, cluster_loss:0.141691\n",
      "Clustering   114: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.023842917251051893\n",
      "Training epoch 115, recon_loss:0.572459, zinb_loss:0.954256, cluster_loss:0.141249\n",
      "Clustering   115: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 116, recon_loss:0.572712, zinb_loss:0.952947, cluster_loss:0.141284\n",
      "Clustering   116: AMI= 0.8023, NMI= 0.8037, ARI= 0.7939, ACC= 0.8920\n",
      "0.016830294530154277\n",
      "Training epoch 117, recon_loss:0.568649, zinb_loss:0.951164, cluster_loss:0.140805\n",
      "Clustering   117: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.014025245441795231\n",
      "Training epoch 118, recon_loss:0.567744, zinb_loss:0.951765, cluster_loss:0.141439\n",
      "Clustering   118: AMI= 0.7998, NMI= 0.8013, ARI= 0.7898, ACC= 0.8906\n",
      "0.012622720897615708\n",
      "Training epoch 119, recon_loss:0.562308, zinb_loss:0.950098, cluster_loss:0.141025\n",
      "Clustering   119: AMI= 0.7936, NMI= 0.7951, ARI= 0.7904, ACC= 0.8948\n",
      "0.012622720897615708\n",
      "Training epoch 120, recon_loss:0.565499, zinb_loss:0.950185, cluster_loss:0.141620\n",
      "Clustering   120: AMI= 0.7998, NMI= 0.8013, ARI= 0.7898, ACC= 0.8906\n",
      "0.012622720897615708\n",
      "Training epoch 121, recon_loss:0.561618, zinb_loss:0.947898, cluster_loss:0.141208\n",
      "Clustering   121: AMI= 0.7936, NMI= 0.7951, ARI= 0.7904, ACC= 0.8948\n",
      "0.012622720897615708\n",
      "Training epoch 122, recon_loss:0.568843, zinb_loss:0.947921, cluster_loss:0.141323\n",
      "Clustering   122: AMI= 0.7982, NMI= 0.7997, ARI= 0.7868, ACC= 0.8878\n",
      "0.015427769985974754\n",
      "Training epoch 123, recon_loss:0.564181, zinb_loss:0.945251, cluster_loss:0.141240\n",
      "Clustering   123: AMI= 0.7988, NMI= 0.8003, ARI= 0.7962, ACC= 0.8990\n",
      "0.021037868162692847\n",
      "Training epoch 124, recon_loss:0.577869, zinb_loss:0.946012, cluster_loss:0.141066\n",
      "Clustering   124: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.025245441795231416\n",
      "Training epoch 125, recon_loss:0.563130, zinb_loss:0.943527, cluster_loss:0.141048\n",
      "Clustering   125: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.02244039270687237\n",
      "Training epoch 126, recon_loss:0.569466, zinb_loss:0.945524, cluster_loss:0.140665\n",
      "Clustering   126: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.02244039270687237\n",
      "Training epoch 127, recon_loss:0.571306, zinb_loss:0.945524, cluster_loss:0.141447\n",
      "Clustering   127: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.023842917251051893\n",
      "Training epoch 128, recon_loss:0.578292, zinb_loss:0.949557, cluster_loss:0.140572\n",
      "Clustering   128: AMI= 0.7933, NMI= 0.7948, ARI= 0.7801, ACC= 0.8822\n",
      "0.025245441795231416\n",
      "Training epoch 129, recon_loss:0.577133, zinb_loss:0.949347, cluster_loss:0.141804\n",
      "Clustering   129: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.025245441795231416\n",
      "Training epoch 130, recon_loss:0.569686, zinb_loss:0.951262, cluster_loss:0.140641\n",
      "Clustering   130: AMI= 0.7916, NMI= 0.7931, ARI= 0.7774, ACC= 0.8822\n",
      "0.02244039270687237\n",
      "Training epoch 131, recon_loss:0.570992, zinb_loss:0.950735, cluster_loss:0.141347\n",
      "Clustering   131: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.02244039270687237\n",
      "Training epoch 132, recon_loss:0.563289, zinb_loss:0.951299, cluster_loss:0.140232\n",
      "Clustering   132: AMI= 0.7901, NMI= 0.7916, ARI= 0.7769, ACC= 0.8822\n",
      "0.02244039270687237\n",
      "Training epoch 133, recon_loss:0.566010, zinb_loss:0.951371, cluster_loss:0.140683\n",
      "Clustering   133: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 134, recon_loss:0.566800, zinb_loss:0.952001, cluster_loss:0.140230\n",
      "Clustering   134: AMI= 0.7901, NMI= 0.7916, ARI= 0.7769, ACC= 0.8822\n",
      "0.021037868162692847\n",
      "Training epoch 135, recon_loss:0.565882, zinb_loss:0.952393, cluster_loss:0.140599\n",
      "Clustering   135: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 136, recon_loss:0.561767, zinb_loss:0.952621, cluster_loss:0.140067\n",
      "Clustering   136: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.019635343618513323\n",
      "Training epoch 137, recon_loss:0.563533, zinb_loss:0.954821, cluster_loss:0.140596\n",
      "Clustering   137: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.015427769985974754\n",
      "Training epoch 138, recon_loss:0.562161, zinb_loss:0.957540, cluster_loss:0.140141\n",
      "Clustering   138: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.009817671809256662\n",
      "Training epoch 139, recon_loss:0.564167, zinb_loss:0.964677, cluster_loss:0.140868\n",
      "Clustering   139: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.004207573632538569\n",
      "Training epoch 140, recon_loss:0.574935, zinb_loss:0.970617, cluster_loss:0.141272\n",
      "Clustering   140: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.0070126227208976155\n",
      "Training epoch 141, recon_loss:0.574378, zinb_loss:0.978030, cluster_loss:0.140567\n",
      "Clustering   141: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.015427769985974754\n",
      "Training epoch 142, recon_loss:0.567562, zinb_loss:0.972313, cluster_loss:0.141010\n",
      "Clustering   142: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 143, recon_loss:0.563666, zinb_loss:0.971015, cluster_loss:0.140086\n",
      "Clustering   143: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 144, recon_loss:0.565442, zinb_loss:0.967235, cluster_loss:0.140770\n",
      "Clustering   144: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.023842917251051893\n",
      "Training epoch 145, recon_loss:0.574124, zinb_loss:0.966415, cluster_loss:0.140407\n",
      "Clustering   145: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.02664796633941094\n",
      "Training epoch 146, recon_loss:0.570939, zinb_loss:0.962836, cluster_loss:0.140856\n",
      "Clustering   146: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.02664796633941094\n",
      "Training epoch 147, recon_loss:0.575750, zinb_loss:0.960175, cluster_loss:0.140598\n",
      "Clustering   147: AMI= 0.7992, NMI= 0.8006, ARI= 0.7881, ACC= 0.8864\n",
      "0.02664796633941094\n",
      "Training epoch 148, recon_loss:0.566969, zinb_loss:0.956485, cluster_loss:0.139849\n",
      "Clustering   148: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.0182328190743338\n",
      "Training epoch 149, recon_loss:0.564207, zinb_loss:0.953127, cluster_loss:0.140197\n",
      "Clustering   149: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.002805049088359046\n",
      "Training epoch 150, recon_loss:0.573406, zinb_loss:0.952357, cluster_loss:0.139711\n",
      "Clustering   150: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.002805049088359046\n",
      "Training epoch 151, recon_loss:0.560113, zinb_loss:0.950089, cluster_loss:0.139744\n",
      "Clustering   151: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.005610098176718092\n",
      "Training epoch 152, recon_loss:0.565350, zinb_loss:0.949469, cluster_loss:0.139530\n",
      "Clustering   152: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.009817671809256662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 153, recon_loss:0.563306, zinb_loss:0.949260, cluster_loss:0.139716\n",
      "Clustering   153: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.009817671809256662\n",
      "Training epoch 154, recon_loss:0.570012, zinb_loss:0.949180, cluster_loss:0.139891\n",
      "Clustering   154: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.014025245441795231\n",
      "Training epoch 155, recon_loss:0.561348, zinb_loss:0.949532, cluster_loss:0.139410\n",
      "Clustering   155: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.014025245441795231\n",
      "Training epoch 156, recon_loss:0.564887, zinb_loss:0.948624, cluster_loss:0.139815\n",
      "Clustering   156: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.012622720897615708\n",
      "Training epoch 157, recon_loss:0.561342, zinb_loss:0.949400, cluster_loss:0.139208\n",
      "Clustering   157: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.012622720897615708\n",
      "Training epoch 158, recon_loss:0.564987, zinb_loss:0.948521, cluster_loss:0.139847\n",
      "Clustering   158: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.012622720897615708\n",
      "Training epoch 159, recon_loss:0.559116, zinb_loss:0.949656, cluster_loss:0.139046\n",
      "Clustering   159: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 160, recon_loss:0.558790, zinb_loss:0.948677, cluster_loss:0.139104\n",
      "Clustering   160: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.011220196353436185\n",
      "Training epoch 161, recon_loss:0.556177, zinb_loss:0.949499, cluster_loss:0.139019\n",
      "Clustering   161: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 162, recon_loss:0.563505, zinb_loss:0.949744, cluster_loss:0.139151\n",
      "Clustering   162: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.004207573632538569\n",
      "Training epoch 163, recon_loss:0.567196, zinb_loss:0.950013, cluster_loss:0.139530\n",
      "Clustering   163: AMI= 0.7966, NMI= 0.7981, ARI= 0.7842, ACC= 0.8878\n",
      "0.005610098176718092\n",
      "Training epoch 164, recon_loss:0.572914, zinb_loss:0.950330, cluster_loss:0.139811\n",
      "Clustering   164: AMI= 0.8020, NMI= 0.8035, ARI= 0.7975, ACC= 0.8990\n",
      "0.015427769985974754\n",
      "Training epoch 165, recon_loss:0.565417, zinb_loss:0.947335, cluster_loss:0.139063\n",
      "Clustering   165: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.019635343618513323\n",
      "Training epoch 166, recon_loss:0.566763, zinb_loss:0.946217, cluster_loss:0.139650\n",
      "Clustering   166: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 167, recon_loss:0.561416, zinb_loss:0.944001, cluster_loss:0.138952\n",
      "Clustering   167: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 168, recon_loss:0.554119, zinb_loss:0.943016, cluster_loss:0.138979\n",
      "Clustering   168: AMI= 0.8011, NMI= 0.8025, ARI= 0.7958, ACC= 0.8976\n",
      "0.0182328190743338\n",
      "Training epoch 169, recon_loss:0.554824, zinb_loss:0.943454, cluster_loss:0.138622\n",
      "Clustering   169: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.0182328190743338\n",
      "Training epoch 170, recon_loss:0.560849, zinb_loss:0.944838, cluster_loss:0.139453\n",
      "Clustering   170: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 171, recon_loss:0.571178, zinb_loss:0.949115, cluster_loss:0.139482\n",
      "Clustering   171: AMI= 0.7952, NMI= 0.7967, ARI= 0.7812, ACC= 0.8822\n",
      "0.025245441795231416\n",
      "Training epoch 172, recon_loss:0.557742, zinb_loss:0.951776, cluster_loss:0.139273\n",
      "Clustering   172: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.023842917251051893\n",
      "Training epoch 173, recon_loss:0.562333, zinb_loss:0.957598, cluster_loss:0.139469\n",
      "Clustering   173: AMI= 0.7999, NMI= 0.8014, ARI= 0.7895, ACC= 0.8878\n",
      "0.021037868162692847\n",
      "Training epoch 174, recon_loss:0.565541, zinb_loss:0.963049, cluster_loss:0.140002\n",
      "Clustering   174: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.02244039270687237\n",
      "Training epoch 175, recon_loss:0.564130, zinb_loss:0.967147, cluster_loss:0.139143\n",
      "Clustering   175: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.0182328190743338\n",
      "Training epoch 176, recon_loss:0.559191, zinb_loss:0.964304, cluster_loss:0.140204\n",
      "Clustering   176: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.014025245441795231\n",
      "Training epoch 177, recon_loss:0.558775, zinb_loss:0.965172, cluster_loss:0.138834\n",
      "Clustering   177: AMI= 0.7966, NMI= 0.7981, ARI= 0.7842, ACC= 0.8878\n",
      "0.012622720897615708\n",
      "Training epoch 178, recon_loss:0.562155, zinb_loss:0.962404, cluster_loss:0.139973\n",
      "Clustering   178: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.015427769985974754\n",
      "Training epoch 179, recon_loss:0.557246, zinb_loss:0.962945, cluster_loss:0.138469\n",
      "Clustering   179: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.0182328190743338\n",
      "Training epoch 180, recon_loss:0.567407, zinb_loss:0.958664, cluster_loss:0.140047\n",
      "Clustering   180: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.021037868162692847\n",
      "Training epoch 181, recon_loss:0.561227, zinb_loss:0.956569, cluster_loss:0.138239\n",
      "Clustering   181: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.02244039270687237\n",
      "Training epoch 182, recon_loss:0.571999, zinb_loss:0.953369, cluster_loss:0.139634\n",
      "Clustering   182: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.02244039270687237\n",
      "Training epoch 183, recon_loss:0.562831, zinb_loss:0.950731, cluster_loss:0.138765\n",
      "Clustering   183: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.021037868162692847\n",
      "Training epoch 184, recon_loss:0.562834, zinb_loss:0.948696, cluster_loss:0.138958\n",
      "Clustering   184: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.0182328190743338\n",
      "Training epoch 185, recon_loss:0.554341, zinb_loss:0.947127, cluster_loss:0.138629\n",
      "Clustering   185: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.0182328190743338\n",
      "Training epoch 186, recon_loss:0.554318, zinb_loss:0.946943, cluster_loss:0.138476\n",
      "Clustering   186: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.0182328190743338\n",
      "Training epoch 187, recon_loss:0.554205, zinb_loss:0.946047, cluster_loss:0.138282\n",
      "Clustering   187: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.0182328190743338\n",
      "Training epoch 188, recon_loss:0.556875, zinb_loss:0.946212, cluster_loss:0.138523\n",
      "Clustering   188: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.015427769985974754\n",
      "Training epoch 189, recon_loss:0.559590, zinb_loss:0.945660, cluster_loss:0.138507\n",
      "Clustering   189: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.014025245441795231\n",
      "Training epoch 190, recon_loss:0.553177, zinb_loss:0.944996, cluster_loss:0.138648\n",
      "Clustering   190: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 191, recon_loss:0.554439, zinb_loss:0.945041, cluster_loss:0.138439\n",
      "Clustering   191: AMI= 0.7966, NMI= 0.7981, ARI= 0.7842, ACC= 0.8878\n",
      "0.009817671809256662\n",
      "Training epoch 192, recon_loss:0.553866, zinb_loss:0.944376, cluster_loss:0.138919\n",
      "Clustering   192: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.009817671809256662\n",
      "Training epoch 193, recon_loss:0.561051, zinb_loss:0.946455, cluster_loss:0.138563\n",
      "Clustering   193: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.011220196353436185\n",
      "Training epoch 194, recon_loss:0.560486, zinb_loss:0.946312, cluster_loss:0.139320\n",
      "Clustering   194: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.012622720897615708\n",
      "Training epoch 195, recon_loss:0.565607, zinb_loss:0.949365, cluster_loss:0.138350\n",
      "Clustering   195: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.014025245441795231\n",
      "Training epoch 196, recon_loss:0.561675, zinb_loss:0.949086, cluster_loss:0.139085\n",
      "Clustering   196: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 197, recon_loss:0.552825, zinb_loss:0.949171, cluster_loss:0.137769\n",
      "Clustering   197: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 198, recon_loss:0.556433, zinb_loss:0.949125, cluster_loss:0.138837\n",
      "Clustering   198: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.02244039270687237\n",
      "Training epoch 199, recon_loss:0.552727, zinb_loss:0.950187, cluster_loss:0.137867\n",
      "Clustering   199: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.02244039270687237\n",
      "Training epoch 200, recon_loss:0.557517, zinb_loss:0.950874, cluster_loss:0.139120\n",
      "Clustering   200: AMI= 0.8050, NMI= 0.8065, ARI= 0.8027, ACC= 0.9032\n",
      "0.023842917251051893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 201, recon_loss:0.560163, zinb_loss:0.952765, cluster_loss:0.138424\n",
      "Clustering   201: AMI= 0.7977, NMI= 0.7992, ARI= 0.7853, ACC= 0.8836\n",
      "0.029453015427769985\n",
      "Training epoch 202, recon_loss:0.559278, zinb_loss:0.951228, cluster_loss:0.139156\n",
      "Clustering   202: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.02664796633941094\n",
      "Training epoch 203, recon_loss:0.555610, zinb_loss:0.951442, cluster_loss:0.138337\n",
      "Clustering   203: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 204, recon_loss:0.551334, zinb_loss:0.949304, cluster_loss:0.138810\n",
      "Clustering   204: AMI= 0.7996, NMI= 0.8011, ARI= 0.7968, ACC= 0.8990\n",
      "0.0182328190743338\n",
      "Training epoch 205, recon_loss:0.555345, zinb_loss:0.949771, cluster_loss:0.138282\n",
      "Clustering   205: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.012622720897615708\n",
      "Training epoch 206, recon_loss:0.554472, zinb_loss:0.948075, cluster_loss:0.138985\n",
      "Clustering   206: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.0070126227208976155\n",
      "Training epoch 207, recon_loss:0.556609, zinb_loss:0.949599, cluster_loss:0.138019\n",
      "Clustering   207: AMI= 0.8008, NMI= 0.8023, ARI= 0.7920, ACC= 0.8948\n",
      "0.001402524544179523\n",
      "Training epoch 208, recon_loss:0.553534, zinb_loss:0.946515, cluster_loss:0.138840\n",
      "Clustering   208: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.005610098176718092\n",
      "Training epoch 209, recon_loss:0.549864, zinb_loss:0.947396, cluster_loss:0.137947\n",
      "Clustering   209: AMI= 0.8008, NMI= 0.8023, ARI= 0.7920, ACC= 0.8948\n",
      "0.005610098176718092\n",
      "Training epoch 210, recon_loss:0.555691, zinb_loss:0.945686, cluster_loss:0.138893\n",
      "Clustering   210: AMI= 0.7991, NMI= 0.8005, ARI= 0.7888, ACC= 0.8920\n",
      "0.002805049088359046\n",
      "Training epoch 211, recon_loss:0.548201, zinb_loss:0.947547, cluster_loss:0.138206\n",
      "Clustering   211: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.005610098176718092\n",
      "Training epoch 212, recon_loss:0.552479, zinb_loss:0.945137, cluster_loss:0.138705\n",
      "Clustering   212: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 213, recon_loss:0.547231, zinb_loss:0.946614, cluster_loss:0.138392\n",
      "Clustering   213: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.012622720897615708\n",
      "Training epoch 214, recon_loss:0.557313, zinb_loss:0.945888, cluster_loss:0.138670\n",
      "Clustering   214: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.0182328190743338\n",
      "Training epoch 215, recon_loss:0.551849, zinb_loss:0.947795, cluster_loss:0.138891\n",
      "Clustering   215: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.0182328190743338\n",
      "Training epoch 216, recon_loss:0.558941, zinb_loss:0.948516, cluster_loss:0.138589\n",
      "Clustering   216: AMI= 0.8039, NMI= 0.8053, ARI= 0.8003, ACC= 0.9018\n",
      "0.019635343618513323\n",
      "Training epoch 217, recon_loss:0.554441, zinb_loss:0.949826, cluster_loss:0.138850\n",
      "Clustering   217: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.019635343618513323\n",
      "Training epoch 218, recon_loss:0.558265, zinb_loss:0.950307, cluster_loss:0.137994\n",
      "Clustering   218: AMI= 0.8019, NMI= 0.8033, ARI= 0.7968, ACC= 0.8990\n",
      "0.016830294530154277\n",
      "Training epoch 219, recon_loss:0.550899, zinb_loss:0.951043, cluster_loss:0.138027\n",
      "Clustering   219: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.015427769985974754\n",
      "Training epoch 220, recon_loss:0.554911, zinb_loss:0.951114, cluster_loss:0.138051\n",
      "Clustering   220: AMI= 0.8027, NMI= 0.8041, ARI= 0.7953, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 221, recon_loss:0.553969, zinb_loss:0.953141, cluster_loss:0.137822\n",
      "Clustering   221: AMI= 0.7982, NMI= 0.7997, ARI= 0.7873, ACC= 0.8906\n",
      "0.0070126227208976155\n",
      "Training epoch 222, recon_loss:0.554675, zinb_loss:0.951755, cluster_loss:0.138559\n",
      "Clustering   222: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.005610098176718092\n",
      "Training epoch 223, recon_loss:0.551136, zinb_loss:0.953014, cluster_loss:0.137688\n",
      "Clustering   223: AMI= 0.8008, NMI= 0.8023, ARI= 0.7920, ACC= 0.8948\n",
      "0.004207573632538569\n",
      "Training epoch 224, recon_loss:0.550015, zinb_loss:0.950311, cluster_loss:0.138656\n",
      "Clustering   224: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.001402524544179523\n",
      "Training epoch 225, recon_loss:0.551960, zinb_loss:0.952525, cluster_loss:0.137652\n",
      "Clustering   225: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.0070126227208976155\n",
      "Training epoch 226, recon_loss:0.554912, zinb_loss:0.950530, cluster_loss:0.139110\n",
      "Clustering   226: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.011220196353436185\n",
      "Training epoch 227, recon_loss:0.554953, zinb_loss:0.953220, cluster_loss:0.137523\n",
      "Clustering   227: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.015427769985974754\n",
      "Training epoch 228, recon_loss:0.550178, zinb_loss:0.949451, cluster_loss:0.138663\n",
      "Clustering   228: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.0182328190743338\n",
      "Training epoch 229, recon_loss:0.553322, zinb_loss:0.950929, cluster_loss:0.137364\n",
      "Clustering   229: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.019635343618513323\n",
      "Training epoch 230, recon_loss:0.552179, zinb_loss:0.948869, cluster_loss:0.138418\n",
      "Clustering   230: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 231, recon_loss:0.552669, zinb_loss:0.949691, cluster_loss:0.137794\n",
      "Clustering   231: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 232, recon_loss:0.554028, zinb_loss:0.949955, cluster_loss:0.138034\n",
      "Clustering   232: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 233, recon_loss:0.562085, zinb_loss:0.951060, cluster_loss:0.138563\n",
      "Clustering   233: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.019635343618513323\n",
      "Training epoch 234, recon_loss:0.565091, zinb_loss:0.953704, cluster_loss:0.138093\n",
      "Clustering   234: AMI= 0.8009, NMI= 0.8024, ARI= 0.7952, ACC= 0.8976\n",
      "0.016830294530154277\n",
      "Training epoch 235, recon_loss:0.549414, zinb_loss:0.952939, cluster_loss:0.137598\n",
      "Clustering   235: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.015427769985974754\n",
      "Training epoch 236, recon_loss:0.548322, zinb_loss:0.954071, cluster_loss:0.137447\n",
      "Clustering   236: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.011220196353436185\n",
      "Training epoch 237, recon_loss:0.560644, zinb_loss:0.955286, cluster_loss:0.138056\n",
      "Clustering   237: AMI= 0.7966, NMI= 0.7981, ARI= 0.7842, ACC= 0.8878\n",
      "0.008415147265077139\n",
      "Training epoch 238, recon_loss:0.559778, zinb_loss:0.956205, cluster_loss:0.138311\n",
      "Clustering   238: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.005610098176718092\n",
      "Training epoch 239, recon_loss:0.551665, zinb_loss:0.955539, cluster_loss:0.137886\n",
      "Clustering   239: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.002805049088359046\n",
      "Training epoch 240, recon_loss:0.543473, zinb_loss:0.954256, cluster_loss:0.137660\n",
      "Clustering   240: AMI= 0.7991, NMI= 0.8005, ARI= 0.7888, ACC= 0.8920\n",
      "0.004207573632538569\n",
      "Training epoch 241, recon_loss:0.542470, zinb_loss:0.954409, cluster_loss:0.137409\n",
      "Clustering   241: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.004207573632538569\n",
      "Training epoch 242, recon_loss:0.553007, zinb_loss:0.956710, cluster_loss:0.138094\n",
      "Clustering   242: AMI= 0.7991, NMI= 0.8005, ARI= 0.7888, ACC= 0.8920\n",
      "0.004207573632538569\n",
      "Training epoch 243, recon_loss:0.552935, zinb_loss:0.959630, cluster_loss:0.137755\n",
      "Clustering   243: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.004207573632538569\n",
      "Training epoch 244, recon_loss:0.558171, zinb_loss:0.963735, cluster_loss:0.137955\n",
      "Clustering   244: AMI= 0.7982, NMI= 0.7997, ARI= 0.7873, ACC= 0.8906\n",
      "0.005610098176718092\n",
      "Training epoch 245, recon_loss:0.550320, zinb_loss:0.963383, cluster_loss:0.137411\n",
      "Clustering   245: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.009817671809256662\n",
      "Training epoch 246, recon_loss:0.556313, zinb_loss:0.965919, cluster_loss:0.137654\n",
      "Clustering   246: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.015427769985974754\n",
      "Training epoch 247, recon_loss:0.556735, zinb_loss:0.963191, cluster_loss:0.137716\n",
      "Clustering   247: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 248, recon_loss:0.550461, zinb_loss:0.961877, cluster_loss:0.137465\n",
      "Clustering   248: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 249, recon_loss:0.550832, zinb_loss:0.957146, cluster_loss:0.137465\n",
      "Clustering   249: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 250, recon_loss:0.545638, zinb_loss:0.955126, cluster_loss:0.137340\n",
      "Clustering   250: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.019635343618513323\n",
      "Training epoch 251, recon_loss:0.545250, zinb_loss:0.952109, cluster_loss:0.137269\n",
      "Clustering   251: AMI= 0.7996, NMI= 0.8011, ARI= 0.7968, ACC= 0.8990\n",
      "0.016830294530154277\n",
      "Training epoch 252, recon_loss:0.541135, zinb_loss:0.950724, cluster_loss:0.137122\n",
      "Clustering   252: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.015427769985974754\n",
      "Training epoch 253, recon_loss:0.546581, zinb_loss:0.949511, cluster_loss:0.137190\n",
      "Clustering   253: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.012622720897615708\n",
      "Training epoch 254, recon_loss:0.547389, zinb_loss:0.948769, cluster_loss:0.137173\n",
      "Clustering   254: AMI= 0.7966, NMI= 0.7981, ARI= 0.7842, ACC= 0.8878\n",
      "0.011220196353436185\n",
      "Training epoch 255, recon_loss:0.556919, zinb_loss:0.948537, cluster_loss:0.137631\n",
      "Clustering   255: AMI= 0.8008, NMI= 0.8023, ARI= 0.7920, ACC= 0.8948\n",
      "0.0070126227208976155\n",
      "Training epoch 256, recon_loss:0.540823, zinb_loss:0.947092, cluster_loss:0.136891\n",
      "Clustering   256: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.001402524544179523\n",
      "Training epoch 257, recon_loss:0.544380, zinb_loss:0.945996, cluster_loss:0.137245\n",
      "Clustering   257: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.0070126227208976155\n",
      "Training epoch 258, recon_loss:0.540594, zinb_loss:0.945100, cluster_loss:0.136966\n",
      "Clustering   258: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.009817671809256662\n",
      "Training epoch 259, recon_loss:0.555509, zinb_loss:0.945867, cluster_loss:0.137565\n",
      "Clustering   259: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.014025245441795231\n",
      "Training epoch 260, recon_loss:0.543579, zinb_loss:0.945013, cluster_loss:0.137432\n",
      "Clustering   260: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 261, recon_loss:0.554898, zinb_loss:0.945093, cluster_loss:0.137298\n",
      "Clustering   261: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 262, recon_loss:0.545537, zinb_loss:0.943783, cluster_loss:0.137530\n",
      "Clustering   262: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 263, recon_loss:0.550030, zinb_loss:0.944166, cluster_loss:0.137130\n",
      "Clustering   263: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.019635343618513323\n",
      "Training epoch 264, recon_loss:0.544208, zinb_loss:0.942838, cluster_loss:0.137198\n",
      "Clustering   264: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.014025245441795231\n",
      "Training epoch 265, recon_loss:0.551173, zinb_loss:0.943068, cluster_loss:0.137143\n",
      "Clustering   265: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.012622720897615708\n",
      "Training epoch 266, recon_loss:0.550728, zinb_loss:0.943487, cluster_loss:0.137199\n",
      "Clustering   266: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.009817671809256662\n",
      "Training epoch 267, recon_loss:0.551793, zinb_loss:0.944079, cluster_loss:0.137422\n",
      "Clustering   267: AMI= 0.7991, NMI= 0.8005, ARI= 0.7888, ACC= 0.8920\n",
      "0.004207573632538569\n",
      "Training epoch 268, recon_loss:0.544268, zinb_loss:0.944580, cluster_loss:0.136874\n",
      "Clustering   268: AMI= 0.7999, NMI= 0.8014, ARI= 0.7904, ACC= 0.8934\n",
      "0.001402524544179523\n",
      "Training epoch 269, recon_loss:0.540252, zinb_loss:0.944511, cluster_loss:0.137097\n",
      "Clustering   269: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.005610098176718092\n",
      "Training epoch 270, recon_loss:0.540314, zinb_loss:0.945224, cluster_loss:0.136642\n",
      "Clustering   270: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.012622720897615708\n",
      "Training epoch 271, recon_loss:0.543438, zinb_loss:0.946887, cluster_loss:0.137153\n",
      "Clustering   271: AMI= 0.7987, NMI= 0.8002, ARI= 0.7951, ACC= 0.8976\n",
      "0.014025245441795231\n",
      "Training epoch 272, recon_loss:0.549266, zinb_loss:0.948132, cluster_loss:0.136789\n",
      "Clustering   272: AMI= 0.7950, NMI= 0.7965, ARI= 0.7813, ACC= 0.8850\n",
      "0.015427769985974754\n",
      "Training epoch 273, recon_loss:0.545371, zinb_loss:0.951168, cluster_loss:0.137433\n",
      "Clustering   273: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.019635343618513323\n",
      "Training epoch 274, recon_loss:0.548651, zinb_loss:0.951874, cluster_loss:0.136746\n",
      "Clustering   274: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 275, recon_loss:0.546810, zinb_loss:0.955149, cluster_loss:0.137910\n",
      "Clustering   275: AMI= 0.8030, NMI= 0.8044, ARI= 0.7992, ACC= 0.9004\n",
      "0.021037868162692847\n",
      "Training epoch 276, recon_loss:0.549967, zinb_loss:0.954674, cluster_loss:0.136762\n",
      "Clustering   276: AMI= 0.7943, NMI= 0.7958, ARI= 0.7799, ACC= 0.8836\n",
      "0.021037868162692847\n",
      "Training epoch 277, recon_loss:0.554643, zinb_loss:0.957181, cluster_loss:0.138659\n",
      "Clustering   277: AMI= 0.8040, NMI= 0.8054, ARI= 0.8010, ACC= 0.9018\n",
      "0.02244039270687237\n",
      "Training epoch 278, recon_loss:0.552270, zinb_loss:0.956533, cluster_loss:0.136907\n",
      "Clustering   278: AMI= 0.7958, NMI= 0.7973, ARI= 0.7828, ACC= 0.8864\n",
      "0.019635343618513323\n",
      "Training epoch 279, recon_loss:0.545275, zinb_loss:0.955092, cluster_loss:0.137437\n",
      "Clustering   279: AMI= 0.8006, NMI= 0.8021, ARI= 0.7985, ACC= 0.9004\n",
      "0.016830294530154277\n",
      "Training epoch 280, recon_loss:0.537278, zinb_loss:0.954205, cluster_loss:0.136832\n",
      "Clustering   280: AMI= 0.7974, NMI= 0.7989, ARI= 0.7857, ACC= 0.8892\n",
      "0.014025245441795231\n",
      "Training epoch 281, recon_loss:0.541548, zinb_loss:0.951905, cluster_loss:0.136588\n",
      "Clustering   281: AMI= 0.7977, NMI= 0.7992, ARI= 0.7934, ACC= 0.8962\n",
      "0.009817671809256662\n",
      "Training epoch 282, recon_loss:0.539546, zinb_loss:0.952138, cluster_loss:0.137067\n",
      "Clustering   282: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.002805049088359046\n",
      "Training epoch 283, recon_loss:0.543183, zinb_loss:0.949986, cluster_loss:0.136098\n",
      "Clustering   283: AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n",
      "0.0\n",
      "delta_label  0.0 < tol  0.001\n",
      "Reach tolerance threshold. Stopping training.\n",
      "Final Result : AMI= 0.8017, NMI= 0.8032, ARI= 0.7937, ACC= 0.8962\n"
     ]
    }
   ],
   "source": [
    "y_pred, final_latent = model.fit(y=y, n_clusters=5, file='10x1kpbmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ffdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scmdc]",
   "language": "python",
   "name": "conda-env-scmdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
